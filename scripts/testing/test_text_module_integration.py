#!/usr/bin/env python3
"""
Textæ¨¡å—é›†æˆæµ‹è¯•
éªŒè¯é‡æ„åçš„ç»Ÿä¸€æ¥å£å’Œé›†æˆåŠŸèƒ½
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath('.'))

def test_unified_interface():
    """æµ‹è¯•ç»Ÿä¸€çš„æ–‡æœ¬å¤„ç†æ¥å£"""
    print("ğŸ” æµ‹è¯•ç»Ÿä¸€æ¥å£...")
    
    try:
        from app.utils.text import process_text, batch_process_texts
        
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯ç»Ÿä¸€æ¥å£çš„åŠŸèƒ½ã€‚åŒ…å«ä¸­æ–‡å’ŒEnglishæ··åˆå†…å®¹ã€‚"
        
        # æµ‹è¯•åˆ†ææ“ä½œ
        result = process_text(test_text, "analyze")
        assert "language" in result
        assert "token_count" in result
        assert "metadata" in result
        print(f"   ğŸ“Š åˆ†æç»“æœ: è¯­è¨€={result['language']}, ä»¤ç‰Œæ•°={result['token_count']}")
        
        # æµ‹è¯•åˆ†å—æ“ä½œ
        result = process_text(test_text, "chunk", {"chunk_size": 30, "chunk_overlap": 10})
        assert "chunks" in result
        assert "chunk_count" in result
        print(f"   ğŸ“„ åˆ†å—ç»“æœ: {result['chunk_count']} ä¸ªå—")
        
        # æµ‹è¯•ä»¤ç‰Œè®¡æ•°
        result = process_text(test_text, "count_tokens", {"model": "gpt-3.5-turbo"})
        assert "token_count" in result
        assert "estimated_cost" in result
        print(f"   ğŸ’° ä»¤ç‰Œç»Ÿè®¡: {result['token_count']} ä»¤ç‰Œ, æˆæœ¬=${result['estimated_cost']:.6f}")
        
        # æµ‹è¯•è¯­è¨€æ£€æµ‹
        result = process_text(test_text, "detect_language")
        assert "language" in result
        assert "confidence" in result
        print(f"   ğŸŒ è¯­è¨€æ£€æµ‹: {result['language']}, ç½®ä¿¡åº¦={result['confidence']}")
        
        # æµ‹è¯•æ‰¹å¤„ç†
        texts = ["ç¬¬ä¸€ä¸ªæ–‡æœ¬", "Second text", "ç¬¬ä¸‰ä¸ªæ–‡æœ¬"]
        batch_results = batch_process_texts(texts, "analyze")
        assert len(batch_results) == 3
        print(f"   ğŸ“š æ‰¹å¤„ç†: å¤„ç†äº† {len(batch_results)} ä¸ªæ–‡æœ¬")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ç»Ÿä¸€æ¥å£æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_comprehensive_analysis():
    """æµ‹è¯•ç»¼åˆåˆ†ææ¥å£"""
    print("ğŸ” æµ‹è¯•ç»¼åˆåˆ†æ...")
    
    try:
        from app.utils.text import analyze_text_comprehensive
        
        test_text = """
        # è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£
        
        è¿™ä¸ªæ–‡æ¡£åŒ…å«å¤šç§å†…å®¹ï¼š
        1. æ ‡é¢˜å’Œåˆ—è¡¨
        2. ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
        3. æ•°å­— 123 å’Œæ—¥æœŸ 2024-12-26
        4. ç½‘å€ https://example.com
        5. é‚®ç®± test@example.com
        
        ```python
        def example_function():
            return "è¿™æ˜¯ä»£ç å—"
        ```
        
        æœ€åä¸€æ®µåŒ…å«äº†å¤æ‚çš„æ ‡ç‚¹ç¬¦å·ï¼Œä»¥åŠ"å¼•ç”¨å†…å®¹"ç­‰ç‰¹æ®Šæ ¼å¼ã€‚
        """
        
        result = analyze_text_comprehensive(test_text)
        
        assert "basic_stats" in result
        assert "metadata" in result  
        assert "chunking" in result
        assert "cost_estimation" in result
        
        stats = result["basic_stats"]
        metadata = result["metadata"]
        
        print(f"   ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡: {stats['char_count']} å­—ç¬¦, {stats['word_count']} è¯, {stats['token_count']} ä»¤ç‰Œ")
        print(f"   ğŸ—ï¸  ç»“æ„ç‰¹å¾: æ ‡é¢˜={metadata['structure']['has_headers']}, åˆ—è¡¨={metadata['structure']['has_lists']}, ä»£ç ={metadata['structure']['has_code']}")
        print(f"   ğŸŒ è¯­è¨€ç‰¹å¾: URL={metadata['language_features']['has_urls']}, é‚®ç®±={metadata['language_features']['has_emails']}")
        print(f"   ğŸ“š åˆ†å—ä¿¡æ¯: {result['chunking']['chunk_count']} ä¸ªå—, å¹³å‡é•¿åº¦={result['chunking']['avg_chunk_length']:.1f}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ç»¼åˆåˆ†ææµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("ğŸ” æµ‹è¯•å‘åå…¼å®¹æ€§...")
    
    try:
        # æµ‹è¯•åŸå§‹æ¥å£æ˜¯å¦ä»ç„¶å¯ç”¨
        from app.utils.text import (
            count_tokens, chunk_text, detect_language, extract_metadata_from_text,
            clean_text, extract_keywords, tokenize_text
        )
        
        test_text = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•å‘åå…¼å®¹æ€§çš„æ–‡æœ¬ã€‚"
        
        # åŸå§‹ä»¤ç‰Œè®¡æ•°æ¥å£
        tokens = count_tokens(test_text)
        assert isinstance(tokens, int)
        print(f"   ğŸ”¢ ä»¤ç‰Œè®¡æ•°: {tokens}")
        
        # åŸå§‹åˆ†å—æ¥å£
        chunks = chunk_text(test_text, chunk_size=20)
        assert isinstance(chunks, list)
        print(f"   ğŸ“„ æ–‡æœ¬åˆ†å—: {len(chunks)} ä¸ªå—")
        
        # åŸå§‹è¯­è¨€æ£€æµ‹æ¥å£
        language = detect_language(test_text)
        assert isinstance(language, str)
        print(f"   ğŸŒ è¯­è¨€æ£€æµ‹: {language}")
        
        # åŸå§‹å…ƒæ•°æ®æå–æ¥å£
        metadata = extract_metadata_from_text(test_text)
        assert isinstance(metadata, dict)
        print(f"   ğŸ“‹ å…ƒæ•°æ®: {len(metadata)} ä¸ªå­—æ®µ")
        
        # åŸå§‹æ–‡æœ¬æ¸…ç†æ¥å£
        cleaned = clean_text(test_text)
        assert isinstance(cleaned, str)
        print(f"   ğŸ§¹ æ–‡æœ¬æ¸…ç†: {len(cleaned)} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"   âŒ å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_new_features():
    """æµ‹è¯•æ–°å¢åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æ–°å¢åŠŸèƒ½...")
    
    try:
        from app.utils.text import (
            TextLanguage, TextProcessingConfig, ChunkConfig, TokenConfig,
            create_text_analyzer, create_token_counter, create_chunker
        )
        
        # æµ‹è¯•é…ç½®ç³»ç»Ÿ
        text_config = TextProcessingConfig(
            language=TextLanguage.CHINESE,
            normalize_whitespace=True,
            max_length=1000
        )
        assert text_config.language == TextLanguage.CHINESE
        print("   âš™ï¸  é…ç½®ç³»ç»Ÿæ­£å¸¸")
        
        # æµ‹è¯•å·¥å‚æ¨¡å¼
        analyzer = create_text_analyzer()
        counter = create_token_counter(use_tiktoken=False)
        chunker = create_chunker("semantic")
        
        test_text = "æµ‹è¯•å·¥å‚æ¨¡å¼åˆ›å»ºçš„ç»„ä»¶ã€‚"
        
        # æµ‹è¯•åˆ†æå™¨
        result = analyzer.analyze(test_text)
        assert result.language in ["zh", "unknown"]
        print(f"   ğŸ” åˆ†æå™¨: è¯­è¨€={result.language}, ä»¤ç‰Œ={result.token_count}")
        
        # æµ‹è¯•è®¡æ•°å™¨
        tokens = counter.count_tokens(test_text)
        cost = counter.estimate_cost(test_text, 0.0001)
        assert tokens > 0
        print(f"   ğŸ”¢ è®¡æ•°å™¨: {tokens} ä»¤ç‰Œ, ${cost:.6f}")
        
        # æµ‹è¯•åˆ†å—å™¨
        chunks = chunker.chunk(test_text * 10)  # é‡å¤æ–‡æœ¬ä¾¿äºåˆ†å—
        assert len(chunks) >= 1
        print(f"   ğŸ“„ åˆ†å—å™¨: {len(chunks)} ä¸ªå—")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ–°å¢åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance_improvement():
    """æµ‹è¯•æ€§èƒ½æ”¹è¿›"""
    print("ğŸ” æµ‹è¯•æ€§èƒ½æ”¹è¿›...")
    
    try:
        import time
        from app.utils.text import analyze_text_comprehensive, batch_process_texts
        
        # åˆ›å»ºå¤§æ–‡æœ¬æµ‹è¯•æ€§èƒ½
        large_text = "è¿™æ˜¯ä¸€ä¸ªå¤§æ–‡æœ¬ç”¨äºæ€§èƒ½æµ‹è¯•ã€‚åŒ…å«å¤šç§è¯­è¨€å’Œæ ¼å¼å†…å®¹ã€‚" * 100
        texts = [f"æµ‹è¯•æ–‡æœ¬ {i}" for i in range(50)]
        
        print(f"   ğŸ“ æµ‹è¯•è§„æ¨¡: å¤§æ–‡æœ¬={len(large_text)}å­—ç¬¦, æ‰¹å¤„ç†={len(texts)}ä¸ªæ–‡æœ¬")
        
        # ç»¼åˆåˆ†ææ€§èƒ½
        start_time = time.time()
        result = analyze_text_comprehensive(large_text)
        analysis_time = time.time() - start_time
        
        assert "basic_stats" in result
        print(f"   âš¡ ç»¼åˆåˆ†æ: {analysis_time:.3f}ç§’")
        
        # æ‰¹å¤„ç†æ€§èƒ½
        start_time = time.time()
        batch_results = batch_process_texts(texts, "analyze")
        batch_time = time.time() - start_time
        
        assert len(batch_results) == len(texts)
        print(f"   âš¡ æ‰¹é‡å¤„ç†: {batch_time:.3f}ç§’ ({len(texts)}ä¸ªæ–‡æœ¬)")
        print(f"   ğŸ“Š å¹³å‡æ¯ä¸ªæ–‡æœ¬: {batch_time/len(texts)*1000:.1f}æ¯«ç§’")
        
        # éªŒè¯æ€§èƒ½è¦æ±‚
        assert analysis_time < 2.0  # ç»¼åˆåˆ†æåº”è¯¥åœ¨2ç§’å†…å®Œæˆ
        assert batch_time < 5.0     # æ‰¹å¤„ç†åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("ğŸ” æµ‹è¯•é”™è¯¯å¤„ç†...")
    
    try:
        from app.utils.text import process_text, TextProcessingError
        
        # æµ‹è¯•æ— æ•ˆæ“ä½œ
        try:
            process_text("æµ‹è¯•", "invalid_operation")
            assert False, "åº”è¯¥æŠ›å‡ºå¼‚å¸¸"
        except ValueError as e:
            print(f"   âœ… æ— æ•ˆæ“ä½œå¼‚å¸¸: {e}")
        
        # æµ‹è¯•ç©ºæ–‡æœ¬å¤„ç†
        result = process_text("", "analyze")
        assert result["char_count"] == 0
        print("   âœ… ç©ºæ–‡æœ¬å¤„ç†æ­£å¸¸")
        
        # æµ‹è¯•å¼‚å¸¸æ–‡æœ¬
        weird_text = "\x00\x01\x02"  # åŒ…å«ç‰¹æ®Šå­—ç¬¦
        result = process_text(weird_text, "analyze")
        assert isinstance(result, dict)
        print("   âœ… å¼‚å¸¸æ–‡æœ¬å¤„ç†æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"   âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    print("=" * 70)
    print("ğŸš€ Textæ¨¡å—é›†æˆæµ‹è¯•")
    print("=" * 70)
    
    tests = [
        ("ç»Ÿä¸€æ¥å£", test_unified_interface),
        ("ç»¼åˆåˆ†æ", test_comprehensive_analysis),
        ("å‘åå…¼å®¹æ€§", test_backward_compatibility),
        ("æ–°å¢åŠŸèƒ½", test_new_features),
        ("æ€§èƒ½æ”¹è¿›", test_performance_improvement),
        ("é”™è¯¯å¤„ç†", test_error_handling),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name}æµ‹è¯•:")
        try:
            if test_func():
                passed += 1
                print(f"   âœ… {test_name}æµ‹è¯•é€šè¿‡")
            else:
                print(f"   âŒ {test_name}æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"   âŒ {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 70)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼textæ¨¡å—é‡æ„å®Œæˆï¼")
        
        print("\nğŸ† é‡æ„æˆå°±:")
        print("  âœ… ç»Ÿä¸€æ¥å£è®¾è®¡ - æä¾›äº†ä¸€è‡´çš„APIä½“éªŒ")
        print("  âœ… ç»¼åˆåˆ†æèƒ½åŠ› - é›†æˆå¤šç§æ–‡æœ¬åˆ†æåŠŸèƒ½")
        print("  âœ… å®Œå…¨å‘åå…¼å®¹ - ä¿æŒæ‰€æœ‰åŸæœ‰æ¥å£å¯ç”¨") 
        print("  âœ… æ–°åŠŸèƒ½å¢å¼º - æ·»åŠ äº†å·¥å‚æ¨¡å¼å’Œé…ç½®ç³»ç»Ÿ")
        print("  âœ… æ€§èƒ½ä¼˜åŒ– - æ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦")
        print("  âœ… å¥å£®é”™è¯¯å¤„ç† - ä¼˜é›…å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ")
        
        print("\nğŸ“ˆ è´¨é‡æŒ‡æ ‡:")
        print("  â€¢ ä»£ç å¯ç»´æŠ¤æ€§: ä»æ··ä¹±åˆ°æ¸…æ™°çš„æ¨¡å—åŒ–ç»“æ„")
        print("  â€¢ åŠŸèƒ½å¯æ‰©å±•æ€§: åŸºäºæŠ½è±¡ç±»çš„å¯æ‰©å±•æ¶æ„")
        print("  â€¢ æ¥å£ä¸€è‡´æ€§: ç»Ÿä¸€çš„é…ç½®å’Œè°ƒç”¨æ¨¡å¼")
        print("  â€¢ æ€§èƒ½è¡¨ç°: ä¼˜åŒ–çš„ç®—æ³•å’Œç¼“å­˜æœºåˆ¶")
        
        print("\nğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’:")
        print("  1. é‡æ„ embedding_utils.py ä¸ºå·¥å‚æ¨¡å¼")
        print("  2. é‡æ„ template_renderer.py ä¸ºå¼•æ“æŠ½è±¡")
        print("  3. å¼€å§‹ security æ¨¡å—ç²¾ç»†åŒ–é‡æ„")
        print("  4. ç¼–å†™å®Œæ•´çš„å•å…ƒæµ‹è¯•å¥—ä»¶")
        
        return True
    else:
        print(f"âŒ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 