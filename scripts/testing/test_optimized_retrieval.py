#!/usr/bin/env python3
"""
æ£€ç´¢ç³»ç»Ÿä¼˜åŒ–æ¨¡å—é›†æˆæµ‹è¯•
æµ‹è¯•æ‰€æœ‰5ä¸ªä¼˜åŒ–æ¨¡å—çš„åŠŸèƒ½å’Œæ€§èƒ½
"""

import asyncio
import logging
import time
import random
import json
from typing import Dict, Any, List
import unittest
from unittest.mock import AsyncMock, MagicMock, patch
import tempfile
import os

# è®¾ç½®æµ‹è¯•ç¯å¢ƒ
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from core.knowledge.optimization import (
    RetrievalConfigManager,
    SearchStrategySelector,
    DataSyncService,
    EnhancedErrorHandler,
    PerformanceOptimizer,
    CacheConfig,
    CacheStrategy,
    SyncJobConfig,
    SyncOperation,
    CircuitBreakerConfig
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TestOptimizedRetrieval(unittest.IsolatedAsyncioTestCase):
    """ä¼˜åŒ–æ£€ç´¢ç³»ç»Ÿé›†æˆæµ‹è¯•"""
    
    async def asyncSetUp(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        logger.info("ğŸš€ å¼€å§‹é›†æˆæµ‹è¯•åˆå§‹åŒ–")
        
        # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
        self.temp_config_file = tempfile.NamedTemporaryFile(
            mode='w', suffix='.yaml', delete=False
        )
        
        # å†™å…¥æµ‹è¯•é…ç½®
        test_config = """
vector_search:
  top_k: 10
  similarity_threshold: 0.7
  engine: "milvus"
  timeout: 30

keyword_search:
  top_k: 10
  engine: "elasticsearch"
  analyzer: "standard"
  fuzzy_enabled: true

hybrid_search:
  vector_weight: 0.7
  keyword_weight: 0.3
  rrf_k: 60
  top_k: 10

cache:
  enabled: true
  strategy: "lru"
  max_size: 100
  ttl_seconds: 300

performance:
  max_concurrent_requests: 10
  enable_query_optimization: true
  monitoring_enabled: true
"""
        self.temp_config_file.write(test_config)
        self.temp_config_file.close()
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self.config_manager = RetrievalConfigManager(self.temp_config_file.name)
        await self.config_manager.initialize()
        
        self.strategy_selector = SearchStrategySelector()
        await self.strategy_selector.initialize()
        
        self.sync_service = DataSyncService()
        await self.sync_service.initialize()
        
        self.error_handler = EnhancedErrorHandler()
        
        cache_config = CacheConfig(
            strategy=CacheStrategy.LRU,
            max_size=100,
            ttl_seconds=300
        )
        self.performance_optimizer = PerformanceOptimizer(cache_config)
        
        logger.info("âœ… é›†æˆæµ‹è¯•åˆå§‹åŒ–å®Œæˆ")
    
    async def asyncTearDown(self):
        """æµ‹è¯•æ¸…ç†"""
        logger.info("ğŸ§¹ å¼€å§‹é›†æˆæµ‹è¯•æ¸…ç†")
        
        # æ¸…ç†æ‰€æœ‰ç»„ä»¶
        await self.config_manager.cleanup()
        await self.strategy_selector.cleanup()
        await self.sync_service.cleanup()
        await self.error_handler.cleanup()
        self.performance_optimizer.cleanup()
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(self.temp_config_file.name):
            os.unlink(self.temp_config_file.name)
        
        logger.info("âœ… é›†æˆæµ‹è¯•æ¸…ç†å®Œæˆ")
    
    async def test_01_config_manager_functionality(self):
        """æµ‹è¯•é…ç½®ç®¡ç†å™¨åŠŸèƒ½"""
        logger.info("ğŸ”§ æµ‹è¯•é…ç½®ç®¡ç†å™¨åŠŸèƒ½")
        
        # æµ‹è¯•é…ç½®è·å–
        config = self.config_manager.get_config()
        self.assertIsNotNone(config)
        
        vector_config = self.config_manager.get_vector_search_config()
        self.assertEqual(vector_config.top_k, 10)
        self.assertEqual(vector_config.similarity_threshold, 0.7)
        
        # æµ‹è¯•é…ç½®éªŒè¯
        self.assertTrue(self.config_manager.validate_config())
        
        # æµ‹è¯•åŠ¨æ€æ›´æ–°
        success = await self.config_manager.update_config({
            "cache": {"max_size": 200}
        })
        self.assertTrue(success)
        
        updated_config = self.config_manager.get_cache_config()
        self.assertEqual(updated_config.max_size, 200)
        
        logger.info("âœ… é…ç½®ç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    async def test_02_strategy_selector_functionality(self):
        """æµ‹è¯•ç­–ç•¥é€‰æ‹©å™¨åŠŸèƒ½"""
        logger.info("ğŸ¯ æµ‹è¯•ç­–ç•¥é€‰æ‹©å™¨åŠŸèƒ½")
        
        # æµ‹è¯•å¼•æ“èƒ½åŠ›è¯„ä¼°
        assessment = await self.strategy_selector.assess_engine_capability("elasticsearch")
        self.assertIsNotNone(assessment)
        self.assertGreaterEqual(assessment.overall_score(), 0)
        
        # æµ‹è¯•ç­–ç•¥é€‰æ‹©
        strategy, params = await self.strategy_selector.select_optimal_strategy(
            query="äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
            knowledge_base_id="test_kb"
        )
        self.assertIsNotNone(strategy)
        self.assertIsNotNone(params)
        
        # æµ‹è¯•ç­–ç•¥æ¨è
        recommendations = await self.strategy_selector.get_strategy_recommendations(
            "æœºå™¨å­¦ä¹ ç®—æ³•"
        )
        self.assertIn('query_analysis', recommendations)
        self.assertIn('recommended_strategies', recommendations)
        
        # æµ‹è¯•æ€§èƒ½ç»Ÿè®¡
        stats = self.strategy_selector.get_performance_stats()
        self.assertIn('engine_assessments', stats)
        
        logger.info("âœ… ç­–ç•¥é€‰æ‹©å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    async def test_03_sync_service_functionality(self):
        """æµ‹è¯•æ•°æ®åŒæ­¥æœåŠ¡åŠŸèƒ½"""
        logger.info("ğŸ”„ æµ‹è¯•æ•°æ®åŒæ­¥æœåŠ¡åŠŸèƒ½")
        
        # æµ‹è¯•æ–‡æ¡£åˆ†å—åŒæ­¥
        job_id = await self.sync_service.sync_document_chunks(
            knowledge_base_id="test_kb",
            document_id="test_doc"
        )
        self.assertIsNotNone(job_id)
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆï¼ˆç®€åŒ–æµ‹è¯•ï¼‰
        await asyncio.sleep(1)
        
        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        result = await self.sync_service.get_job_status(job_id)
        self.assertIsNotNone(result)
        
        # æµ‹è¯•å¢é‡åŒæ­¥
        incremental_job_id = await self.sync_service.incremental_sync(
            data_type="document_chunk",
            last_sync_time=time.time() - 3600
        )
        self.assertIsNotNone(incremental_job_id)
        
        # æµ‹è¯•åŒæ­¥ç»Ÿè®¡
        stats = await self.sync_service.get_sync_statistics()
        self.assertIn('active_jobs', stats)
        self.assertIn('total_jobs', stats)
        
        logger.info("âœ… æ•°æ®åŒæ­¥æœåŠ¡åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    async def test_04_error_handler_functionality(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†å™¨åŠŸèƒ½"""
        logger.info("ğŸ›¡ï¸ æµ‹è¯•é”™è¯¯å¤„ç†å™¨åŠŸèƒ½")
        
        # æµ‹è¯•ç†”æ–­å™¨åˆ›å»º
        cb = self.error_handler.get_or_create_circuit_breaker("test_cb")
        self.assertIsNotNone(cb)
        
        # æ¨¡æ‹ŸæˆåŠŸè¯·æ±‚
        async def success_func():
            await asyncio.sleep(0.1)
            return "success"
        
        result = await cb.call(success_func)
        self.assertEqual(result, "success")
        
        # æ¨¡æ‹Ÿå¤±è´¥è¯·æ±‚
        async def fail_func():
            raise ValueError("æµ‹è¯•é”™è¯¯")
        
        with self.assertRaises(ValueError):
            await cb.call(fail_func)
        
        # æµ‹è¯•é”™è¯¯ç»Ÿè®¡
        stats = self.error_handler.get_error_statistics()
        self.assertIn('global_stats', stats)
        self.assertIn('circuit_breakers', stats)
        
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        health = await self.error_handler.health_check()
        self.assertIn('status', health)
        
        logger.info("âœ… é”™è¯¯å¤„ç†å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    async def test_05_performance_optimizer_functionality(self):
        """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨åŠŸèƒ½"""
        logger.info("âš¡ æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨åŠŸèƒ½")
        
        # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        cache_key = self.performance_optimizer._generate_cache_key(
            "test query", {"param": "value"}
        )
        
        # è®¾ç½®ç¼“å­˜
        await self.performance_optimizer.set_cached_result(cache_key, "test_result")
        
        # è·å–ç¼“å­˜
        cached_result = await self.performance_optimizer.get_cached_result(cache_key)
        self.assertEqual(cached_result, "test_result")
        
        # æµ‹è¯•æŸ¥è¯¢ä¼˜åŒ–
        optimized_query = self.performance_optimizer.query_optimizer.optimize_query(
            "   è¿™æ˜¯ä¸€ä¸ª  æµ‹è¯•æŸ¥è¯¢   "
        )
        self.assertEqual(optimized_query.strip(), "è¿™æ˜¯ä¸€ä¸ª æµ‹è¯•æŸ¥è¯¢")
        
        # æµ‹è¯•ä¼˜åŒ–æ‰§è¡Œ
        async def mock_query_func(query, **kwargs):
            await asyncio.sleep(0.05)  # æ¨¡æ‹ŸæŸ¥è¯¢å»¶è¿Ÿ
            return f"ç»“æœfor: {query}"
        
        result = await self.performance_optimizer.optimize_and_execute(
            query_func=mock_query_func,
            query="test query",
            parameters={"top_k": 5}
        )
        self.assertIn("test query", result)
        
        # æµ‹è¯•æ€§èƒ½ç»Ÿè®¡
        stats = self.performance_optimizer.get_performance_stats()
        self.assertIn('request_metrics', stats)
        self.assertIn('cache_metrics', stats)
        
        logger.info("âœ… æ€§èƒ½ä¼˜åŒ–å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    async def test_06_integration_workflow(self):
        """æµ‹è¯•å®Œæ•´é›†æˆå·¥ä½œæµ"""
        logger.info("ğŸ”— æµ‹è¯•å®Œæ•´é›†æˆå·¥ä½œæµ")
        
        # æ¨¡æ‹Ÿå®Œæ•´çš„æœç´¢è¯·æ±‚å¤„ç†æµç¨‹
        async def integrated_search(query: str, knowledge_base_id: str = "test_kb"):
            """é›†æˆçš„æœç´¢å‡½æ•°"""
            
            # 1. ç­–ç•¥é€‰æ‹©
            strategy, strategy_params = await self.strategy_selector.select_optimal_strategy(
                query=query,
                knowledge_base_id=knowledge_base_id
            )
            
            # 2. æ€§èƒ½ä¼˜åŒ–æ‰§è¡Œ
            async def search_func(optimized_query, **params):
                # æ¨¡æ‹Ÿæœç´¢é€»è¾‘
                await asyncio.sleep(0.1)
                return {
                    "results": [
                        {"id": f"doc_{i}", "content": f"ç»“æœ {i}: {optimized_query}"}
                        for i in range(5)
                    ],
                    "strategy": strategy.value,
                    "params": strategy_params
                }
            
            result = await self.performance_optimizer.optimize_and_execute(
                query_func=search_func,
                query=query,
                parameters={"knowledge_base_id": knowledge_base_id}
            )
            
            return result
        
        # æ‰§è¡Œé›†æˆæœç´¢
        search_result = await integrated_search("äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•")
        
        self.assertIsNotNone(search_result)
        self.assertIn("results", search_result)
        self.assertIn("strategy", search_result)
        self.assertEqual(len(search_result["results"]), 5)
        
        logger.info("âœ… å®Œæ•´é›†æˆå·¥ä½œæµæµ‹è¯•é€šè¿‡")
    
    async def test_07_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        logger.info("ğŸš€ æµ‹è¯•å¹¶å‘æ€§èƒ½")
        
        async def concurrent_search(query_id: int):
            """å¹¶å‘æœç´¢å‡½æ•°"""
            query = f"æµ‹è¯•æŸ¥è¯¢ {query_id}"
            
            async def mock_search(q, **kwargs):
                # æ¨¡æ‹Ÿéšæœºå»¶è¿Ÿ
                await asyncio.sleep(random.uniform(0.05, 0.2))
                return f"ç»“æœ for {q}"
            
            return await self.performance_optimizer.optimize_and_execute(
                query_func=mock_search,
                query=query
            )
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        num_concurrent = 20
        tasks = [
            concurrent_search(i) for i in range(num_concurrent)
        ]
        
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        # éªŒè¯ç»“æœ
        self.assertEqual(len(results), num_concurrent)
        for i, result in enumerate(results):
            self.assertIn(f"æµ‹è¯•æŸ¥è¯¢ {i}", result)
        
        # æ£€æŸ¥æ€§èƒ½
        total_time = end_time - start_time
        self.assertLess(total_time, 5.0)  # åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
        
        # æ£€æŸ¥æ€§èƒ½ç»Ÿè®¡
        stats = self.performance_optimizer.get_performance_stats()
        cache_hit_rate = stats['cache_metrics']['hit_rate']
        
        logger.info(f"å¹¶å‘æµ‹è¯•å®Œæˆ: {num_concurrent}ä¸ªè¯·æ±‚, è€—æ—¶: {total_time:.2f}s, ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.2%}")
        logger.info("âœ… å¹¶å‘æ€§èƒ½æµ‹è¯•é€šè¿‡")
    
    async def test_08_error_recovery(self):
        """æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶"""
        logger.info("ğŸ”§ æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶")
        
        # åˆ›å»ºä¼šå¤±è´¥çš„å‡½æ•°
        failure_count = 0
        
        async def unreliable_function():
            nonlocal failure_count
            failure_count += 1
            
            if failure_count <= 3:
                raise ConnectionError(f"æ¨¡æ‹Ÿè¿æ¥é”™è¯¯ {failure_count}")
            else:
                return "æœ€ç»ˆæˆåŠŸ"
        
        # æµ‹è¯•é‡è¯•æœºåˆ¶
        @self.error_handler.retry_on_failure(max_retries=5, base_delay=0.1)
        async def retry_test():
            return await unreliable_function()
        
        result = await retry_test()
        self.assertEqual(result, "æœ€ç»ˆæˆåŠŸ")
        self.assertEqual(failure_count, 4)  # 3æ¬¡å¤±è´¥ + 1æ¬¡æˆåŠŸ
        
        # æµ‹è¯•ç†”æ–­å™¨æ¢å¤
        cb = self.error_handler.get_or_create_circuit_breaker(
            "recovery_test",
            CircuitBreakerConfig(failure_threshold=2, recovery_timeout=1)
        )
        
        # è§¦å‘ç†”æ–­å™¨
        for _ in range(3):
            try:
                await cb.call(lambda: exec('raise ValueError("æµ‹è¯•å¤±è´¥")'))
            except:
                pass
        
        # éªŒè¯ç†”æ–­å™¨å¼€å¯
        stats = cb.get_stats()
        self.assertEqual(stats['state'], 'open')
        
        # ç­‰å¾…æ¢å¤æ—¶é—´
        await asyncio.sleep(1.1)
        
        # æµ‹è¯•æ¢å¤
        await cb.call(lambda: "æ¢å¤æˆåŠŸ")
        
        logger.info("âœ… é”™è¯¯æ¢å¤æœºåˆ¶æµ‹è¯•é€šè¿‡")
    
    async def test_09_performance_benchmarks(self):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸ“Š æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        # åŸºå‡†æµ‹è¯•é…ç½®
        test_cases = [
            {"queries": 100, "cache_enabled": False, "name": "æ— ç¼“å­˜"},
            {"queries": 100, "cache_enabled": True, "name": "æœ‰ç¼“å­˜"},
        ]
        
        results = {}
        
        for test_case in test_cases:
            # é‡ç½®ä¼˜åŒ–å™¨
            cache_config = CacheConfig(
                strategy=CacheStrategy.LRU,
                max_size=1000 if test_case["cache_enabled"] else 0
            )
            optimizer = PerformanceOptimizer(cache_config)
            
            async def benchmark_search(query, **kwargs):
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæœç´¢å»¶è¿Ÿ
                return f"ç»“æœ for {query}"
            
            # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
            start_time = time.time()
            
            tasks = []
            for i in range(test_case["queries"]):
                query = f"åŸºå‡†æµ‹è¯•æŸ¥è¯¢ {i % 10}"  # é‡å¤æŸ¥è¯¢ä»¥æµ‹è¯•ç¼“å­˜æ•ˆæœ
                task = optimizer.optimize_and_execute(
                    query_func=benchmark_search,
                    query=query,
                    use_cache=test_case["cache_enabled"]
                )
                tasks.append(task)
            
            await asyncio.gather(*tasks)
            end_time = time.time()
            
            # æ”¶é›†ç»Ÿè®¡
            stats = optimizer.get_performance_stats()
            results[test_case["name"]] = {
                "total_time": end_time - start_time,
                "avg_response_time": stats['request_metrics']['avg_response_time'],
                "cache_hit_rate": stats['cache_metrics']['hit_rate'],
                "total_requests": stats['request_metrics']['total_requests']
            }
            
            optimizer.cleanup()
        
        # éªŒè¯ç¼“å­˜æ•ˆæœ
        cache_result = results["æœ‰ç¼“å­˜"]
        no_cache_result = results["æ— ç¼“å­˜"]
        
        self.assertLess(cache_result["total_time"], no_cache_result["total_time"])
        self.assertGreater(cache_result["cache_hit_rate"], 0.5)  # ç¼“å­˜å‘½ä¸­ç‡åº”è¯¥å¤§äº50%
        
        logger.info("åŸºå‡†æµ‹è¯•ç»“æœ:")
        for name, result in results.items():
            logger.info(f"  {name}:")
            logger.info(f"    æ€»æ—¶é—´: {result['total_time']:.2f}s")
            logger.info(f"    å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time']:.2f}ms")
            logger.info(f"    ç¼“å­˜å‘½ä¸­ç‡: {result['cache_hit_rate']:.2%}")
        
        logger.info("âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡")
    
    async def test_10_comprehensive_integration(self):
        """ç»¼åˆé›†æˆæµ‹è¯•"""
        logger.info("ğŸ¯ æ‰§è¡Œç»¼åˆé›†æˆæµ‹è¯•")
        
        # æ¨¡æ‹ŸçœŸå®çš„æœç´¢æœåŠ¡
        class OptimizedSearchService:
            def __init__(self):
                self.config_manager = self.config_manager
                self.strategy_selector = self.strategy_selector
                self.sync_service = self.sync_service
                self.error_handler = self.error_handler
                self.optimizer = self.performance_optimizer
            
            @self.error_handler.circuit_breaker("search_service")
            @self.optimizer.cache_decorator()
            async def search(self, query: str, **params):
                # é€‰æ‹©ç­–ç•¥
                strategy, strategy_params = await self.strategy_selector.select_optimal_strategy(
                    query=query
                )
                
                # æ¨¡æ‹Ÿæœç´¢
                await asyncio.sleep(0.05)
                
                return {
                    "query": query,
                    "strategy": strategy.value,
                    "results": [
                        {"id": f"result_{i}", "score": 0.9 - i * 0.1}
                        for i in range(5)
                    ],
                    "total": 5,
                    "strategy_params": strategy_params
                }
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æœç´¢æœåŠ¡
        search_service = OptimizedSearchService()
        
        # æ‰§è¡Œå¤šä¸ªæœç´¢è¯·æ±‚
        queries = [
            "äººå·¥æ™ºèƒ½å‘å±•å†å²",
            "æœºå™¨å­¦ä¹ ç®—æ³•",
            "æ·±åº¦å­¦ä¹ åº”ç”¨",
            "è‡ªç„¶è¯­è¨€å¤„ç†",
            "è®¡ç®—æœºè§†è§‰æŠ€æœ¯"
        ]
        
        results = []
        for query in queries:
            try:
                result = await search_service.search(query)
                results.append(result)
            except Exception as e:
                logger.warning(f"æœç´¢å¤±è´¥: {query} - {str(e)}")
        
        # éªŒè¯ç»“æœ
        self.assertEqual(len(results), len(queries))
        for result in results:
            self.assertIn("query", result)
            self.assertIn("strategy", result)
            self.assertIn("results", result)
            self.assertEqual(len(result["results"]), 5)
        
        # æ£€æŸ¥ç³»ç»Ÿæ•´ä½“çŠ¶æ€
        overall_stats = {
            "config_version": self.config_manager.get_config_version(),
            "strategy_stats": self.strategy_selector.get_performance_stats(),
            "sync_stats": await self.sync_service.get_sync_statistics(),
            "error_stats": self.error_handler.get_error_statistics(),
            "performance_stats": self.performance_optimizer.get_performance_stats()
        }
        
        logger.info("ç³»ç»Ÿæ•´ä½“çŠ¶æ€æ£€æŸ¥:")
        logger.info(f"  é…ç½®ç‰ˆæœ¬: {overall_stats['config_version']}")
        logger.info(f"  åŒæ­¥ä»»åŠ¡: {overall_stats['sync_stats']['total_jobs']}")
        logger.info(f"  é”™è¯¯ç‡: {overall_stats['error_stats']['global_stats']['failure_rate']:.2%}")
        logger.info(f"  ç¼“å­˜å‘½ä¸­ç‡: {overall_stats['performance_stats']['cache_metrics']['hit_rate']:.2%}")
        
        logger.info("âœ… ç»¼åˆé›†æˆæµ‹è¯•é€šè¿‡")


async def run_integration_tests():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ£€ç´¢ç³»ç»Ÿä¼˜åŒ–é›†æˆæµ‹è¯•")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestOptimizedRetrieval)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š é›†æˆæµ‹è¯•ç»“æœæ‘˜è¦")
    logger.info("="*60)
    logger.info(f"æ€»æµ‹è¯•æ•°: {result.testsRun}")
    logger.info(f"æˆåŠŸæ•°: {result.testsRun - len(result.failures) - len(result.errors)}")
    logger.info(f"å¤±è´¥æ•°: {len(result.failures)}")
    logger.info(f"é”™è¯¯æ•°: {len(result.errors)}")
    
    if result.failures:
        logger.error("âŒ å¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            logger.error(f"  - {test}: {traceback}")
    
    if result.errors:
        logger.error("ğŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            logger.error(f"  - {test}: {traceback}")
    
    if result.wasSuccessful():
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ£€ç´¢ç³»ç»Ÿä¼˜åŒ–æ¨¡å—é›†æˆæˆåŠŸï¼")
        return True
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
        return False


if __name__ == "__main__":
    # è¿è¡Œé›†æˆæµ‹è¯•
    success = asyncio.run(run_integration_tests())
    exit(0 if success else 1) 