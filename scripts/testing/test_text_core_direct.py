#!/usr/bin/env python3
"""
Textæ ¸å¿ƒæ¨¡å—ç›´æ¥æµ‹è¯•
é¿å…å¤æ‚ä¾èµ–ï¼Œç›´æ¥æµ‹è¯•é‡æ„çš„ç»„ä»¶
"""

import sys
import os

# æ·»åŠ è·¯å¾„ä»¥ç›´æ¥å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app', 'utils', 'text', 'core'))

def test_base_components():
    """æµ‹è¯•åŸºç¡€ç»„ä»¶"""
    print("ğŸ” æµ‹è¯•åŸºç¡€ç»„ä»¶...")
    
    try:
        # ç›´æ¥å¯¼å…¥baseæ¨¡å—
        import base
        
        # æµ‹è¯•æšä¸¾
        assert base.TextLanguage.CHINESE.value == "zh"
        assert base.TextLanguage.ENGLISH.value == "en"
        print("   âœ… è¯­è¨€æšä¸¾æ­£å¸¸")
        
        # æµ‹è¯•é…ç½®ç±»
        config = base.TextProcessingConfig(
            language=base.TextLanguage.CHINESE,
            normalize_whitespace=True
        )
        assert config.language == base.TextLanguage.CHINESE
        assert config.encoding == "utf-8"
        print("   âœ… é…ç½®ç±»æ­£å¸¸")
        
        # æµ‹è¯•åˆ†å—é…ç½®
        chunk_config = base.ChunkConfig(chunk_size=1500, chunk_overlap=300)
        assert chunk_config.chunk_size == 1500
        assert chunk_config.chunk_overlap == 300
        print("   âœ… åˆ†å—é…ç½®æ­£å¸¸")
        
        # æµ‹è¯•ä»¤ç‰Œé…ç½®
        token_config = base.TokenConfig(model="gpt-4")
        assert token_config.model == "gpt-4"
        print("   âœ… ä»¤ç‰Œé…ç½®æ­£å¸¸")
        
        # æµ‹è¯•å¼‚å¸¸ä½“ç³»
        try:
            raise base.InvalidTextError("æµ‹è¯•å¼‚å¸¸")
        except base.TextProcessingError:
            print("   âœ… å¼‚å¸¸ä½“ç³»æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"   âŒ åŸºç¡€ç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_tokenizer_direct():
    """ç›´æ¥æµ‹è¯•ä»¤ç‰Œè®¡æ•°å™¨"""
    print("ğŸ” æµ‹è¯•ä»¤ç‰Œè®¡æ•°å™¨...")
    
    try:
        import base
        import tokenizer
        
        # æµ‹è¯•ç®€å•è®¡æ•°å™¨
        config = base.TokenConfig(model="gpt-3.5-turbo")
        simple_counter = tokenizer.SimpleTokenCounter(config)
        
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯ä»¤ç‰Œè®¡æ•°åŠŸèƒ½ã€‚"
        token_count = simple_counter.count_tokens(test_text)
        
        assert isinstance(token_count, int)
        assert token_count > 0
        print(f"   ğŸ“Š æ–‡æœ¬ä»¤ç‰Œæ•°: {token_count}")
        
        # æµ‹è¯•æˆæœ¬ä¼°ç®—
        cost = simple_counter.estimate_cost(test_text, 0.0001)
        assert isinstance(cost, float)
        assert cost > 0
        print(f"   ğŸ’° ä¼°ç®—æˆæœ¬: ${cost:.6f}")
        
        # æµ‹è¯•æ‰¹é‡è®¡æ•°
        texts = ["ç¬¬ä¸€ä¸ªæ–‡æœ¬", "ç¬¬äºŒä¸ªæ–‡æœ¬", "ç¬¬ä¸‰ä¸ªæ›´é•¿çš„æ–‡æœ¬ç”¨äºæµ‹è¯•"]
        batch_counts = simple_counter.batch_count_tokens(texts)
        assert len(batch_counts) == 3
        assert all(isinstance(count, int) and count > 0 for count in batch_counts)
        print(f"   ğŸ“š æ‰¹é‡è®¡æ•°: {batch_counts}")
        
        # æµ‹è¯•è¯­è¨€æ£€æµ‹
        zh_text = "è¿™æ˜¯ä¸­æ–‡æ–‡æœ¬"
        en_text = "This is English text"
        
        zh_lang = simple_counter._detect_simple_language(zh_text)
        en_lang = simple_counter._detect_simple_language(en_text)
        
        assert zh_lang == 'zh'
        assert en_lang == 'en'
        print(f"   ğŸŒ è¯­è¨€æ£€æµ‹: ä¸­æ–‡={zh_lang}, è‹±æ–‡={en_lang}")
        
        # æµ‹è¯•å·¥å‚å‡½æ•°
        counter = tokenizer.create_token_counter(use_tiktoken=False, config=config)
        assert isinstance(counter, tokenizer.SimpleTokenCounter)
        print("   âœ… å·¥å‚å‡½æ•°æ­£å¸¸")
        
        # æµ‹è¯•å‘åå…¼å®¹å‡½æ•°
        compat_count = tokenizer.count_tokens(test_text, "gpt-3.5-turbo")
        assert isinstance(compat_count, int)
        print(f"   ğŸ”„ å…¼å®¹æ¥å£: {compat_count} ä»¤ç‰Œ")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ä»¤ç‰Œè®¡æ•°å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_chunker_direct():
    """ç›´æ¥æµ‹è¯•æ–‡æœ¬åˆ†å—å™¨"""
    print("ğŸ” æµ‹è¯•æ–‡æœ¬åˆ†å—å™¨...")
    
    try:
        import base
        import chunker
        
        # æµ‹è¯•æ–‡æœ¬
        test_text = """
        è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ï¼ŒåŒ…å«äº†ä¸€äº›åŸºæœ¬çš„å†…å®¹ç”¨äºæµ‹è¯•åˆ†å—åŠŸèƒ½ã€‚è¿™æ®µæ–‡æœ¬åº”è¯¥è¶³å¤Ÿé•¿ï¼Œä»¥ä¾¿èƒ½å¤Ÿè§¦å‘åˆ†å—é€»è¾‘ã€‚
        
        è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬ï¼Œå®ƒåŒ…å«äº†ä¸åŒçš„å†…å®¹ã€‚è¿™æ®µæ–‡æœ¬çš„ç›®çš„æ˜¯æµ‹è¯•æ®µè½æ„ŸçŸ¥çš„åˆ†å—åŠŸèƒ½ï¼Œç¡®ä¿åˆ†å—å™¨èƒ½å¤Ÿæ­£ç¡®å¤„ç†æ®µè½è¾¹ç•Œã€‚
        
        è¿™æ˜¯ç¬¬ä¸‰æ®µæ–‡æœ¬ï¼Œç›¸å¯¹è¾ƒçŸ­ä¸€äº›ã€‚ä½†ä»ç„¶åŒ…å«è¶³å¤Ÿçš„å†…å®¹æ¥æµ‹è¯•å„ç§åˆ†å—ç­–ç•¥ã€‚æœ€åä¸€å¥è¯ç”¨æ¥ç»“æŸè¿™ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚
        """ * 2
        
        # æµ‹è¯•æ™ºèƒ½åˆ†å—å™¨
        config = base.ChunkConfig(chunk_size=200, chunk_overlap=50, min_chunk_size=50)
        smart_chunker = chunker.SmartTextChunker(config)
        
        chunks = smart_chunker.chunk(test_text)
        assert isinstance(chunks, list)
        assert len(chunks) > 1
        print(f"   ğŸ“„ æ™ºèƒ½åˆ†å—ç»“æœ: {len(chunks)} ä¸ªå—")
        
        # éªŒè¯å—çš„å¤§å°å’Œå†…å®¹
        total_chars = 0
        for i, chunk in enumerate(chunks):
            chunk_len = len(chunk)
            total_chars += chunk_len
            print(f"     å— {i+1}: {chunk_len} å­—ç¬¦")
            assert chunk_len >= config.min_chunk_size
            assert chunk.strip()  # ç¡®ä¿æ²¡æœ‰ç©ºå—
        
        # æµ‹è¯•è¯­ä¹‰åˆ†å—å™¨
        semantic_chunker = chunker.SemanticChunker(config)
        semantic_chunks = semantic_chunker.chunk(test_text)
        assert isinstance(semantic_chunks, list)
        print(f"   ğŸ§  è¯­ä¹‰åˆ†å—ç»“æœ: {len(semantic_chunks)} ä¸ªå—")
        
        # æµ‹è¯•å›ºå®šå¤§å°åˆ†å—å™¨
        fixed_chunker = chunker.FixedSizeChunker(config)
        fixed_chunks = fixed_chunker.chunk(test_text)
        assert isinstance(fixed_chunks, list)
        print(f"   ğŸ“ å›ºå®šåˆ†å—ç»“æœ: {len(fixed_chunks)} ä¸ªå—")
        
        # æµ‹è¯•è¾¹ç•Œæ£€æµ‹
        boundary_text = "å¥å­1ã€‚å¥å­2ï¼å¥å­3ï¼Ÿæ–°æ®µè½\n\nå¦ä¸€ä¸ªæ®µè½ã€‚"
        boundary_config = base.ChunkConfig(chunk_size=15, chunk_overlap=5)
        boundary_chunker = chunker.SmartTextChunker(boundary_config)
        boundary_chunks = boundary_chunker.chunk(boundary_text)
        print(f"   ğŸ¯ è¾¹ç•Œæ£€æµ‹: {len(boundary_chunks)} ä¸ªå—")
        for i, chunk in enumerate(boundary_chunks):
            print(f"     è¾¹ç•Œå— {i+1}: '{chunk}'")
        
        # æµ‹è¯•å·¥å‚å‡½æ•°
        factory_chunker = chunker.create_chunker("smart", config)
        assert isinstance(factory_chunker, chunker.SmartTextChunker)
        
        factory_chunker2 = chunker.create_chunker("semantic", config)
        assert isinstance(factory_chunker2, chunker.SemanticChunker)
        print("   âœ… å·¥å‚å‡½æ•°æ­£å¸¸")
        
        # æµ‹è¯•å‘åå…¼å®¹å‡½æ•°
        compat_chunks = chunker.chunk_text(test_text, chunk_size=200, chunk_overlap=50)
        assert isinstance(compat_chunks, list)
        print(f"   ğŸ”„ å…¼å®¹æ¥å£: {len(compat_chunks)} ä¸ªå—")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ–‡æœ¬åˆ†å—å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance_comparison():
    """æµ‹è¯•æ€§èƒ½å¯¹æ¯”"""
    print("ğŸ” æµ‹è¯•æ€§èƒ½å¯¹æ¯”...")
    
    try:
        import time
        import base
        import tokenizer
        import chunker
        
        # åˆ›å»ºå¤§æ–‡æœ¬è¿›è¡Œæ€§èƒ½æµ‹è¯•
        large_text = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæ€§èƒ½æµ‹è¯•çš„é•¿æ–‡æœ¬ã€‚å®ƒåŒ…å«äº†å¤šä¸ªå¥å­å’Œæ®µè½ï¼Œç”¨æ¥æµ‹è¯•æˆ‘ä»¬é‡æ„åçš„ç»„ä»¶æ€§èƒ½ã€‚" * 200
        
        print(f"   ğŸ“ æµ‹è¯•æ–‡æœ¬é•¿åº¦: {len(large_text)} å­—ç¬¦")
        
        # ä»¤ç‰Œè®¡æ•°æ€§èƒ½æµ‹è¯•
        config = base.TokenConfig()
        counter = tokenizer.SimpleTokenCounter(config)
        
        start_time = time.time()
        for _ in range(10):  # å¤šæ¬¡æµ‹è¯•å–å¹³å‡
            tokens = counter.count_tokens(large_text)
        token_time = (time.time() - start_time) / 10
        
        print(f"   âš¡ ä»¤ç‰Œè®¡æ•°æ€§èƒ½: {token_time:.4f}ç§’/æ¬¡ ({tokens} ä»¤ç‰Œ)")
        
        # åˆ†å—æ€§èƒ½æµ‹è¯•
        chunk_config = base.ChunkConfig(chunk_size=500, chunk_overlap=100)
        smart_chunker = chunker.SmartTextChunker(chunk_config)
        
        start_time = time.time()
        for _ in range(5):  # åˆ†å—æ¯”è¾ƒè€—æ—¶ï¼Œæµ‹è¯•æ¬¡æ•°å°‘ä¸€äº›
            chunks = smart_chunker.chunk(large_text)
        chunk_time = (time.time() - start_time) / 5
        
        print(f"   âš¡ æ™ºèƒ½åˆ†å—æ€§èƒ½: {chunk_time:.4f}ç§’/æ¬¡ ({len(chunks)} å—)")
        
        # æ¯”è¾ƒä¸åŒåˆ†å—å™¨çš„æ€§èƒ½
        fixed_chunker = chunker.FixedSizeChunker(chunk_config)
        semantic_chunker = chunker.SemanticChunker(chunk_config)
        
        start_time = time.time()
        fixed_chunks = fixed_chunker.chunk(large_text)
        fixed_time = time.time() - start_time
        
        start_time = time.time()
        semantic_chunks = semantic_chunker.chunk(large_text)
        semantic_time = time.time() - start_time
        
        print(f"   ğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        print(f"     å›ºå®šåˆ†å—: {fixed_time:.4f}ç§’ ({len(fixed_chunks)} å—)")
        print(f"     è¯­ä¹‰åˆ†å—: {semantic_time:.4f}ç§’ ({len(semantic_chunks)} å—)")
        print(f"     æ™ºèƒ½åˆ†å—: {chunk_time:.4f}ç§’ ({len(chunks)} å—)")
        
        # éªŒè¯æ€§èƒ½è¦æ±‚
        assert token_time < 0.1  # ä»¤ç‰Œè®¡æ•°åº”è¯¥å¾ˆå¿«
        assert chunk_time < 1.0  # åˆ†å—åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 70)
    print("ğŸš€ Textæ ¸å¿ƒæ¨¡å—ç›´æ¥æµ‹è¯•")
    print("=" * 70)
    
    tests = [
        ("åŸºç¡€ç»„ä»¶", test_base_components),
        ("ä»¤ç‰Œè®¡æ•°å™¨", test_tokenizer_direct),
        ("æ–‡æœ¬åˆ†å—å™¨", test_chunker_direct),
        ("æ€§èƒ½å¯¹æ¯”", test_performance_comparison),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name}æµ‹è¯•:")
        try:
            if test_func():
                passed += 1
                print(f"   âœ… {test_name}æµ‹è¯•é€šè¿‡")
            else:
                print(f"   âŒ {test_name}æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"   âŒ {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 70)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼textæ ¸å¿ƒæ¨¡å—é‡æ„æˆåŠŸï¼")
        
        print("\nğŸ“ˆ é‡æ„æˆæœ:")
        print("  âœ… æŠ½è±¡åŸºç±»æ¶æ„ - å»ºç«‹äº†ç»Ÿä¸€çš„æ¥å£æ ‡å‡†")
        print("  âœ… ä»¤ç‰Œè®¡æ•°å™¨ä¼˜åŒ– - æ”¯æŒå¤šç§æ¨¡å‹å’Œç¼“å­˜æœºåˆ¶")
        print("  âœ… æ™ºèƒ½æ–‡æœ¬åˆ†å— - è¾¹ç•Œæ„ŸçŸ¥å’Œè¯­ä¹‰åˆ†å‰²")
        print("  âœ… æ€§èƒ½ä¼˜åŒ– - é¢„ç¼–è¯‘æ­£åˆ™å’Œç®—æ³•æ”¹è¿›")
        print("  âœ… å‘åå…¼å®¹ - ä¿æŒåŸæœ‰æ¥å£å¯ç”¨")
        print("  âœ… å·¥å‚æ¨¡å¼ - æ”¯æŒçµæ´»çš„ç»„ä»¶åˆ›å»º")
        
        print("\nğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’:")
        print("  1. å®Œæˆæ–‡æœ¬åˆ†æå™¨(analyzer.py)é‡æ„")
        print("  2. å®Œæˆembedding_utils.pyé‡æ„")
        print("  3. å®Œæˆtemplate_renderer.pyé‡æ„")
        print("  4. åˆ›å»ºç»Ÿä¸€çš„__init__.pyæ¥å£")
        print("  5. ç¼–å†™å®Œæ•´çš„å•å…ƒæµ‹è¯•")
        
        return True
    else:
        print(f"âŒ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 