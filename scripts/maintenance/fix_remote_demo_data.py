#!/usr/bin/env python3
"""
ä¿®å¤è¿œç¨‹æ•°æ®åº“æ¼”ç¤ºæ•°æ®åˆ›å»ºçš„è„šæœ¬
é€‚é…ç°æœ‰çš„è¡¨ç»“æ„ï¼Œæ­£ç¡®åˆ›å»ºæ¼”ç¤ºæ•°æ®
"""

import psycopg2
import time
import json
from datetime import datetime
from psycopg2.extras import RealDictCursor, Json

# è¿œç¨‹æ•°æ®åº“è¿æ¥é…ç½®
REMOTE_DB_CONFIG = {
    'host': '167.71.85.231',
    'port': 5432,
    'user': 'zzdsj',
    'password': 'zzdsj123',
    'database': 'zzdsj'
}

def get_or_create_document():
    """è·å–æˆ–åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆçš„documentè®°å½•"""
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG, cursor_factory=RealDictCursor)
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰æ–‡æ¡£
        cursor.execute("SELECT id FROM documents LIMIT 1")
        existing_doc = cursor.fetchone()
        
        if existing_doc:
            doc_id = existing_doc['id']
            print(f"   ä½¿ç”¨ç°æœ‰æ–‡æ¡£ID: {doc_id}")
        else:
            # åˆ›å»ºä¸€ä¸ªæ–°çš„æ–‡æ¡£è®°å½•
            cursor.execute("""
                INSERT INTO documents (title, content, knowledge_base_id)
                VALUES (%s, %s, %s)
                RETURNING id
            """, (
                "æ¼”ç¤ºæ–‡æ¡£", 
                "è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºæ–‡æ¡£ï¼Œç”¨äºæµ‹è¯•å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†ç³»ç»Ÿ", 
                1  # å‡è®¾knowledge_base_idä¸º1
            ))
            
            doc_result = cursor.fetchone()
            doc_id = doc_result['id']
            print(f"   åˆ›å»ºæ–°æ–‡æ¡£ID: {doc_id}")
            conn.commit()
        
        cursor.close()
        conn.close()
        return doc_id
        
    except Exception as e:
        print(f"   âŒ è·å–æ–‡æ¡£IDå¤±è´¥: {e}")
        # å¦‚æœå¤±è´¥ï¼Œè¿”å›Noneï¼Œåç»­è·³è¿‡document_chunksçš„æ’å…¥
        return None

def create_compatible_demo_data():
    """åˆ›å»ºå…¼å®¹ç°æœ‰è¡¨ç»“æ„çš„æ¼”ç¤ºæ•°æ®"""
    print("ğŸ² åˆ›å»ºå…¼å®¹çš„æ¼”ç¤ºæ•°æ®...")
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG, cursor_factory=RealDictCursor)
        cursor = conn.cursor()
        
        # 1. åˆ›å»ºæµ‹è¯•æ–‡æ¡£åˆ°å¢å¼ºç‰ˆæ³¨å†Œè¡¨
        demo_file_id = "demo_file_" + str(int(time.time()))
        print(f"ğŸ“„ åˆ›å»ºæµ‹è¯•æ–‡æ¡£: {demo_file_id}")
        
        cursor.execute("""
            INSERT INTO document_registry_enhanced 
            (file_id, filename, content_type, file_size, file_hash, storage_backend, 
             storage_path, kb_id, doc_id, user_id, chunk_count, vector_count, es_doc_count)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (file_hash) DO NOTHING
            RETURNING file_id
        """, (
            demo_file_id, "demo_document.pdf", "application/pdf", 1024000, 
            "demo_hash_" + str(int(time.time())), "minio", "/demo/path/demo_document.pdf",
            "demo_kb_001", "demo_doc_001", "demo_user_001", 3, 3, 3
        ))
        
        result = cursor.fetchone()
        if result:
            created_file_id = result['file_id']
            print(f"âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸ: {created_file_id}")
        else:
            print("â„¹ï¸ æ–‡æ¡£å·²å­˜åœ¨ï¼ˆåŸºäºhashå†²çªæ£€æµ‹ï¼‰")
            created_file_id = demo_file_id
        
        # 2. è·å–æˆ–åˆ›å»ºæœ‰æ•ˆçš„document ID
        print("ğŸ”— å¤„ç†document_chunksçš„å¤–é”®çº¦æŸ...")
        document_id = get_or_create_document()
        
        # 3. å‘ç°æœ‰çš„document_chunksè¡¨æ·»åŠ æ•°æ®ï¼ˆä½¿ç”¨ç°æœ‰ç»“æ„ï¼‰
        if document_id:
            print("ğŸ”ª å‘ç°æœ‰document_chunksè¡¨æ·»åŠ æ¼”ç¤ºæ•°æ®...")
            for i in range(3):
                # ä½¿ç”¨JsonåŒ…è£…å™¨æ¥æ­£ç¡®å¤„ç†JSONBæ•°æ®
                metadata = Json({
                    "chunk_index": i,
                    "source_file": demo_file_id,
                    "chunk_type": "demo",
                    "created_at": datetime.now().isoformat()
                })
                
                cursor.execute("""
                    INSERT INTO document_chunks 
                    (document_id, content, metadata, embedding_id, token_count)
                    VALUES (%s, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    document_id,  # ä½¿ç”¨æœ‰æ•ˆçš„document_id
                    f"è¿™æ˜¯æ¼”ç¤ºæ–‡æ¡£çš„ç¬¬{i+1}ä¸ªåˆ‡ç‰‡å†…å®¹...åŒ…å«äº†é‡è¦çš„ä¿¡æ¯å’ŒçŸ¥è¯†ç‚¹ã€‚è¿™ä¸ªåˆ‡ç‰‡åŒ…å«äº†å…³äºæ™ºæ”¿çŸ¥è¯†åº“ç³»ç»Ÿçš„è¯¦ç»†è¯´æ˜ï¼Œå±•ç¤ºäº†å¦‚ä½•è¿›è¡Œæ–‡æ¡£ç®¡ç†å’Œå‘é‡æœç´¢ã€‚",
                    metadata,
                    f"embedding_{i+1}_{int(time.time())}", 
                    200 + i*50
                ))
                
                chunk_result = cursor.fetchone()
                chunk_db_id = chunk_result['id']
                print(f"   âœ… åˆ‡ç‰‡ {i+1} åˆ›å»ºæˆåŠŸï¼ŒID: {chunk_db_id}")
        else:
            print("âš ï¸ è·³è¿‡document_chunksæ’å…¥ï¼ˆæ— æ³•è·å–æœ‰æ•ˆçš„document_idï¼‰")
        
        # 4. åˆ›å»ºå¢å¼ºç‰ˆå‘é‡å…³è”æ•°æ®
        print("ğŸ§  åˆ›å»ºå¢å¼ºç‰ˆå‘é‡å…³è”æ•°æ®...")
        for i in range(3):
            # ç”Ÿæˆè™šæ‹Ÿçš„chunk_idç”¨äºå¢å¼ºç‰ˆè¡¨
            virtual_chunk_id = f"demo_chunk_{i+1}_{int(time.time())}"
            
            # ä½¿ç”¨JSONå­—ç¬¦ä¸²è€Œä¸æ˜¯å­—å…¸
            vector_metadata = json.dumps({
                "chunk_index": i,
                "demo": True,
                "created_at": datetime.now().isoformat(),
                "embedding_model": "text-embedding-ada-002",
                "vector_source": "demo_creation"
            })
            
            cursor.execute("""
                INSERT INTO document_vectors_enhanced 
                (file_id, chunk_id, vector_id, vector_collection, embedding_model, embedding_dimension, vector_metadata)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, (
                created_file_id, virtual_chunk_id, f"vector_{i+1}_{int(time.time())}", 
                "demo_collection", "text-embedding-ada-002", 1536, vector_metadata
            ))
            print(f"   âœ… å‘é‡å…³è” {i+1} åˆ›å»ºæˆåŠŸ")
        
        # 5. åˆ›å»ºESåˆ†ç‰‡å…³è”æ•°æ®
        print("ğŸ” åˆ›å»ºESåˆ†ç‰‡å…³è”æ•°æ®...")
        for i in range(3):
            virtual_chunk_id = f"demo_chunk_{i+1}_{int(time.time())}"
            
            # ä½¿ç”¨JSONå­—ç¬¦ä¸²
            es_metadata = json.dumps({
                "chunk_index": i,
                "demo": True,
                "es_version": "8.x",
                "shard_info": f"shard_{i+1}",
                "created_at": datetime.now().isoformat()
            })
            
            cursor.execute("""
                INSERT INTO document_es_shards 
                (file_id, chunk_id, es_index, es_doc_id, es_shard_id, es_doc_type, es_metadata)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, (
                created_file_id, virtual_chunk_id, "demo_index", f"es_doc_{i+1}_{int(time.time())}", 
                f"shard_{i+1}", "document", es_metadata
            ))
            print(f"   âœ… ESåˆ†ç‰‡å…³è” {i+1} åˆ›å»ºæˆåŠŸ")
        
        # 6. åˆ›å»ºå¤„ç†å†å²è®°å½•
        print("ğŸ“ˆ åˆ›å»ºå¤„ç†å†å²è®°å½•...")
        cursor.execute("""
            INSERT INTO document_processing_history 
            (file_id, operation_type, operation_status, operation_details, 
             started_at, completed_at, duration_ms, operated_by)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            created_file_id, "demo_creation", "completed", 
            "æ¼”ç¤ºæ•°æ®åˆ›å»ºå®Œæˆ - å…¼å®¹ç°æœ‰è¡¨ç»“æ„ç‰ˆæœ¬ï¼ŒåŒ…å«JSONæ ¼å¼å…ƒæ•°æ®ï¼Œå¤„ç†äº†å¤–é”®çº¦æŸ", 
            datetime.now(), datetime.now(), 150, "system"
        ))
        print("   âœ… å¤„ç†å†å²è®°å½•åˆ›å»ºæˆåŠŸ")
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print("\nğŸ‰ å…¼å®¹æ¼”ç¤ºæ•°æ®åˆ›å»ºæˆåŠŸ!")
        print(f"ğŸ“Š åˆ›å»ºçš„æ•°æ®:")
        print(f"   â€¢ æ–‡æ¡£ID: {created_file_id}")
        if document_id:
            print(f"   â€¢ 3ä¸ªæ–‡æ¡£åˆ‡ç‰‡ï¼ˆç°æœ‰è¡¨ç»“æ„ï¼Œå¸¦JSONBå…ƒæ•°æ®ï¼‰")
        else:
            print(f"   â€¢ æ–‡æ¡£åˆ‡ç‰‡åˆ›å»ºè·³è¿‡ï¼ˆå¤–é”®çº¦æŸé—®é¢˜ï¼‰")
        print(f"   â€¢ 3ä¸ªå‘é‡å…³è”ï¼ˆå¢å¼ºç‰ˆè¡¨ï¼‰")
        print(f"   â€¢ 3ä¸ªESåˆ†ç‰‡å…³è”ï¼ˆå¢å¼ºç‰ˆè¡¨ï¼‰")
        print(f"   â€¢ 1ä¸ªå¤„ç†å†å²è®°å½•")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ¼”ç¤ºæ•°æ®å¤±è´¥: {e}")
        return False

def verify_demo_data():
    """éªŒè¯æ¼”ç¤ºæ•°æ®"""
    print("\nğŸ” éªŒè¯æ¼”ç¤ºæ•°æ®...")
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG, cursor_factory=RealDictCursor)
        cursor = conn.cursor()
        
        # æ£€æŸ¥å„è¡¨çš„æ•°æ®é‡
        tables_to_check = [
            'document_registry_enhanced',
            'document_chunks', 
            'document_vectors_enhanced',
            'document_es_shards',
            'document_processing_history'
        ]
        
        for table in tables_to_check:
            cursor.execute(f"SELECT COUNT(*) as count FROM {table}")
            count = cursor.fetchone()['count']
            print(f"   ğŸ“‹ {table}: {count} æ¡è®°å½•")
        
        # æ£€æŸ¥æœ€æ–°çš„æ¼”ç¤ºæ•°æ®
        cursor.execute("""
            SELECT file_id, filename, chunk_count, vector_count, es_doc_count 
            FROM document_registry_enhanced 
            WHERE filename = 'demo_document.pdf'
            ORDER BY created_at DESC 
            LIMIT 1
        """)
        
        demo_doc = cursor.fetchone()
        if demo_doc:
            print(f"\nğŸ“„ æœ€æ–°æ¼”ç¤ºæ–‡æ¡£:")
            print(f"   â€¢ æ–‡ä»¶ID: {demo_doc['file_id']}")
            print(f"   â€¢ æ–‡ä»¶å: {demo_doc['filename']}")
            print(f"   â€¢ åˆ‡ç‰‡æ•°: {demo_doc['chunk_count']}")
            print(f"   â€¢ å‘é‡æ•°: {demo_doc['vector_count']}")
            print(f"   â€¢ ESæ–‡æ¡£æ•°: {demo_doc['es_doc_count']}")
        
        # æ£€æŸ¥document_chunksçš„JSONBæ•°æ®
        cursor.execute("""
            SELECT id, metadata, token_count
            FROM document_chunks 
            WHERE metadata->>'chunk_type' = 'demo'
            ORDER BY id DESC 
            LIMIT 3
        """)
        
        demo_chunks = cursor.fetchall()
        if demo_chunks:
            print(f"\nğŸ”ª æ¼”ç¤ºåˆ‡ç‰‡æ•°æ®:")
            for chunk in demo_chunks:
                print(f"   â€¢ åˆ‡ç‰‡ID: {chunk['id']}, ä»¤ç‰Œæ•°: {chunk['token_count']}")
                print(f"     å…ƒæ•°æ®: {chunk['metadata']}")
        
        cursor.close()
        conn.close()
        
        return True
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è¿œç¨‹æ•°æ®åº“æ¼”ç¤ºæ•°æ®ä¿®å¤å·¥å…·")
    print("é€‚é…ç°æœ‰è¡¨ç»“æ„ï¼Œåˆ›å»ºå…¼å®¹çš„æ¼”ç¤ºæ•°æ®")
    print("="*60)
    
    # åˆ›å»ºæ¼”ç¤ºæ•°æ®
    if create_compatible_demo_data():
        # éªŒè¯æ•°æ®
        verify_demo_data()
        
        print("\nâœ… æ¼”ç¤ºæ•°æ®ä¿®å¤å®Œæˆ!")
        print("ğŸ’¡ ç°åœ¨è¿œç¨‹æ•°æ®åº“å·²åŒ…å«å®Œæ•´çš„æ¼”ç¤ºæ•°æ®ï¼Œ")
        print("   å¯ä»¥æµ‹è¯•å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†åŠŸèƒ½ã€‚")
        print("\nğŸ”§ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("   1. æµ‹è¯•å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†å™¨è¿æ¥")
        print("   2. éªŒè¯æ··åˆæœç´¢åŠŸèƒ½")
        print("   3. è¿è¡Œå®Œæ•´ç³»ç»Ÿæµ‹è¯•")
        
        return True
    else:
        print("\nâŒ æ¼”ç¤ºæ•°æ®ä¿®å¤å¤±è´¥!")
        return False

if __name__ == "__main__":
    try:
        success = main()
        exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        exit(1) 