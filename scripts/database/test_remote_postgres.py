#!/usr/bin/env python3
"""
ËøúÁ®ãPostgreSQLÊï∞ÊçÆÂ∫ìËøûÊé•ÊµãËØïÂíåÂàùÂßãÂåñËÑöÊú¨ - Â¢ûÂº∫Áâà
ÊµãËØïËøûÊé•Âà∞ËøúÁ®ãÊúçÂä°Âô®Âπ∂ÊâßË°åÂÆåÊï¥ÁöÑÊï∞ÊçÆÂ∫ìÂàùÂßãÂåñ
ÂåÖÂê´Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®ÁªìÊûÑÔºåÊîØÊåÅÂêëÈáèchunk ID„ÄÅÊñáÊ°£IDÂíåESÂàÜÁâáÊï∞ÊçÆÁöÑÂÆåÊï¥ÂÖ≥ËÅîËøΩË∏™
Êñ∞Â¢ûÁ≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•„ÄÅÊÄßËÉΩÁõëÊéß„ÄÅÁ¥¢ÂºïÂàÜÊûêÁ≠âÈ´òÁ∫ßÂäüËÉΩ
"""

import psycopg2
import psycopg2.extras
import os
import sys
from pathlib import Path
import time
from datetime import datetime, timedelta
import uuid
import json
import argparse
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

# ËøúÁ®ãÊï∞ÊçÆÂ∫ìËøûÊé•ÈÖçÁΩÆ
REMOTE_DB_CONFIG = {
    'host': '167.71.85.231',
    'port': 5432,
    'user': 'zzdsj',
    'password': 'zzdsj123',
    'database': 'zzdsj'
}

def print_header(title: str):
    """ÊâìÂç∞Ê†áÈ¢ò"""
    print(f"\n{'='*60}")
    print(f"üîó {title}")
    print(f"{'='*60}")

def print_step(step: str, status: str = "INFO"):
    """ÊâìÂç∞Ê≠•È™§‰ø°ÊÅØ"""
    icons = {"INFO": "üìã", "SUCCESS": "‚úÖ", "ERROR": "‚ùå", "WARNING": "‚ö†Ô∏è"}
    icon = icons.get(status, "üìã")
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {icon} {step}")

def test_connection():
    """ÊµãËØïÊï∞ÊçÆÂ∫ìËøûÊé•"""
    print_step("ÊµãËØïËøúÁ®ãPostgreSQLÊï∞ÊçÆÂ∫ìËøûÊé•...")
    
    try:
        # Â∞ùËØïËøûÊé•Êï∞ÊçÆÂ∫ì
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        cursor = conn.cursor()
        
        # ÊâßË°åÁÆÄÂçïÊü•ËØ¢
        cursor.execute("SELECT version();")
        version = cursor.fetchone()[0]
        print_step(f"Êï∞ÊçÆÂ∫ìÁâàÊú¨: {version}", "SUCCESS")
        
        # Ê£ÄÊü•ÂΩìÂâçÊï∞ÊçÆÂ∫ì‰ø°ÊÅØ
        cursor.execute("SELECT current_database(), current_user, current_timestamp;")
        db_info = cursor.fetchone()
        print_step(f"Êï∞ÊçÆÂ∫ì: {db_info[0]}, Áî®Êà∑: {db_info[1]}, Êó∂Èó¥: {db_info[2]}", "SUCCESS")
        
        # Ê£ÄÊü•Êï∞ÊçÆÂ∫ìÊùÉÈôê
        cursor.execute("""
            SELECT datname, has_database_privilege(current_user, datname, 'CREATE') as can_create
            FROM pg_database 
            WHERE datname = current_database();
        """)
        perm_info = cursor.fetchone()
        print_step(f"Êï∞ÊçÆÂ∫ì '{perm_info[0]}' CREATEÊùÉÈôê: {perm_info[1]}", "SUCCESS" if perm_info[1] else "WARNING")
        
        # ÂÖ≥Èó≠ËøûÊé•
        cursor.close()
        conn.close()
        
        print_step("Êï∞ÊçÆÂ∫ìËøûÊé•ÊµãËØïÊàêÂäüÔºÅ", "SUCCESS")
        return True
        
    except psycopg2.Error as e:
        print_step(f"Êï∞ÊçÆÂ∫ìËøûÊé•Â§±Ë¥•: {e}", "ERROR")
        return False
    except Exception as e:
        print_step(f"ËøûÊé•ÊµãËØïÂºÇÂ∏∏: {e}", "ERROR")
        return False

def check_existing_tables():
    """Ê£ÄÊü•Áé∞ÊúâË°®ÁªìÊûÑ"""
    print_step("Ê£ÄÊü•Êï∞ÊçÆÂ∫ìÁé∞ÊúâË°®ÁªìÊûÑ...")
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        cursor = conn.cursor()
        
        # Êü•ËØ¢ÊâÄÊúâË°®
        cursor.execute("""
            SELECT schemaname, tablename, tableowner 
            FROM pg_tables 
            WHERE schemaname = 'public'
            ORDER BY tablename;
        """)
        tables = cursor.fetchall()
        
        if tables:
            print_step(f"ÂèëÁé∞ {len(tables)} ‰∏™Áé∞ÊúâË°®:", "INFO")
            for schema, table, owner in tables:
                print(f"  ‚Ä¢ {table} (owner: {owner})")
        else:
            print_step("Êï∞ÊçÆÂ∫ì‰∏≠ÊöÇÊó†Áî®Êà∑Ë°®", "INFO")
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÊï∞ÊçÆ
        total_rows = 0
        for schema, table, owner in tables:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table};")
                count = cursor.fetchone()[0]
                if count > 0:
                    print_step(f"Ë°® '{table}' ÂåÖÂê´ {count} Ë°åÊï∞ÊçÆ", "INFO")
                    total_rows += count
            except Exception as e:
                print_step(f"Ë°® '{table}' Êü•ËØ¢Â§±Ë¥•: {str(e)[:50]}...", "WARNING")
        
        print_step(f"Êï∞ÊçÆÂ∫ìÊÄªËÆ∞ÂΩïÊï∞: {total_rows}", "INFO")
        
        cursor.close()
        conn.close()
        
        return len(tables)
        
    except psycopg2.Error as e:
        print_step(f"Ê£ÄÊü•Ë°®ÁªìÊûÑÂ§±Ë¥•: {e}", "ERROR")
        return -1

def create_enhanced_document_tables():
    """ÂàõÂª∫Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®ÁªìÊûÑ"""
    print_step("ÂàõÂª∫Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®ÁªìÊûÑ...")
    
    enhanced_tables_sql = """
    -- 1. ÊñáÊ°£Ê≥®ÂÜåË°®ÔºàÂ¢ûÂº∫ÁâàÔºâ
    CREATE TABLE IF NOT EXISTS document_registry_enhanced (
        file_id VARCHAR(36) PRIMARY KEY,
        filename VARCHAR(255) NOT NULL,
        content_type VARCHAR(100),
        file_size BIGINT NOT NULL,
        file_hash VARCHAR(64) NOT NULL,
        storage_backend VARCHAR(50) NOT NULL,
        storage_path VARCHAR(500),
        upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        kb_id VARCHAR(36),
        doc_id VARCHAR(36),
        user_id VARCHAR(36),
        metadata TEXT,
        status VARCHAR(20) DEFAULT 'uploaded',
        processing_status VARCHAR(20) DEFAULT 'pending',
        chunk_count INTEGER DEFAULT 0,
        vector_count INTEGER DEFAULT 0,
        es_doc_count INTEGER DEFAULT 0,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_hash)
    );

    -- 2. ÊñáÊ°£ÂàáÁâáË°®ÔºàchunkÁ∫ßÂà´ÁöÑËøΩË∏™Ôºâ
    CREATE TABLE IF NOT EXISTS document_chunks (
        chunk_id VARCHAR(36) PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry_enhanced(file_id) ON DELETE CASCADE,
        chunk_index INTEGER NOT NULL,
        chunk_text TEXT,
        chunk_size INTEGER,
        chunk_hash VARCHAR(64),
        chunk_metadata TEXT,
        processing_status VARCHAR(20) DEFAULT 'pending',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_id, chunk_index)
    );

    -- 3. ÂêëÈáèÊï∞ÊçÆÂÖ≥ËÅîË°®ÔºàÂ¢ûÂº∫ÁâàÔºâ
    CREATE TABLE IF NOT EXISTS document_vectors_enhanced (
        id SERIAL PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry_enhanced(file_id) ON DELETE CASCADE,
        chunk_id VARCHAR(36) REFERENCES document_chunks(chunk_id) ON DELETE CASCADE,
        vector_id VARCHAR(100) NOT NULL,
        vector_collection VARCHAR(100),
        vector_index VARCHAR(100),
        embedding_model VARCHAR(100),
        embedding_dimension INTEGER,
        vector_metadata TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_id, chunk_id, vector_id)
    );

    -- 4. ESÊñáÊ°£ÂàÜÁâáÂÖ≥ËÅîË°®
    CREATE TABLE IF NOT EXISTS document_es_shards (
        id SERIAL PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry_enhanced(file_id) ON DELETE CASCADE,
        chunk_id VARCHAR(36) REFERENCES document_chunks(chunk_id) ON DELETE CASCADE,
        es_index VARCHAR(100) NOT NULL,
        es_doc_id VARCHAR(100) NOT NULL,
        es_shard_id VARCHAR(50),
        es_routing VARCHAR(100),
        es_doc_type VARCHAR(50),
        es_metadata TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(es_index, es_doc_id)
    );

    -- 5. ÊñáÊ°£Â§ÑÁêÜÂéÜÂè≤Ë°®
    CREATE TABLE IF NOT EXISTS document_processing_history (
        id SERIAL PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry_enhanced(file_id) ON DELETE CASCADE,
        operation_type VARCHAR(50) NOT NULL,
        operation_status VARCHAR(20) NOT NULL,
        operation_details TEXT,
        error_message TEXT,
        started_at TIMESTAMP,
        completed_at TIMESTAMP,
        duration_ms INTEGER,
        operated_by VARCHAR(36),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );

    -- ÂàõÂª∫Á¥¢Âºï
    -- ÊñáÊ°£Ê≥®ÂÜåË°®Á¥¢Âºï
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_filename ON document_registry_enhanced(filename);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_hash ON document_registry_enhanced(file_hash);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_kb_id ON document_registry_enhanced(kb_id);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_doc_id ON document_registry_enhanced(doc_id);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_user_id ON document_registry_enhanced(user_id);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_status ON document_registry_enhanced(status);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_proc_status ON document_registry_enhanced(processing_status);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_upload_time ON document_registry_enhanced(upload_time);

    -- ÊñáÊ°£ÂàáÁâáË°®Á¥¢Âºï
    CREATE INDEX IF NOT EXISTS idx_doc_chunks_file_id ON document_chunks(file_id);
    CREATE INDEX IF NOT EXISTS idx_doc_chunks_index ON document_chunks(chunk_index);
    CREATE INDEX IF NOT EXISTS idx_doc_chunks_hash ON document_chunks(chunk_hash);
    CREATE INDEX IF NOT EXISTS idx_doc_chunks_status ON document_chunks(processing_status);

    -- ÂêëÈáèÊï∞ÊçÆË°®Á¥¢Âºï
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_file_id ON document_vectors_enhanced(file_id);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_chunk_id ON document_vectors_enhanced(chunk_id);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_vector_id ON document_vectors_enhanced(vector_id);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_collection ON document_vectors_enhanced(vector_collection);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_index ON document_vectors_enhanced(vector_index);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_model ON document_vectors_enhanced(embedding_model);

    -- ESÂàÜÁâáË°®Á¥¢Âºï
    CREATE INDEX IF NOT EXISTS idx_doc_es_file_id ON document_es_shards(file_id);
    CREATE INDEX IF NOT EXISTS idx_doc_es_chunk_id ON document_es_shards(chunk_id);
    CREATE INDEX IF NOT EXISTS idx_doc_es_index ON document_es_shards(es_index);
    CREATE INDEX IF NOT EXISTS idx_doc_es_doc_id ON document_es_shards(es_doc_id);
    CREATE INDEX IF NOT EXISTS idx_doc_es_shard_id ON document_es_shards(es_shard_id);
    CREATE INDEX IF NOT EXISTS idx_doc_es_routing ON document_es_shards(es_routing);

    -- Â§ÑÁêÜÂéÜÂè≤Ë°®Á¥¢Âºï
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_file_id ON document_processing_history(file_id);
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_op_type ON document_processing_history(operation_type);
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_status ON document_processing_history(operation_status);
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_started ON document_processing_history(started_at);
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_user ON document_processing_history(operated_by);

    -- ÂÖºÂÆπÊóßÁâàÊú¨ÔºöÂàõÂª∫ÂéüÊúâÊñáÊ°£Ë°®ÔºàÂ¶ÇÊûúÈúÄË¶ÅÔºâ
    CREATE TABLE IF NOT EXISTS document_registry (
        file_id VARCHAR(36) PRIMARY KEY,
        filename VARCHAR(255) NOT NULL,
        content_type VARCHAR(100),
        file_size BIGINT NOT NULL,
        file_hash VARCHAR(64) NOT NULL,
        storage_backend VARCHAR(50) NOT NULL,
        storage_path VARCHAR(500),
        upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        kb_id VARCHAR(36),
        doc_id VARCHAR(36),
        metadata TEXT,
        status VARCHAR(20) DEFAULT 'uploaded',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_hash)
    );

    CREATE TABLE IF NOT EXISTS document_vectors (
        id SERIAL PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry(file_id) ON DELETE CASCADE,
        vector_id VARCHAR(100) NOT NULL,
        chunk_id VARCHAR(100),
        vector_collection VARCHAR(100),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_id, vector_id)
    );

    -- ÂàõÂª∫ÂéüÊúâË°®ÁöÑÁ¥¢Âºï
    CREATE INDEX IF NOT EXISTS idx_document_registry_filename ON document_registry(filename);
    CREATE INDEX IF NOT EXISTS idx_document_registry_hash ON document_registry(file_hash);
    CREATE INDEX IF NOT EXISTS idx_document_registry_kb_id ON document_registry(kb_id);
    CREATE INDEX IF NOT EXISTS idx_document_registry_doc_id ON document_registry(doc_id);
    CREATE INDEX IF NOT EXISTS idx_document_registry_status ON document_registry(status);

    CREATE INDEX IF NOT EXISTS idx_document_vectors_file_id ON document_vectors(file_id);
    CREATE INDEX IF NOT EXISTS idx_document_vectors_vector_id ON document_vectors(vector_id);
    CREATE INDEX IF NOT EXISTS idx_document_vectors_collection ON document_vectors(vector_collection);
    """
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        conn.autocommit = True
        cursor = conn.cursor()
        
        print_step("ÊâßË°åÂ¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®ÂàõÂª∫SQL...", "INFO")
        cursor.execute(enhanced_tables_sql)
        
        print_step("Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®ÂàõÂª∫ÊàêÂäüÔºÅ", "SUCCESS")
        
        cursor.close()
        conn.close()
        
        return True
        
    except psycopg2.Error as e:
        print_step(f"ÂàõÂª∫Â¢ûÂº∫ÁâàË°®Â§±Ë¥•: {e}", "ERROR")
        return False

def execute_sql_file(sql_file_path: str, confirm_required: bool = True):
    """ÊâßË°åSQLÊñá‰ª∂"""
    
    if not os.path.exists(sql_file_path):
        print_step(f"SQLÊñá‰ª∂‰∏çÂ≠òÂú®: {sql_file_path}", "ERROR")
        return False
    
    print_step(f"ÂáÜÂ§áÊâßË°åSQLÊñá‰ª∂: {sql_file_path}")
    
    # ËØªÂèñSQLÊñá‰ª∂ÂÜÖÂÆπ
    try:
        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_content = f.read()
        
        print_step(f"SQLÊñá‰ª∂Â§ßÂ∞è: {len(sql_content)} Â≠óÁ¨¶", "INFO")
        
        # ÊòæÁ§∫Êñá‰ª∂ÂâçÂá†Ë°åÈ¢ÑËßà
        lines = sql_content.split('\n')[:10]
        print_step("SQLÊñá‰ª∂È¢ÑËßà:", "INFO")
        for i, line in enumerate(lines, 1):
            if line.strip():
                print(f"  {i:2}: {line}")
        print("  ...")
        
    except Exception as e:
        print_step(f"ËØªÂèñSQLÊñá‰ª∂Â§±Ë¥•: {e}", "ERROR")
        return False
    
    # Á°ÆËÆ§ÊâßË°å
    if confirm_required:
        print_step("Âç≥Â∞ÜÊâßË°åÊï∞ÊçÆÂ∫ìÂàùÂßãÂåñËÑöÊú¨", "WARNING")
        confirmation = input("\nÊòØÂê¶ÁªßÁª≠ÊâßË°åÔºüËøôÂ∞ÜÂàõÂª∫/‰øÆÊîπÊï∞ÊçÆÂ∫ìË°®ÁªìÊûÑ (y/N): ").strip().lower()
        if confirmation not in ['y', 'yes', 'ÊòØ']:
            print_step("Áî®Êà∑ÂèñÊ∂àÊâßË°å", "INFO")
            return False
    
    # ÊâßË°åSQL
    try:
        print_step("ÂºÄÂßãÊâßË°åSQLËÑöÊú¨...", "INFO")
        start_time = time.time()
        
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        conn.autocommit = True  # ÂêØÁî®Ëá™Âä®Êèê‰∫§
        cursor = conn.cursor()
        
        # ÂàÜÂâ≤Âπ∂ÊâßË°åSQLËØ≠Âè•
        # ËøôÈáåÁÆÄÂçïÊåâÂàÜÂè∑ÂàÜÂâ≤ÔºåÂÆûÈôÖÂèØËÉΩÈúÄË¶ÅÊõ¥Â§çÊùÇÁöÑËß£Êûê
        statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]
        
        print_step(f"ÂÖ±Êúâ {len(statements)} ‰∏™SQLËØ≠Âè•ÂæÖÊâßË°å", "INFO")
        
        executed_count = 0
        error_count = 0
        
        for i, statement in enumerate(statements, 1):
            if not statement:
                continue
                
            try:
                cursor.execute(statement)
                executed_count += 1
                
                # ÊØè100‰∏™ËØ≠Âè•Êä•Âëä‰∏ÄÊ¨°ËøõÂ∫¶
                if i % 100 == 0:
                    print_step(f"Â∑≤ÊâßË°å {i}/{len(statements)} ‰∏™ËØ≠Âè•", "INFO")
                    
            except psycopg2.Error as e:
                error_count += 1
                # Âè™ÊòæÁ§∫ÂâçÂá†‰∏™ÈîôËØØÔºåÈÅøÂÖçÂà∑Â±è
                if error_count <= 5:
                    print_step(f"ËØ≠Âè• {i} ÊâßË°åÂ§±Ë¥•: {str(e)[:100]}...", "WARNING")
                elif error_count == 6:
                    print_step("Êõ¥Â§öÈîôËØØÂ∑≤ÁúÅÁï•...", "WARNING")
        
        end_time = time.time()
        duration = end_time - start_time
        
        print_step(f"SQLÊâßË°åÂÆåÊàêÔºÅ", "SUCCESS")
        print_step(f"ÊâßË°åÊó∂Èó¥: {duration:.2f} Áßí", "INFO")
        print_step(f"ÊàêÂäüÊâßË°å: {executed_count} ‰∏™ËØ≠Âè•", "SUCCESS")
        if error_count > 0:
            print_step(f"ÊâßË°åÈîôËØØ: {error_count} ‰∏™ËØ≠Âè•", "WARNING")
        
        cursor.close()
        conn.close()
        
        return error_count == 0
        
    except psycopg2.Error as e:
        print_step(f"ÊâßË°åSQLÊñá‰ª∂Â§±Ë¥•: {e}", "ERROR")
        return False
    except Exception as e:
        print_step(f"ÊâßË°åËøáÁ®ãÂºÇÂ∏∏: {e}", "ERROR")
        return False

def create_test_data():
    """ÂàõÂª∫ÊµãËØïÊï∞ÊçÆ"""
    print_step("ÂàõÂª∫Âü∫Á°ÄÊµãËØïÊï∞ÊçÆ...")
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        cursor = conn.cursor()
        
        # ÂàõÂª∫ÈªòËÆ§ÁÆ°ÁêÜÂëòÁî®Êà∑
        admin_id = str(uuid.uuid4())
        admin_sql = """
            INSERT INTO users (id, username, email, hashed_password, full_name, is_superuser)
            VALUES (%s, 'admin', 'admin@zzdsj.com', '$2b$12$LQv3c1yqBo69SFqjfUmNnuebNZr8cCsVIIuQ1y.U9VC.ExnQd7CtO', 'Á≥ªÁªüÁÆ°ÁêÜÂëò', true)
            ON CONFLICT (username) DO NOTHING;
        """
        cursor.execute(admin_sql, (admin_id,))
        
        # ÂàõÂª∫ÈªòËÆ§ËßíËâ≤
        try:
            role_id = str(uuid.uuid4())
            role_sql = """
                INSERT INTO roles (id, name, description, is_default)
                VALUES (%s, 'admin', 'Á≥ªÁªüÁÆ°ÁêÜÂëòËßíËâ≤', false)
                ON CONFLICT (name) DO NOTHING;
            """
            cursor.execute(role_sql, (role_id,))
        except psycopg2.Error as e:
            print_step(f"ÂàõÂª∫ËßíËâ≤Êó∂Ë∑≥Ëøá: {str(e)[:50]}...", "INFO")
        
        # ÂàõÂª∫Âü∫Á°ÄÊùÉÈôê
        permissions = [
            ('user_management', 'Áî®Êà∑ÁÆ°ÁêÜ', 'ÁÆ°ÁêÜÁ≥ªÁªüÁî®Êà∑'),
            ('knowledge_base_management', 'Áü•ËØÜÂ∫ìÁÆ°ÁêÜ', 'ÁÆ°ÁêÜÁü•ËØÜÂ∫ì'),
            ('system_config', 'Á≥ªÁªüÈÖçÁΩÆ', 'ÈÖçÁΩÆÁ≥ªÁªüÂèÇÊï∞'),
            ('file_management', 'Êñá‰ª∂ÁÆ°ÁêÜ', 'ÁÆ°ÁêÜÊñá‰ª∂‰∏ä‰º†ÂíåÂ≠òÂÇ®'),
            ('document_processing', 'ÊñáÊ°£Â§ÑÁêÜ', 'Â§ÑÁêÜÊñáÊ°£ÂêëÈáèÂåñÂíåÁ¥¢Âºï')
        ]
        
        for code, name, desc in permissions:
            try:
                perm_id = str(uuid.uuid4())
                perm_sql = """
                    INSERT INTO permissions (id, code, name, description)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (code) DO NOTHING;
                """
                cursor.execute(perm_sql, (perm_id, code, name, desc))
            except psycopg2.Error:
                # ÊùÉÈôêË°®ÂèØËÉΩ‰∏çÂ≠òÂú®ÔºåË∑≥Ëøá
                pass
        
        # ÂàõÂª∫ÈÖçÁΩÆÁ±ªÂà´
        categories = [
            ('system', 'Á≥ªÁªüÈÖçÁΩÆ', 'Âü∫Á°ÄÁ≥ªÁªüÈÖçÁΩÆ'),
            ('ai_models', 'AIÊ®°Âûã', 'AIÊ®°ÂûãÁõ∏ÂÖ≥ÈÖçÁΩÆ'),
            ('storage', 'Â≠òÂÇ®ÈÖçÁΩÆ', 'Êñá‰ª∂ÂíåÊï∞ÊçÆÂ≠òÂÇ®ÈÖçÁΩÆ'),
            ('document_processing', 'ÊñáÊ°£Â§ÑÁêÜ', 'ÊñáÊ°£Â§ÑÁêÜÂíåÂêëÈáèÂåñÈÖçÁΩÆ')
        ]
        
        for code, name, desc in categories:
            try:
                cat_id = str(uuid.uuid4())
                cat_sql = """
                    INSERT INTO config_categories (id, name, description, is_system)
                    VALUES (%s, %s, %s, true)
                    ON CONFLICT (name) DO NOTHING;
                """
                cursor.execute(cat_sql, (cat_id, name, desc))
            except psycopg2.Error:
                # ÈÖçÁΩÆÁ±ªÂà´Ë°®ÂèØËÉΩ‰∏çÂ≠òÂú®ÔºåË∑≥Ëøá
                pass
        
        # ÂàõÂª∫ÈªòËÆ§Áü•ËØÜÂ∫ì
        try:
            kb_sql = """
                INSERT INTO knowledge_bases (name, description, type)
                VALUES ('ÈªòËÆ§Áü•ËØÜÂ∫ì', 'Á≥ªÁªüÈªòËÆ§Áü•ËØÜÂ∫ì', 'default')
                ON CONFLICT DO NOTHING;
            """
            cursor.execute(kb_sql)
        except psycopg2.Error:
            # Áü•ËØÜÂ∫ìË°®ÂèØËÉΩ‰∏çÂ≠òÂú®ÔºåË∑≥Ëøá
            pass
        
        # ÂàõÂª∫ÊµãËØïÊñáÊ°£Êï∞ÊçÆÔºàÊºîÁ§∫Â¢ûÂº∫ÁâàË°®ÁöÑ‰ΩøÁî®Ôºâ
        try:
            # ÊµãËØïÊñáÊ°£
            test_file_id = str(uuid.uuid4())
            test_doc_sql = """
                INSERT INTO document_registry_enhanced 
                (file_id, filename, content_type, file_size, file_hash, storage_backend, 
                 storage_path, kb_id, doc_id, user_id, metadata, status, processing_status)
                VALUES (%s, 'test_document.pdf', 'application/pdf', 1024000, 
                        'test_hash_123', 'minio', '/test/test_document.pdf', 
                        '1', 'test_doc_1', %s, '{"source": "test", "category": "demo"}', 
                        'uploaded', 'completed')
                ON CONFLICT (file_hash) DO NOTHING;
            """
            cursor.execute(test_doc_sql, (test_file_id, admin_id))
            
            # ÊµãËØïÂàáÁâá
            chunk_id = str(uuid.uuid4())
            chunk_sql = """
                INSERT INTO document_chunks 
                (chunk_id, file_id, chunk_index, chunk_text, chunk_size, chunk_hash, processing_status)
                VALUES (%s, %s, 0, 'ËøôÊòØ‰∏Ä‰∏™ÊµãËØïÊñáÊ°£ÂàáÁâáÁöÑÁ§∫‰æãÂÜÖÂÆπ„ÄÇ', 50, 'chunk_hash_1', 'completed')
                ON CONFLICT (file_id, chunk_index) DO NOTHING;
            """
            cursor.execute(chunk_sql, (chunk_id, test_file_id))
            
            # ÊµãËØïÂêëÈáèÂÖ≥ËÅî
            vector_sql = """
                INSERT INTO document_vectors_enhanced 
                (file_id, chunk_id, vector_id, vector_collection, vector_index, 
                 embedding_model, embedding_dimension)
                VALUES (%s, %s, 'vec_001', 'default_collection', 'default_index', 
                        'text-embedding-ada-002', 1536)
                ON CONFLICT (file_id, chunk_id, vector_id) DO NOTHING;
            """
            cursor.execute(vector_sql, (test_file_id, chunk_id))
            
            # ÊµãËØïESÂàÜÁâáÂÖ≥ËÅî
            es_sql = """
                INSERT INTO document_es_shards 
                (file_id, chunk_id, es_index, es_doc_id, es_doc_type)
                VALUES (%s, %s, 'documents', 'doc_001', 'document')
                ON CONFLICT (es_index, es_doc_id) DO NOTHING;
            """
            cursor.execute(es_sql, (test_file_id, chunk_id))
            
            print_step("ÂàõÂª∫‰∫ÜÊºîÁ§∫Êï∞ÊçÆÔºàÂ¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜÔºâ", "SUCCESS")
            
        except psycopg2.Error as e:
            print_step(f"ÂàõÂª∫ÊºîÁ§∫Êï∞ÊçÆÂ§±Ë¥•ÔºàÂèØËÉΩË°®‰∏çÂ≠òÂú®Ôºâ: {str(e)[:50]}...", "INFO")
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print_step("ÊµãËØïÊï∞ÊçÆÂàõÂª∫ÊàêÂäüÔºÅ", "SUCCESS")
        return True
        
    except psycopg2.Error as e:
        print_step(f"ÂàõÂª∫ÊµãËØïÊï∞ÊçÆÂ§±Ë¥•: {e}", "ERROR")
        return False

def verify_installation():
    """È™åËØÅÂÆâË£ÖÁªìÊûú"""
    print_step("È™åËØÅÊï∞ÊçÆÂ∫ìÂÆâË£ÖÁªìÊûú...")
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        cursor = conn.cursor()
        
        # Ê£ÄÊü•ÂÖ≥ÈîÆË°®ÊòØÂê¶Â≠òÂú®
        required_tables = [
            'users', 'document_registry_enhanced', 'document_chunks', 
            'document_vectors_enhanced', 'document_es_shards', 'document_processing_history'
        ]
        
        # ÂèØÈÄâË°®ÔºàÂéüÊúâÁ≥ªÁªüË°®Ôºâ
        optional_tables = [
            'roles', 'permissions', 'knowledge_bases', 
            'documents', 'assistants', 'system_configs', 'model_providers',
            'document_registry', 'document_vectors'
        ]
        
        missing_tables = []
        existing_tables = []
        optional_existing = []
        
        # Ê£ÄÊü•ÂøÖÈúÄË°®
        for table in required_tables:
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, (table,))
            
            if cursor.fetchone()[0]:
                existing_tables.append(table)
            else:
                missing_tables.append(table)
        
        # Ê£ÄÊü•ÂèØÈÄâË°®
        for table in optional_tables:
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, (table,))
            
            if cursor.fetchone()[0]:
                optional_existing.append(table)
        
        print_step(f"Ê†∏ÂøÉË°®Ê£ÄÊü•: {len(existing_tables)}/{len(required_tables)} Â≠òÂú®", 
                  "SUCCESS" if not missing_tables else "WARNING")
        
        print_step(f"ÂèØÈÄâË°®Ê£ÄÊü•: {len(optional_existing)}/{len(optional_tables)} Â≠òÂú®", "INFO")
        
        if missing_tables:
            print_step(f"Áº∫Â§±Ê†∏ÂøÉË°®: {', '.join(missing_tables)}", "WARNING")
        
        # Ê£ÄÊü•Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®ÁöÑÊï∞ÊçÆ
        if 'document_registry_enhanced' in existing_tables:
            cursor.execute("SELECT COUNT(*) FROM document_registry_enhanced;")
            doc_count = cursor.fetchone()[0]
            print_step(f"Â¢ûÂº∫ÁâàÊñáÊ°£Ê≥®ÂÜåË°®ËÆ∞ÂΩïÊï∞: {doc_count}", "INFO")
            
            if 'document_chunks' in existing_tables:
                cursor.execute("SELECT COUNT(*) FROM document_chunks;")
                chunk_count = cursor.fetchone()[0]
                print_step(f"ÊñáÊ°£ÂàáÁâáËÆ∞ÂΩïÊï∞: {chunk_count}", "INFO")
            
            if 'document_vectors_enhanced' in existing_tables:
                cursor.execute("SELECT COUNT(*) FROM document_vectors_enhanced;")
                vector_count = cursor.fetchone()[0]
                print_step(f"Â¢ûÂº∫ÁâàÂêëÈáèÂÖ≥ËÅîËÆ∞ÂΩïÊï∞: {vector_count}", "INFO")
            
            if 'document_es_shards' in existing_tables:
                cursor.execute("SELECT COUNT(*) FROM document_es_shards;")
                es_count = cursor.fetchone()[0]
                print_step(f"ESÂàÜÁâáÂÖ≥ËÅîËÆ∞ÂΩïÊï∞: {es_count}", "INFO")
        
        # Ê£ÄÊü•Áî®Êà∑Êï∞ÊçÆ
        if 'users' in existing_tables:
            cursor.execute("SELECT COUNT(*) FROM users WHERE is_superuser = true;")
            admin_count = cursor.fetchone()[0]
            print_step(f"ÁÆ°ÁêÜÂëòÁî®Êà∑Êï∞: {admin_count}", "SUCCESS" if admin_count > 0 else "WARNING")
        
        cursor.close()
        conn.close()
        
        success = len(missing_tables) == 0
        print_step("Êï∞ÊçÆÂ∫ìÈ™åËØÅÂÆåÊàêÔºÅ", "SUCCESS" if success else "WARNING")
        return success
        
    except psycopg2.Error as e:
        print_step(f"È™åËØÅÂÆâË£ÖÂ§±Ë¥•: {e}", "ERROR")
        return False

@dataclass
class DatabaseHealthReport:
    """Êï∞ÊçÆÂ∫ìÂÅ•Â∫∑Êä•Âëä"""
    timestamp: str
    connection_status: str
    performance_metrics: Dict
    index_analysis: Dict
    query_analysis: Dict
    recommendations: List[str]
    overall_score: int

class DatabaseHealthChecker:
    """Êï∞ÊçÆÂ∫ìÂÅ•Â∫∑Ê£ÄÊü•Âô®"""
    
    def __init__(self, config: Dict):
        self.config = config
    
    def check_performance_metrics(self, cursor) -> Dict:
        """Ê£ÄÊü•ÊÄßËÉΩÊåáÊ†á"""
        metrics = {}
        
        try:
            # 1. Êï∞ÊçÆÂ∫ìÂ§ßÂ∞èÂíåÂ¢ûÈïøË∂ãÂäø
            cursor.execute("""
                SELECT 
                    pg_size_pretty(pg_database_size(current_database())) as db_size,
                    pg_database_size(current_database()) as db_size_bytes
            """)
            size_info = cursor.fetchone()
            metrics['database_size'] = {
                'human_readable': size_info[0],
                'bytes': size_info[1]
            }
            
            # 2. ËøûÊé•ÁªüËÆ°
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_connections,
                    COUNT(*) FILTER (WHERE state = 'active') as active_connections,
                    COUNT(*) FILTER (WHERE state = 'idle') as idle_connections,
                    COUNT(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction
                FROM pg_stat_activity 
                WHERE datname = current_database()
            """)
            conn_stats = cursor.fetchone()
            metrics['connections'] = {
                'total': conn_stats[0],
                'active': conn_stats[1],
                'idle': conn_stats[2],
                'idle_in_transaction': conn_stats[3]
            }
            
            # 3. ÁºìÂ≠òÂëΩ‰∏≠Áéá
            cursor.execute("""
                SELECT 
                    ROUND(100.0 * sum(blks_hit) / NULLIF(sum(blks_hit) + sum(blks_read), 0), 2) as cache_hit_ratio,
                    sum(blks_read) as blocks_read,
                    sum(blks_hit) as blocks_hit
                FROM pg_stat_database 
                WHERE datname = current_database()
            """)
            cache_stats = cursor.fetchone()
            metrics['cache'] = {
                'hit_ratio': cache_stats[0] or 0.0,
                'blocks_read': cache_stats[1] or 0,
                'blocks_hit': cache_stats[2] or 0
            }
            
            # 4. ‰∫ãÂä°ÁªüËÆ°
            cursor.execute("""
                SELECT 
                    xact_commit,
                    xact_rollback,
                    deadlocks,
                    conflicts,
                    temp_files,
                    temp_bytes
                FROM pg_stat_database 
                WHERE datname = current_database()
            """)
            tx_stats = cursor.fetchone()
            metrics['transactions'] = {
                'commits': tx_stats[0] or 0,
                'rollbacks': tx_stats[1] or 0,
                'deadlocks': tx_stats[2] or 0,
                'conflicts': tx_stats[3] or 0,
                'temp_files': tx_stats[4] or 0,
                'temp_bytes': tx_stats[5] or 0
            }
            
            # 5. WALÁªüËÆ°
            cursor.execute("""
                SELECT 
                    pg_wal_lsn_diff(pg_current_wal_lsn(), '0/0') / 1024 / 1024 as wal_size_mb,
                    pg_current_wal_lsn() as current_wal_lsn
            """)
            wal_stats = cursor.fetchone()
            metrics['wal'] = {
                'size_mb': round(wal_stats[0], 2),
                'current_lsn': str(wal_stats[1])
            }
            
        except Exception as e:
            metrics['error'] = str(e)
            
        return metrics
    
    def analyze_indexes(self, cursor) -> Dict:
        """ÂàÜÊûêÁ¥¢Âºï‰ΩøÁî®ÊÉÖÂÜµ"""
        analysis = {}
        
        try:
            # 1. Êú™‰ΩøÁî®ÁöÑÁ¥¢Âºï
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan,
                    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
                FROM pg_stat_user_indexes 
                WHERE idx_scan = 0 
                AND schemaname = 'public'
                ORDER BY pg_relation_size(indexrelid) DESC
            """)
            unused_indexes = cursor.fetchall()
            analysis['unused_indexes'] = [
                {
                    'table': row[1],
                    'index': row[2],
                    'size': row[4]
                }
                for row in unused_indexes
            ]
            
            # 2. ‰ΩéÊïàÁ¥¢Âºï (Êâ´ÊèèÊ¨°Êï∞Â∞ë‰ΩÜÂç†Áî®Á©∫Èó¥Â§ß)
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan,
                    idx_tup_read,
                    idx_tup_fetch,
                    pg_size_pretty(pg_relation_size(indexrelid)) as index_size,
                    pg_relation_size(indexrelid) as index_bytes
                FROM pg_stat_user_indexes 
                WHERE schemaname = 'public'
                AND pg_relation_size(indexrelid) > 1024 * 1024  -- Â§ß‰∫é1MB
                AND idx_scan < 100  -- Êâ´ÊèèÊ¨°Êï∞Â∞ë‰∫é100Ê¨°
                ORDER BY pg_relation_size(indexrelid) DESC
            """)
            inefficient_indexes = cursor.fetchall()
            analysis['inefficient_indexes'] = [
                {
                    'table': row[1],
                    'index': row[2],
                    'scan_count': row[3],
                    'size': row[6],
                    'efficiency_score': round((row[3] or 0) / max(row[7] / (1024*1024), 1), 2)
                }
                for row in inefficient_indexes
            ]
            
            # 3. Áº∫Â§±Á¥¢ÂºïÂª∫ËÆÆ (Âü∫‰∫éÊü•ËØ¢Ê®°Âºè)
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    seq_scan,
                    seq_tup_read,
                    idx_scan,
                    idx_tup_fetch,
                    CASE 
                        WHEN seq_scan > 0 THEN seq_tup_read / seq_scan 
                        ELSE 0 
                    END as avg_seq_read
                FROM pg_stat_user_tables 
                WHERE schemaname = 'public'
                AND seq_scan > idx_scan  -- È°∫Â∫èÊâ´ÊèèÂ§ö‰∫éÁ¥¢ÂºïÊâ´Êèè
                ORDER BY seq_tup_read DESC
            """)
            seq_scan_heavy = cursor.fetchall()
            analysis['missing_index_candidates'] = [
                {
                    'table': row[1],
                    'seq_scans': row[2],
                    'seq_reads': row[3],
                    'avg_read_per_scan': round(row[6], 2),
                    'priority': 'HIGH' if row[6] > 1000 else 'MEDIUM'
                }
                for row in seq_scan_heavy if row[6] > 100
            ]
            
        except Exception as e:
            analysis['error'] = str(e)
            
        return analysis
    
    def analyze_queries(self, cursor) -> Dict:
        """ÂàÜÊûêÊü•ËØ¢ÊÄßËÉΩ"""
        analysis = {}
        
        try:
            # Ê£ÄÊü•ÊòØÂê¶ÂêØÁî®‰∫Üpg_stat_statements
            cursor.execute("""
                SELECT EXISTS (
                    SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements'
                );
            """)
            has_pg_stat_statements = cursor.fetchone()[0]
            
            if has_pg_stat_statements:
                # ÊúÄËÄóÊó∂ÁöÑÊü•ËØ¢
                cursor.execute("""
                    SELECT 
                        LEFT(query, 100) as query_snippet,
                        calls,
                        total_exec_time,
                        mean_exec_time,
                        max_exec_time,
                        stddev_exec_time
                    FROM pg_stat_statements 
                    WHERE query NOT LIKE '%pg_stat_statements%'
                    ORDER BY total_exec_time DESC 
                    LIMIT 10
                """)
                slow_queries = cursor.fetchall()
                analysis['slow_queries'] = [
                    {
                        'query': row[0],
                        'calls': row[1],
                        'total_time': round(row[2], 2),
                        'mean_time': round(row[3], 2),
                        'max_time': round(row[4], 2)
                    }
                    for row in slow_queries
                ]
                
                # Ë∞ÉÁî®È¢ëÁéáÊúÄÈ´òÁöÑÊü•ËØ¢
                cursor.execute("""
                    SELECT 
                        LEFT(query, 100) as query_snippet,
                        calls,
                        total_exec_time,
                        mean_exec_time
                    FROM pg_stat_statements 
                    WHERE query NOT LIKE '%pg_stat_statements%'
                    ORDER BY calls DESC 
                    LIMIT 10
                """)
                frequent_queries = cursor.fetchall()
                analysis['frequent_queries'] = [
                    {
                        'query': row[0],
                        'calls': row[1],
                        'total_time': round(row[2], 2),
                        'mean_time': round(row[3], 2)
                    }
                    for row in frequent_queries
                ]
            else:
                analysis['pg_stat_statements'] = 'not_enabled'
                analysis['recommendation'] = 'Enable pg_stat_statements extension for query analysis'
            
            # ÂΩìÂâçÊ¥ªË∑ÉÁöÑÈïøÊó∂Èó¥ËøêË°åÊü•ËØ¢
            cursor.execute("""
                SELECT 
                    pid,
                    now() - query_start as duration,
                    state,
                    LEFT(query, 100) as query_snippet
                FROM pg_stat_activity 
                WHERE state = 'active' 
                AND query_start < now() - interval '30 seconds'
                AND query NOT LIKE '%pg_stat_activity%'
                ORDER BY query_start ASC
            """)
            long_running = cursor.fetchall()
            analysis['long_running_queries'] = [
                {
                    'pid': row[0],
                    'duration': str(row[1]),
                    'state': row[2],
                    'query': row[3]
                }
                for row in long_running
            ]
            
        except Exception as e:
            analysis['error'] = str(e)
            
        return analysis
    
    def generate_recommendations(self, metrics: Dict, index_analysis: Dict, query_analysis: Dict) -> List[str]:
        """ÁîüÊàê‰ºòÂåñÂª∫ËÆÆ"""
        recommendations = []
        
        # ÊÄßËÉΩÁõ∏ÂÖ≥Âª∫ËÆÆ
        if 'cache' in metrics:
            cache_hit_ratio = metrics['cache']['hit_ratio']
            if cache_hit_ratio < 90:
                recommendations.append(f"‚ö†Ô∏è ÁºìÂ≠òÂëΩ‰∏≠ÁéáËæÉ‰Ωé ({cache_hit_ratio}%)ÔºåÂª∫ËÆÆË∞ÉÊï¥shared_buffersÂèÇÊï∞")
        
        # ËøûÊé•Áõ∏ÂÖ≥Âª∫ËÆÆ
        if 'connections' in metrics:
            idle_in_tx = metrics['connections']['idle_in_transaction']
            if idle_in_tx > 5:
                recommendations.append(f"‚ö†Ô∏è ÂèëÁé∞ {idle_in_tx} ‰∏™Á©∫Èó≤‰∫ãÂä°ËøûÊé•ÔºåÂèØËÉΩÂ≠òÂú®ËøûÊé•Ê≥ÑÊºè")
        
        # Á¥¢ÂºïÁõ∏ÂÖ≥Âª∫ËÆÆ
        if 'unused_indexes' in index_analysis:
            unused_count = len(index_analysis['unused_indexes'])
            if unused_count > 0:
                recommendations.append(f"üí° ÂèëÁé∞ {unused_count} ‰∏™Êú™‰ΩøÁî®ÁöÑÁ¥¢ÂºïÔºåÂª∫ËÆÆÊ∏ÖÁêÜ‰ª•ËäÇÁúÅÁ©∫Èó¥")
        
        if 'missing_index_candidates' in index_analysis:
            missing_count = len(index_analysis['missing_index_candidates'])
            if missing_count > 0:
                recommendations.append(f"üí° ÂèëÁé∞ {missing_count} ‰∏™Ë°®ÂèØËÉΩÈúÄË¶ÅÊ∑ªÂä†Á¥¢Âºï‰ª•ÊèêÈ´òÊü•ËØ¢ÊÄßËÉΩ")
        
        # Êü•ËØ¢Áõ∏ÂÖ≥Âª∫ËÆÆ
        if 'long_running_queries' in query_analysis:
            long_count = len(query_analysis['long_running_queries'])
            if long_count > 0:
                recommendations.append(f"‚ö†Ô∏è ÂèëÁé∞ {long_count} ‰∏™ÈïøÊó∂Èó¥ËøêË°åÁöÑÊü•ËØ¢ÔºåÂª∫ËÆÆÊ£ÄÊü•Âíå‰ºòÂåñ")
        
        if query_analysis.get('pg_stat_statements') == 'not_enabled':
            recommendations.append("üí° Âª∫ËÆÆÂêØÁî®pg_stat_statementsÊâ©Â±ï‰ª•Ëé∑ÂæóÊõ¥ËØ¶ÁªÜÁöÑÊü•ËØ¢ÂàÜÊûê")
        
        # WALÁõ∏ÂÖ≥Âª∫ËÆÆ
        if 'wal' in metrics:
            wal_size = metrics['wal']['size_mb']
            if wal_size > 1000:  # Â§ß‰∫é1GB
                recommendations.append(f"‚ö†Ô∏è WALÂ§ßÂ∞èËæÉÂ§ß ({wal_size} MB)ÔºåÂª∫ËÆÆÊ£ÄÊü•checkpointÈÖçÁΩÆ")
        
        if not recommendations:
            recommendations.append("‚úÖ Êï∞ÊçÆÂ∫ìÁä∂ÊÄÅËâØÂ•ΩÔºåÊú™ÂèëÁé∞ÊòéÊòæÁöÑÊÄßËÉΩÈóÆÈ¢ò")
        
        return recommendations
    
    def calculate_health_score(self, metrics: Dict, index_analysis: Dict, query_analysis: Dict) -> int:
        """ËÆ°ÁÆóÂÅ•Â∫∑ËØÑÂàÜ (0-100)"""
        score = 100
        
        # ÁºìÂ≠òÂëΩ‰∏≠ÁéáÂΩ±Âìç (ÊúÄÂ§öÊâ£30ÂàÜ)
        if 'cache' in metrics:
            cache_ratio = metrics['cache']['hit_ratio']
            if cache_ratio < 95:
                score -= min(30, (95 - cache_ratio) * 2)
        
        # Á©∫Èó≤‰∫ãÂä°ËøûÊé•ÂΩ±Âìç (ÊØè‰∏™Êâ£5ÂàÜ)
        if 'connections' in metrics:
            idle_in_tx = metrics['connections']['idle_in_transaction']
            score -= min(25, idle_in_tx * 5)
        
        # Êú™‰ΩøÁî®Á¥¢ÂºïÂΩ±Âìç (ÊØè‰∏™Êâ£2ÂàÜ)
        if 'unused_indexes' in index_analysis:
            unused_count = len(index_analysis['unused_indexes'])
            score -= min(20, unused_count * 2)
        
        # ÈïøÊó∂Èó¥ËøêË°åÊü•ËØ¢ÂΩ±Âìç (ÊØè‰∏™Êâ£3ÂàÜ)
        if 'long_running_queries' in query_analysis:
            long_count = len(query_analysis['long_running_queries'])
            score -= min(15, long_count * 3)
        
        return max(0, score)
    
    def run_health_check(self) -> DatabaseHealthReport:
        """ÊâßË°åÂÆåÊï¥ÁöÑÂÅ•Â∫∑Ê£ÄÊü•"""
        print_step("ÂºÄÂßãÊï∞ÊçÆÂ∫ìÂÅ•Â∫∑Ê£ÄÊü•...", "INFO")
        
        try:
            conn = psycopg2.connect(**self.config)
            cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
            
            # Ê£ÄÊü•ÂêÑÈ°πÊåáÊ†á
            performance_metrics = self.check_performance_metrics(cursor)
            index_analysis = self.analyze_indexes(cursor)
            query_analysis = self.analyze_queries(cursor)
            
            # ÁîüÊàêÂª∫ËÆÆ
            recommendations = self.generate_recommendations(
                performance_metrics, index_analysis, query_analysis
            )
            
            # ËÆ°ÁÆóÂÅ•Â∫∑ËØÑÂàÜ
            health_score = self.calculate_health_score(
                performance_metrics, index_analysis, query_analysis
            )
            
            cursor.close()
            conn.close()
            
            # ÂàõÂª∫ÂÅ•Â∫∑Êä•Âëä
            report = DatabaseHealthReport(
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                connection_status="connected",
                performance_metrics=performance_metrics,
                index_analysis=index_analysis,
                query_analysis=query_analysis,
                recommendations=recommendations,
                overall_score=health_score
            )
            
            return report
            
        except Exception as e:
            print_step(f"ÂÅ•Â∫∑Ê£ÄÊü•Â§±Ë¥•: {e}", "ERROR")
            return DatabaseHealthReport(
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                connection_status="failed",
                performance_metrics={},
                index_analysis={},
                query_analysis={},
                recommendations=[f"ËøûÊé•Â§±Ë¥•: {e}"],
                overall_score=0
            )

def print_health_report(report: DatabaseHealthReport):
    """ÊâìÂç∞ÂÅ•Â∫∑Ê£ÄÊü•Êä•Âëä"""
    print_header("Êï∞ÊçÆÂ∫ìÂÅ•Â∫∑Ê£ÄÊü•Êä•Âëä")
    
    # Êï¥‰ΩìËØÑÂàÜ
    score_color = "SUCCESS" if report.overall_score >= 80 else "WARNING" if report.overall_score >= 60 else "ERROR"
    print_step(f"Êï¥‰ΩìÂÅ•Â∫∑ËØÑÂàÜ: {report.overall_score}/100", score_color)
    
    # ÊÄßËÉΩÊåáÊ†á
    if report.performance_metrics:
        print_step("ÊÄßËÉΩÊåáÊ†á:", "INFO")
        metrics = report.performance_metrics
        
        if 'database_size' in metrics:
            print(f"  üìä Êï∞ÊçÆÂ∫ìÂ§ßÂ∞è: {metrics['database_size']['human_readable']}")
        
        if 'connections' in metrics:
            conn = metrics['connections']
            print(f"  üîó ËøûÊé•ÁªüËÆ°: ÊÄªËÆ° {conn['total']}, Ê¥ªË∑É {conn['active']}, Á©∫Èó≤ {conn['idle']}")
            if conn['idle_in_transaction'] > 0:
                print(f"  ‚ö†Ô∏è  Á©∫Èó≤‰∫ãÂä°: {conn['idle_in_transaction']} ‰∏™")
        
        if 'cache' in metrics:
            print(f"  üíæ ÁºìÂ≠òÂëΩ‰∏≠Áéá: {metrics['cache']['hit_ratio']}%")
        
        if 'transactions' in metrics:
            tx = metrics['transactions']
            if tx['deadlocks'] > 0:
                print(f"  üîí Ê≠ªÈîÅÊï∞: {tx['deadlocks']}")
    
    # Á¥¢ÂºïÂàÜÊûê
    if report.index_analysis:
        print_step("Á¥¢ÂºïÂàÜÊûê:", "INFO")
        
        unused = report.index_analysis.get('unused_indexes', [])
        if unused:
            print(f"  üóëÔ∏è  Êú™‰ΩøÁî®Á¥¢Âºï: {len(unused)} ‰∏™")
            for idx in unused[:3]:  # Âè™ÊòæÁ§∫Ââç3‰∏™
                print(f"    ‚Ä¢ {idx['table']}.{idx['index']} ({idx['size']})")
        
        missing = report.index_analysis.get('missing_index_candidates', [])
        if missing:
            print(f"  üí° Âª∫ËÆÆÊ∑ªÂä†Á¥¢ÂºïÁöÑË°®: {len(missing)} ‰∏™")
            for table in missing[:3]:
                print(f"    ‚Ä¢ {table['table']} (‰ºòÂÖàÁ∫ß: {table['priority']})")
    
    # Êü•ËØ¢ÂàÜÊûê
    if report.query_analysis:
        print_step("Êü•ËØ¢ÂàÜÊûê:", "INFO")
        
        long_running = report.query_analysis.get('long_running_queries', [])
        if long_running:
            print(f"  üêå ÈïøÊó∂Èó¥ËøêË°åÊü•ËØ¢: {len(long_running)} ‰∏™")
            for query in long_running[:2]:
                print(f"    ‚Ä¢ PID {query['pid']}: {query['duration']} - {query['query'][:50]}...")
        
        if 'slow_queries' in report.query_analysis:
            slow_count = len(report.query_analysis['slow_queries'])
            print(f"  üìà ÊÖ¢Êü•ËØ¢ÁªüËÆ°: {slow_count} Êù°ËÆ∞ÂΩï")
    
    # ‰ºòÂåñÂª∫ËÆÆ
    if report.recommendations:
        print_step("‰ºòÂåñÂª∫ËÆÆ:", "INFO")
        for i, rec in enumerate(report.recommendations, 1):
            print(f"  {i}. {rec}")
    
    print(f"\nüìÖ Êä•ÂëäÊó∂Èó¥: {report.timestamp}")

def system_health_check():
    """ÊâßË°åÁ≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•"""
    checker = DatabaseHealthChecker(REMOTE_DB_CONFIG)
    report = checker.run_health_check()
    
    # ÊâìÂç∞Êä•Âëä
    print_health_report(report)
    
    # ‰øùÂ≠òÊä•ÂëäÂà∞Êñá‰ª∂
    report_dir = Path("health_reports")
    report_dir.mkdir(exist_ok=True)
    
    report_file = report_dir / f"health_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(asdict(report), f, indent=2, ensure_ascii=False)
    
    print_step(f"ÂÅ•Â∫∑Ê£ÄÊü•Êä•ÂëäÂ∑≤‰øùÂ≠òÂà∞: {report_file}", "SUCCESS")
    
    return report.overall_score >= 70

def main():
    """‰∏ªÂáΩÊï∞"""
    parser = argparse.ArgumentParser(description="ËøúÁ®ãPostgreSQLÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂ∑•ÂÖ∑")
    parser.add_argument('--mode', choices=['full', 'test', 'health', 'init'], default='full',
                       help='ËøêË°åÊ®°Âºè: full(ÂÆåÊï¥ÂàùÂßãÂåñ), test(‰ªÖÊµãËØïËøûÊé•), health(ÂÅ•Â∫∑Ê£ÄÊü•), init(‰ªÖÂàùÂßãÂåñ)')
    parser.add_argument('--skip-confirmation', action='store_true',
                       help='Ë∑≥ËøáÁ°ÆËÆ§ÊèêÁ§∫')
    
    args = parser.parse_args()
    
    if args.mode == 'test':
        # ‰ªÖÊµãËØïËøûÊé•
        print_header("Êï∞ÊçÆÂ∫ìËøûÊé•ÊµãËØï")
        return test_connection()
    
    elif args.mode == 'health':
        # ‰ªÖÊâßË°åÂÅ•Â∫∑Ê£ÄÊü•
        print_header("Êï∞ÊçÆÂ∫ìÂÅ•Â∫∑Ê£ÄÊü•")
        return system_health_check()
    
    elif args.mode == 'init':
        # ‰ªÖÊâßË°åÂàùÂßãÂåñ
        print_header("Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñ")
        
        # ÊµãËØïËøûÊé•
        if not test_connection():
            print_step("ËøûÊé•ÊµãËØïÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü•ÁΩëÁªúÂíåÈÖçÁΩÆ", "ERROR")
            return False
        
        # ÂàõÂª∫Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®
        if not create_enhanced_document_tables():
            print_step("Â¢ûÂº∫ÁâàË°®ÂàõÂª∫Â§±Ë¥•", "ERROR")
            return False
        
        # ÊâßË°åÊï∞ÊçÆÂ∫ìÂàùÂßãÂåñ
        sql_file_path = "database_complete.sql"
        if os.path.exists(sql_file_path):
            if not execute_sql_file(sql_file_path, confirm_required=not args.skip_confirmation):
                print_step("Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñÂ§±Ë¥•", "WARNING")
        
        return True
    
    else:  # full mode
        print_header("ËøúÁ®ãPostgreSQLÊï∞ÊçÆÂ∫ìËøûÊé•ÊµãËØïÂíåÂ¢ûÂº∫ÁâàÂàùÂßãÂåñ")
        
        print("üéØ ÁõÆÊ†áÊúçÂä°Âô®‰ø°ÊÅØ:")
        print(f"  üìç Âú∞ÂùÄ: {REMOTE_DB_CONFIG['host']}:{REMOTE_DB_CONFIG['port']}")
        print(f"  üë§ Áî®Êà∑: {REMOTE_DB_CONFIG['user']}")
        print(f"  üóÑÔ∏è  Êï∞ÊçÆÂ∫ì: {REMOTE_DB_CONFIG['database']}")
        print("\nüîß Â¢ûÂº∫ÁâàÂäüËÉΩ:")
        print("  ‚Ä¢ ÂÆåÊï¥ÁöÑÂêëÈáèchunk IDËøΩË∏™")
        print("  ‚Ä¢ ESÊñáÊ°£ÂàÜÁâáÊï∞ÊçÆÂÖ≥ËÅî")
        print("  ‚Ä¢ Áªü‰∏ÄÂà†Èô§Êìç‰ΩúÊîØÊåÅ")
        print("  ‚Ä¢ ËØ¶ÁªÜÁöÑÂ§ÑÁêÜÂéÜÂè≤ËÆ∞ÂΩï")
        print("  ‚Ä¢ Á≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•ÂíåÊÄßËÉΩÁõëÊéß")
        print("  ‚Ä¢ Á¥¢Âºï‰ΩøÁî®ÂàÜÊûêÂíå‰ºòÂåñÂª∫ËÆÆ")
        
        # Ê≠•È™§1: ÊµãËØïËøûÊé•
        if not test_connection():
            print_step("ËøûÊé•ÊµãËØïÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü•ÁΩëÁªúÂíåÈÖçÁΩÆ", "ERROR")
            return False
        
        # Ê≠•È™§2: Ê£ÄÊü•Áé∞ÊúâË°®
        table_count = check_existing_tables()
        if table_count == -1:
            print_step("Êó†Ê≥ïÊ£ÄÊü•Áé∞ÊúâË°®ÁªìÊûÑ", "ERROR")
            return False
        
        # Ê≠•È™§3: ÂàõÂª∫Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®
        print_step("ÂºÄÂßãÂàõÂª∫Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜË°®ÁªìÊûÑ...", "INFO")
        if not create_enhanced_document_tables():
            print_step("Â¢ûÂº∫ÁâàË°®ÂàõÂª∫Â§±Ë¥•", "ERROR")
            return False
        
        # Ê≠•È™§4: ÊâßË°åÊï∞ÊçÆÂ∫ìÂàùÂßãÂåñÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ
        sql_file_path = "database_complete.sql"
        if os.path.exists(sql_file_path):
            print_step("ÂèëÁé∞Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñÊñá‰ª∂ÔºåÂáÜÂ§áÊâßË°å...", "INFO")
            if not execute_sql_file(sql_file_path, confirm_required=not args.skip_confirmation):
                print_step("Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñÂ§±Ë¥•", "WARNING")
        else:
            print_step(f"Êú™ÊâæÂà∞Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñÊñá‰ª∂: {sql_file_path} (Ë∑≥Ëøá)", "INFO")
        
        # Ê≠•È™§5: ÂàõÂª∫ÊµãËØïÊï∞ÊçÆ
        if not create_test_data():
            print_step("ÂàõÂª∫ÊµãËØïÊï∞ÊçÆÂ§±Ë¥•", "WARNING")
        
        # Ê≠•È™§6: È™åËØÅÂÆâË£Ö
        if not verify_installation():
            print_step("Êï∞ÊçÆÂ∫ìÈ™åËØÅÂ§±Ë¥•", "WARNING")
            return False
        
        # Ê≠•È™§7: ÊâßË°åÁ≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•
        if not system_health_check():
            print_step("Á≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•Â§±Ë¥•", "WARNING")
            return False
        
        # ÊàêÂäüÂÆåÊàê
        print_header("Â¢ûÂº∫ÁâàÊï∞ÊçÆÂ∫ìÂàùÂßãÂåñÂÆåÊàê")
    print_step("üéâ ËøúÁ®ãPostgreSQLÊï∞ÊçÆÂ∫ìÂ¢ûÂº∫ÁâàÂàùÂßãÂåñÊàêÂäüÔºÅ", "SUCCESS")
    print_step("üìã Êñ∞Â¢ûÂäüËÉΩ:", "INFO")
    print("  ‚úÖ ÂÆåÊï¥ÁöÑÊñáÊ°£chunkËøΩË∏™")
    print("  ‚úÖ ÂêëÈáèÊï∞ÊçÆÁ≤æÁ°ÆÂÖ≥ËÅî")
    print("  ‚úÖ ESÊñáÊ°£ÂàÜÁâáÂÖ≥ËÅî")
    print("  ‚úÖ Áªü‰∏ÄÂà†Èô§Êìç‰ΩúÊîØÊåÅ")
    print("  ‚úÖ ËØ¶ÁªÜÁöÑÊìç‰ΩúÂéÜÂè≤ËÆ∞ÂΩï")
    
    print_step("üîß ÂêéÁª≠Ê≠•È™§:", "INFO")
    print("  1. Êõ¥Êñ∞Â∫îÁî®Á®ãÂ∫è‰ΩøÁî®Â¢ûÂº∫ÁâàÊñáÊ°£ÁÆ°ÁêÜÂô®")
    print("  2. ÈÖçÁΩÆESÂíåÂêëÈáèÊï∞ÊçÆÂ∫ìËøûÊé•")
    print("  3. ÊµãËØïÁªü‰∏ÄÂà†Èô§ÂäüËÉΩ")
    print("  4. ËøÅÁßªÁé∞ÊúâÊñáÊ°£Êï∞ÊçÆÔºàÂ¶ÇÈúÄË¶ÅÔºâ")
    
    print_step("üîë ÈªòËÆ§ÁÆ°ÁêÜÂëòË¥¶Êà∑:", "INFO")
    print("  Áî®Êà∑Âêç: admin")
    print("  ÈÇÆÁÆ±: admin@zzdsj.com")
    print("  ÂØÜÁ†Å: admin123 (ËØ∑ÂèäÊó∂‰øÆÊîπ)")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print_step("\nÁî®Êà∑‰∏≠Êñ≠Êìç‰Ωú", "INFO")
        sys.exit(1)
    except Exception as e:
        print_step(f"Á®ãÂ∫èÂºÇÂ∏∏: {e}", "ERROR")
        sys.exit(1) 