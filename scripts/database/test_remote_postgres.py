#!/usr/bin/env python3
"""
è¿œç¨‹PostgreSQLæ•°æ®åº“è¿æ¥æµ‹è¯•å’Œåˆå§‹åŒ–è„šæœ¬ - å¢å¼ºç‰ˆ
æµ‹è¯•è¿æ¥åˆ°è¿œç¨‹æœåŠ¡å™¨å¹¶æ‰§è¡Œå®Œæ•´çš„æ•°æ®åº“åˆå§‹åŒ–
åŒ…å«å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨ç»“æ„ï¼Œæ”¯æŒå‘é‡chunk IDã€æ–‡æ¡£IDå’ŒESåˆ†ç‰‡æ•°æ®çš„å®Œæ•´å…³è”è¿½è¸ª
æ–°å¢ç³»ç»Ÿå¥åº·æ£€æŸ¥ã€æ€§èƒ½ç›‘æ§ã€ç´¢å¼•åˆ†æç­‰é«˜çº§åŠŸèƒ½
"""

import psycopg2
import psycopg2.extras
import os
import sys
from pathlib import Path
import time
from datetime import datetime, timedelta
import uuid
import json
import argparse
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

# è¿œç¨‹æ•°æ®åº“è¿æ¥é…ç½®
REMOTE_DB_CONFIG = {
    'host': '167.71.85.231',
    'port': 5432,
    'user': 'zzdsj',
    'password': 'zzdsj123',
    'database': 'zzdsj'
}

def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f"ğŸ”— {title}")
    print(f"{'='*60}")

def print_step(step: str, status: str = "INFO"):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    icons = {"INFO": "ğŸ“‹", "SUCCESS": "âœ…", "ERROR": "âŒ", "WARNING": "âš ï¸"}
    icon = icons.get(status, "ğŸ“‹")
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {icon} {step}")

def test_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    print_step("æµ‹è¯•è¿œç¨‹PostgreSQLæ•°æ®åº“è¿æ¥...")
    
    try:
        # å°è¯•è¿æ¥æ•°æ®åº“
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        cursor = conn.cursor()
        
        # æ‰§è¡Œç®€å•æŸ¥è¯¢
        cursor.execute("SELECT version();")
        version = cursor.fetchone()[0]
        print_step(f"æ•°æ®åº“ç‰ˆæœ¬: {version}", "SUCCESS")
        
        # æ£€æŸ¥å½“å‰æ•°æ®åº“ä¿¡æ¯
        cursor.execute("SELECT current_database(), current_user, current_timestamp;")
        db_info = cursor.fetchone()
        print_step(f"æ•°æ®åº“: {db_info[0]}, ç”¨æˆ·: {db_info[1]}, æ—¶é—´: {db_info[2]}", "SUCCESS")
        
        # æ£€æŸ¥æ•°æ®åº“æƒé™
        cursor.execute("""
            SELECT datname, has_database_privilege(current_user, datname, 'CREATE') as can_create
            FROM pg_database 
            WHERE datname = current_database();
        """)
        perm_info = cursor.fetchone()
        print_step(f"æ•°æ®åº“ '{perm_info[0]}' CREATEæƒé™: {perm_info[1]}", "SUCCESS" if perm_info[1] else "WARNING")
        
        # å…³é—­è¿æ¥
        cursor.close()
        conn.close()
        
        print_step("æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸï¼", "SUCCESS")
        return True
        
    except psycopg2.Error as e:
        print_step(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}", "ERROR")
        return False
    except Exception as e:
        print_step(f"è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}", "ERROR")
        return False

def check_existing_tables():
    """æ£€æŸ¥ç°æœ‰è¡¨ç»“æ„"""
    print_step("æ£€æŸ¥æ•°æ®åº“ç°æœ‰è¡¨ç»“æ„...")
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        cursor = conn.cursor()
        
        # æŸ¥è¯¢æ‰€æœ‰è¡¨
        cursor.execute("""
            SELECT schemaname, tablename, tableowner 
            FROM pg_tables 
            WHERE schemaname = 'public'
            ORDER BY tablename;
        """)
        tables = cursor.fetchall()
        
        if tables:
            print_step(f"å‘ç° {len(tables)} ä¸ªç°æœ‰è¡¨:", "INFO")
            for schema, table, owner in tables:
                print(f"  â€¢ {table} (owner: {owner})")
        else:
            print_step("æ•°æ®åº“ä¸­æš‚æ— ç”¨æˆ·è¡¨", "INFO")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        total_rows = 0
        for schema, table, owner in tables:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table};")
                count = cursor.fetchone()[0]
                if count > 0:
                    print_step(f"è¡¨ '{table}' åŒ…å« {count} è¡Œæ•°æ®", "INFO")
                    total_rows += count
            except Exception as e:
                print_step(f"è¡¨ '{table}' æŸ¥è¯¢å¤±è´¥: {str(e)[:50]}...", "WARNING")
        
        print_step(f"æ•°æ®åº“æ€»è®°å½•æ•°: {total_rows}", "INFO")
        
        cursor.close()
        conn.close()
        
        return len(tables)
        
    except psycopg2.Error as e:
        print_step(f"æ£€æŸ¥è¡¨ç»“æ„å¤±è´¥: {e}", "ERROR")
        return -1

def create_enhanced_document_tables():
    """åˆ›å»ºå¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨ç»“æ„"""
    print_step("åˆ›å»ºå¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨ç»“æ„...")
    
    enhanced_tables_sql = """
    -- 1. æ–‡æ¡£æ³¨å†Œè¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰
    CREATE TABLE IF NOT EXISTS document_registry_enhanced (
        file_id VARCHAR(36) PRIMARY KEY,
        filename VARCHAR(255) NOT NULL,
        content_type VARCHAR(100),
        file_size BIGINT NOT NULL,
        file_hash VARCHAR(64) NOT NULL,
        storage_backend VARCHAR(50) NOT NULL,
        storage_path VARCHAR(500),
        upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        kb_id VARCHAR(36),
        doc_id VARCHAR(36),
        user_id VARCHAR(36),
        metadata TEXT,
        status VARCHAR(20) DEFAULT 'uploaded',
        processing_status VARCHAR(20) DEFAULT 'pending',
        chunk_count INTEGER DEFAULT 0,
        vector_count INTEGER DEFAULT 0,
        es_doc_count INTEGER DEFAULT 0,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_hash)
    );

    -- 2. æ–‡æ¡£åˆ‡ç‰‡è¡¨ï¼ˆchunkçº§åˆ«çš„è¿½è¸ªï¼‰
    CREATE TABLE IF NOT EXISTS document_chunks (
        chunk_id VARCHAR(36) PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry_enhanced(file_id) ON DELETE CASCADE,
        chunk_index INTEGER NOT NULL,
        chunk_text TEXT,
        chunk_size INTEGER,
        chunk_hash VARCHAR(64),
        chunk_metadata TEXT,
        processing_status VARCHAR(20) DEFAULT 'pending',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_id, chunk_index)
    );

    -- 3. å‘é‡æ•°æ®å…³è”è¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰
    CREATE TABLE IF NOT EXISTS document_vectors_enhanced (
        id SERIAL PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry_enhanced(file_id) ON DELETE CASCADE,
        chunk_id VARCHAR(36) REFERENCES document_chunks(chunk_id) ON DELETE CASCADE,
        vector_id VARCHAR(100) NOT NULL,
        vector_collection VARCHAR(100),
        vector_index VARCHAR(100),
        embedding_model VARCHAR(100),
        embedding_dimension INTEGER,
        vector_metadata TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_id, chunk_id, vector_id)
    );

    -- 4. ESæ–‡æ¡£åˆ†ç‰‡å…³è”è¡¨
    CREATE TABLE IF NOT EXISTS document_es_shards (
        id SERIAL PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry_enhanced(file_id) ON DELETE CASCADE,
        chunk_id VARCHAR(36) REFERENCES document_chunks(chunk_id) ON DELETE CASCADE,
        es_index VARCHAR(100) NOT NULL,
        es_doc_id VARCHAR(100) NOT NULL,
        es_shard_id VARCHAR(50),
        es_routing VARCHAR(100),
        es_doc_type VARCHAR(50),
        es_metadata TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(es_index, es_doc_id)
    );

    -- 5. æ–‡æ¡£å¤„ç†å†å²è¡¨
    CREATE TABLE IF NOT EXISTS document_processing_history (
        id SERIAL PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry_enhanced(file_id) ON DELETE CASCADE,
        operation_type VARCHAR(50) NOT NULL,
        operation_status VARCHAR(20) NOT NULL,
        operation_details TEXT,
        error_message TEXT,
        started_at TIMESTAMP,
        completed_at TIMESTAMP,
        duration_ms INTEGER,
        operated_by VARCHAR(36),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );

    -- åˆ›å»ºç´¢å¼•
    -- æ–‡æ¡£æ³¨å†Œè¡¨ç´¢å¼•
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_filename ON document_registry_enhanced(filename);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_hash ON document_registry_enhanced(file_hash);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_kb_id ON document_registry_enhanced(kb_id);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_doc_id ON document_registry_enhanced(doc_id);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_user_id ON document_registry_enhanced(user_id);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_status ON document_registry_enhanced(status);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_proc_status ON document_registry_enhanced(processing_status);
    CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_upload_time ON document_registry_enhanced(upload_time);

    -- æ–‡æ¡£åˆ‡ç‰‡è¡¨ç´¢å¼•
    CREATE INDEX IF NOT EXISTS idx_doc_chunks_file_id ON document_chunks(file_id);
    CREATE INDEX IF NOT EXISTS idx_doc_chunks_index ON document_chunks(chunk_index);
    CREATE INDEX IF NOT EXISTS idx_doc_chunks_hash ON document_chunks(chunk_hash);
    CREATE INDEX IF NOT EXISTS idx_doc_chunks_status ON document_chunks(processing_status);

    -- å‘é‡æ•°æ®è¡¨ç´¢å¼•
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_file_id ON document_vectors_enhanced(file_id);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_chunk_id ON document_vectors_enhanced(chunk_id);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_vector_id ON document_vectors_enhanced(vector_id);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_collection ON document_vectors_enhanced(vector_collection);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_index ON document_vectors_enhanced(vector_index);
    CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_model ON document_vectors_enhanced(embedding_model);

    -- ESåˆ†ç‰‡è¡¨ç´¢å¼•
    CREATE INDEX IF NOT EXISTS idx_doc_es_file_id ON document_es_shards(file_id);
    CREATE INDEX IF NOT EXISTS idx_doc_es_chunk_id ON document_es_shards(chunk_id);
    CREATE INDEX IF NOT EXISTS idx_doc_es_index ON document_es_shards(es_index);
    CREATE INDEX IF NOT EXISTS idx_doc_es_doc_id ON document_es_shards(es_doc_id);
    CREATE INDEX IF NOT EXISTS idx_doc_es_shard_id ON document_es_shards(es_shard_id);
    CREATE INDEX IF NOT EXISTS idx_doc_es_routing ON document_es_shards(es_routing);

    -- å¤„ç†å†å²è¡¨ç´¢å¼•
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_file_id ON document_processing_history(file_id);
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_op_type ON document_processing_history(operation_type);
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_status ON document_processing_history(operation_status);
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_started ON document_processing_history(started_at);
    CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_user ON document_processing_history(operated_by);

    -- å…¼å®¹æ—§ç‰ˆæœ¬ï¼šåˆ›å»ºåŸæœ‰æ–‡æ¡£è¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    CREATE TABLE IF NOT EXISTS document_registry (
        file_id VARCHAR(36) PRIMARY KEY,
        filename VARCHAR(255) NOT NULL,
        content_type VARCHAR(100),
        file_size BIGINT NOT NULL,
        file_hash VARCHAR(64) NOT NULL,
        storage_backend VARCHAR(50) NOT NULL,
        storage_path VARCHAR(500),
        upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        kb_id VARCHAR(36),
        doc_id VARCHAR(36),
        metadata TEXT,
        status VARCHAR(20) DEFAULT 'uploaded',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_hash)
    );

    CREATE TABLE IF NOT EXISTS document_vectors (
        id SERIAL PRIMARY KEY,
        file_id VARCHAR(36) REFERENCES document_registry(file_id) ON DELETE CASCADE,
        vector_id VARCHAR(100) NOT NULL,
        chunk_id VARCHAR(100),
        vector_collection VARCHAR(100),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_id, vector_id)
    );

    -- åˆ›å»ºåŸæœ‰è¡¨çš„ç´¢å¼•
    CREATE INDEX IF NOT EXISTS idx_document_registry_filename ON document_registry(filename);
    CREATE INDEX IF NOT EXISTS idx_document_registry_hash ON document_registry(file_hash);
    CREATE INDEX IF NOT EXISTS idx_document_registry_kb_id ON document_registry(kb_id);
    CREATE INDEX IF NOT EXISTS idx_document_registry_doc_id ON document_registry(doc_id);
    CREATE INDEX IF NOT EXISTS idx_document_registry_status ON document_registry(status);

    CREATE INDEX IF NOT EXISTS idx_document_vectors_file_id ON document_vectors(file_id);
    CREATE INDEX IF NOT EXISTS idx_document_vectors_vector_id ON document_vectors(vector_id);
    CREATE INDEX IF NOT EXISTS idx_document_vectors_collection ON document_vectors(vector_collection);
    """
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        conn.autocommit = True
        cursor = conn.cursor()
        
        print_step("æ‰§è¡Œå¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨åˆ›å»ºSQL...", "INFO")
        cursor.execute(enhanced_tables_sql)
        
        print_step("å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨åˆ›å»ºæˆåŠŸï¼", "SUCCESS")
        
        cursor.close()
        conn.close()
        
        return True
        
    except psycopg2.Error as e:
        print_step(f"åˆ›å»ºå¢å¼ºç‰ˆè¡¨å¤±è´¥: {e}", "ERROR")
        return False

def execute_sql_file(sql_file_path: str, confirm_required: bool = True):
    """æ‰§è¡ŒSQLæ–‡ä»¶"""
    
    if not os.path.exists(sql_file_path):
        print_step(f"SQLæ–‡ä»¶ä¸å­˜åœ¨: {sql_file_path}", "ERROR")
        return False
    
    print_step(f"å‡†å¤‡æ‰§è¡ŒSQLæ–‡ä»¶: {sql_file_path}")
    
    # è¯»å–SQLæ–‡ä»¶å†…å®¹
    try:
        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_content = f.read()
        
        print_step(f"SQLæ–‡ä»¶å¤§å°: {len(sql_content)} å­—ç¬¦", "INFO")
        
        # æ˜¾ç¤ºæ–‡ä»¶å‰å‡ è¡Œé¢„è§ˆ
        lines = sql_content.split('\n')[:10]
        print_step("SQLæ–‡ä»¶é¢„è§ˆ:", "INFO")
        for i, line in enumerate(lines, 1):
            if line.strip():
                print(f"  {i:2}: {line}")
        print("  ...")
        
    except Exception as e:
        print_step(f"è¯»å–SQLæ–‡ä»¶å¤±è´¥: {e}", "ERROR")
        return False
    
    # ç¡®è®¤æ‰§è¡Œ
    if confirm_required:
        print_step("å³å°†æ‰§è¡Œæ•°æ®åº“åˆå§‹åŒ–è„šæœ¬", "WARNING")
        confirmation = input("\næ˜¯å¦ç»§ç»­æ‰§è¡Œï¼Ÿè¿™å°†åˆ›å»º/ä¿®æ”¹æ•°æ®åº“è¡¨ç»“æ„ (y/N): ").strip().lower()
        if confirmation not in ['y', 'yes', 'æ˜¯']:
            print_step("ç”¨æˆ·å–æ¶ˆæ‰§è¡Œ", "INFO")
            return False
    
    # æ‰§è¡ŒSQL
    try:
        print_step("å¼€å§‹æ‰§è¡ŒSQLè„šæœ¬...", "INFO")
        start_time = time.time()
        
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        conn.autocommit = True  # å¯ç”¨è‡ªåŠ¨æäº¤
        cursor = conn.cursor()
        
        # åˆ†å‰²å¹¶æ‰§è¡ŒSQLè¯­å¥
        # è¿™é‡Œç®€å•æŒ‰åˆ†å·åˆ†å‰²ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£æ
        statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]
        
        print_step(f"å…±æœ‰ {len(statements)} ä¸ªSQLè¯­å¥å¾…æ‰§è¡Œ", "INFO")
        
        executed_count = 0
        error_count = 0
        
        for i, statement in enumerate(statements, 1):
            if not statement:
                continue
                
            try:
                cursor.execute(statement)
                executed_count += 1
                
                # æ¯100ä¸ªè¯­å¥æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                if i % 100 == 0:
                    print_step(f"å·²æ‰§è¡Œ {i}/{len(statements)} ä¸ªè¯­å¥", "INFO")
                    
            except psycopg2.Error as e:
                error_count += 1
                # åªæ˜¾ç¤ºå‰å‡ ä¸ªé”™è¯¯ï¼Œé¿å…åˆ·å±
                if error_count <= 5:
                    print_step(f"è¯­å¥ {i} æ‰§è¡Œå¤±è´¥: {str(e)[:100]}...", "WARNING")
                elif error_count == 6:
                    print_step("æ›´å¤šé”™è¯¯å·²çœç•¥...", "WARNING")
        
        end_time = time.time()
        duration = end_time - start_time
        
        print_step(f"SQLæ‰§è¡Œå®Œæˆï¼", "SUCCESS")
        print_step(f"æ‰§è¡Œæ—¶é—´: {duration:.2f} ç§’", "INFO")
        print_step(f"æˆåŠŸæ‰§è¡Œ: {executed_count} ä¸ªè¯­å¥", "SUCCESS")
        if error_count > 0:
            print_step(f"æ‰§è¡Œé”™è¯¯: {error_count} ä¸ªè¯­å¥", "WARNING")
        
        cursor.close()
        conn.close()
        
        return error_count == 0
        
    except psycopg2.Error as e:
        print_step(f"æ‰§è¡ŒSQLæ–‡ä»¶å¤±è´¥: {e}", "ERROR")
        return False
    except Exception as e:
        print_step(f"æ‰§è¡Œè¿‡ç¨‹å¼‚å¸¸: {e}", "ERROR")
        return False

def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    print_step("åˆ›å»ºåŸºç¡€æµ‹è¯•æ•°æ®...")
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        cursor = conn.cursor()
        
        # åˆ›å»ºé»˜è®¤ç®¡ç†å‘˜ç”¨æˆ·
        admin_id = str(uuid.uuid4())
        admin_sql = """
            INSERT INTO users (id, username, email, hashed_password, full_name, is_superuser)
            VALUES (%s, 'admin', 'admin@zzdsj.com', '$2b$12$LQv3c1yqBo69SFqjfUmNnuebNZr8cCsVIIuQ1y.U9VC.ExnQd7CtO', 'ç³»ç»Ÿç®¡ç†å‘˜', true)
            ON CONFLICT (username) DO NOTHING;
        """
        cursor.execute(admin_sql, (admin_id,))
        
        # åˆ›å»ºé»˜è®¤è§’è‰²
        try:
            role_id = str(uuid.uuid4())
            role_sql = """
                INSERT INTO roles (id, name, description, is_default)
                VALUES (%s, 'admin', 'ç³»ç»Ÿç®¡ç†å‘˜è§’è‰²', false)
                ON CONFLICT (name) DO NOTHING;
            """
            cursor.execute(role_sql, (role_id,))
        except psycopg2.Error as e:
            print_step(f"åˆ›å»ºè§’è‰²æ—¶è·³è¿‡: {str(e)[:50]}...", "INFO")
        
        # åˆ›å»ºåŸºç¡€æƒé™
        permissions = [
            ('user_management', 'ç”¨æˆ·ç®¡ç†', 'ç®¡ç†ç³»ç»Ÿç”¨æˆ·'),
            ('knowledge_base_management', 'çŸ¥è¯†åº“ç®¡ç†', 'ç®¡ç†çŸ¥è¯†åº“'),
            ('system_config', 'ç³»ç»Ÿé…ç½®', 'é…ç½®ç³»ç»Ÿå‚æ•°'),
            ('file_management', 'æ–‡ä»¶ç®¡ç†', 'ç®¡ç†æ–‡ä»¶ä¸Šä¼ å’Œå­˜å‚¨'),
            ('document_processing', 'æ–‡æ¡£å¤„ç†', 'å¤„ç†æ–‡æ¡£å‘é‡åŒ–å’Œç´¢å¼•')
        ]
        
        for code, name, desc in permissions:
            try:
                perm_id = str(uuid.uuid4())
                perm_sql = """
                    INSERT INTO permissions (id, code, name, description)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (code) DO NOTHING;
                """
                cursor.execute(perm_sql, (perm_id, code, name, desc))
            except psycopg2.Error:
                # æƒé™è¡¨å¯èƒ½ä¸å­˜åœ¨ï¼Œè·³è¿‡
                pass
        
        # åˆ›å»ºé…ç½®ç±»åˆ«
        categories = [
            ('system', 'ç³»ç»Ÿé…ç½®', 'åŸºç¡€ç³»ç»Ÿé…ç½®'),
            ('ai_models', 'AIæ¨¡å‹', 'AIæ¨¡å‹ç›¸å…³é…ç½®'),
            ('storage', 'å­˜å‚¨é…ç½®', 'æ–‡ä»¶å’Œæ•°æ®å­˜å‚¨é…ç½®'),
            ('document_processing', 'æ–‡æ¡£å¤„ç†', 'æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–é…ç½®')
        ]
        
        for code, name, desc in categories:
            try:
                cat_id = str(uuid.uuid4())
                cat_sql = """
                    INSERT INTO config_categories (id, name, description, is_system)
                    VALUES (%s, %s, %s, true)
                    ON CONFLICT (name) DO NOTHING;
                """
                cursor.execute(cat_sql, (cat_id, name, desc))
            except psycopg2.Error:
                # é…ç½®ç±»åˆ«è¡¨å¯èƒ½ä¸å­˜åœ¨ï¼Œè·³è¿‡
                pass
        
        # åˆ›å»ºé»˜è®¤çŸ¥è¯†åº“
        try:
            kb_sql = """
                INSERT INTO knowledge_bases (name, description, type)
                VALUES ('é»˜è®¤çŸ¥è¯†åº“', 'ç³»ç»Ÿé»˜è®¤çŸ¥è¯†åº“', 'default')
                ON CONFLICT DO NOTHING;
            """
            cursor.execute(kb_sql)
        except psycopg2.Error:
            # çŸ¥è¯†åº“è¡¨å¯èƒ½ä¸å­˜åœ¨ï¼Œè·³è¿‡
            pass
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£æ•°æ®ï¼ˆæ¼”ç¤ºå¢å¼ºç‰ˆè¡¨çš„ä½¿ç”¨ï¼‰
        try:
            # æµ‹è¯•æ–‡æ¡£
            test_file_id = str(uuid.uuid4())
            test_doc_sql = """
                INSERT INTO document_registry_enhanced 
                (file_id, filename, content_type, file_size, file_hash, storage_backend, 
                 storage_path, kb_id, doc_id, user_id, metadata, status, processing_status)
                VALUES (%s, 'test_document.pdf', 'application/pdf', 1024000, 
                        'test_hash_123', 'minio', '/test/test_document.pdf', 
                        '1', 'test_doc_1', %s, '{"source": "test", "category": "demo"}', 
                        'uploaded', 'completed')
                ON CONFLICT (file_hash) DO NOTHING;
            """
            cursor.execute(test_doc_sql, (test_file_id, admin_id))
            
            # æµ‹è¯•åˆ‡ç‰‡
            chunk_id = str(uuid.uuid4())
            chunk_sql = """
                INSERT INTO document_chunks 
                (chunk_id, file_id, chunk_index, chunk_text, chunk_size, chunk_hash, processing_status)
                VALUES (%s, %s, 0, 'è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£åˆ‡ç‰‡çš„ç¤ºä¾‹å†…å®¹ã€‚', 50, 'chunk_hash_1', 'completed')
                ON CONFLICT (file_id, chunk_index) DO NOTHING;
            """
            cursor.execute(chunk_sql, (chunk_id, test_file_id))
            
            # æµ‹è¯•å‘é‡å…³è”
            vector_sql = """
                INSERT INTO document_vectors_enhanced 
                (file_id, chunk_id, vector_id, vector_collection, vector_index, 
                 embedding_model, embedding_dimension)
                VALUES (%s, %s, 'vec_001', 'default_collection', 'default_index', 
                        'text-embedding-ada-002', 1536)
                ON CONFLICT (file_id, chunk_id, vector_id) DO NOTHING;
            """
            cursor.execute(vector_sql, (test_file_id, chunk_id))
            
            # æµ‹è¯•ESåˆ†ç‰‡å…³è”
            es_sql = """
                INSERT INTO document_es_shards 
                (file_id, chunk_id, es_index, es_doc_id, es_doc_type)
                VALUES (%s, %s, 'documents', 'doc_001', 'document')
                ON CONFLICT (es_index, es_doc_id) DO NOTHING;
            """
            cursor.execute(es_sql, (test_file_id, chunk_id))
            
            print_step("åˆ›å»ºäº†æ¼”ç¤ºæ•°æ®ï¼ˆå¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†ï¼‰", "SUCCESS")
            
        except psycopg2.Error as e:
            print_step(f"åˆ›å»ºæ¼”ç¤ºæ•°æ®å¤±è´¥ï¼ˆå¯èƒ½è¡¨ä¸å­˜åœ¨ï¼‰: {str(e)[:50]}...", "INFO")
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print_step("æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸï¼", "SUCCESS")
        return True
        
    except psycopg2.Error as e:
        print_step(f"åˆ›å»ºæµ‹è¯•æ•°æ®å¤±è´¥: {e}", "ERROR")
        return False

def verify_installation():
    """éªŒè¯å®‰è£…ç»“æœ"""
    print_step("éªŒè¯æ•°æ®åº“å®‰è£…ç»“æœ...")
    
    try:
        conn = psycopg2.connect(**REMOTE_DB_CONFIG)
        cursor = conn.cursor()
        
        # æ£€æŸ¥å…³é”®è¡¨æ˜¯å¦å­˜åœ¨
        required_tables = [
            'users', 'document_registry_enhanced', 'document_chunks', 
            'document_vectors_enhanced', 'document_es_shards', 'document_processing_history'
        ]
        
        # å¯é€‰è¡¨ï¼ˆåŸæœ‰ç³»ç»Ÿè¡¨ï¼‰
        optional_tables = [
            'roles', 'permissions', 'knowledge_bases', 
            'documents', 'assistants', 'system_configs', 'model_providers',
            'document_registry', 'document_vectors'
        ]
        
        missing_tables = []
        existing_tables = []
        optional_existing = []
        
        # æ£€æŸ¥å¿…éœ€è¡¨
        for table in required_tables:
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, (table,))
            
            if cursor.fetchone()[0]:
                existing_tables.append(table)
            else:
                missing_tables.append(table)
        
        # æ£€æŸ¥å¯é€‰è¡¨
        for table in optional_tables:
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, (table,))
            
            if cursor.fetchone()[0]:
                optional_existing.append(table)
        
        print_step(f"æ ¸å¿ƒè¡¨æ£€æŸ¥: {len(existing_tables)}/{len(required_tables)} å­˜åœ¨", 
                  "SUCCESS" if not missing_tables else "WARNING")
        
        print_step(f"å¯é€‰è¡¨æ£€æŸ¥: {len(optional_existing)}/{len(optional_tables)} å­˜åœ¨", "INFO")
        
        if missing_tables:
            print_step(f"ç¼ºå¤±æ ¸å¿ƒè¡¨: {', '.join(missing_tables)}", "WARNING")
        
        # æ£€æŸ¥å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨çš„æ•°æ®
        if 'document_registry_enhanced' in existing_tables:
            cursor.execute("SELECT COUNT(*) FROM document_registry_enhanced;")
            doc_count = cursor.fetchone()[0]
            print_step(f"å¢å¼ºç‰ˆæ–‡æ¡£æ³¨å†Œè¡¨è®°å½•æ•°: {doc_count}", "INFO")
            
            if 'document_chunks' in existing_tables:
                cursor.execute("SELECT COUNT(*) FROM document_chunks;")
                chunk_count = cursor.fetchone()[0]
                print_step(f"æ–‡æ¡£åˆ‡ç‰‡è®°å½•æ•°: {chunk_count}", "INFO")
            
            if 'document_vectors_enhanced' in existing_tables:
                cursor.execute("SELECT COUNT(*) FROM document_vectors_enhanced;")
                vector_count = cursor.fetchone()[0]
                print_step(f"å¢å¼ºç‰ˆå‘é‡å…³è”è®°å½•æ•°: {vector_count}", "INFO")
            
            if 'document_es_shards' in existing_tables:
                cursor.execute("SELECT COUNT(*) FROM document_es_shards;")
                es_count = cursor.fetchone()[0]
                print_step(f"ESåˆ†ç‰‡å…³è”è®°å½•æ•°: {es_count}", "INFO")
        
        # æ£€æŸ¥ç”¨æˆ·æ•°æ®
        if 'users' in existing_tables:
            cursor.execute("SELECT COUNT(*) FROM users WHERE is_superuser = true;")
            admin_count = cursor.fetchone()[0]
            print_step(f"ç®¡ç†å‘˜ç”¨æˆ·æ•°: {admin_count}", "SUCCESS" if admin_count > 0 else "WARNING")
        
        cursor.close()
        conn.close()
        
        success = len(missing_tables) == 0
        print_step("æ•°æ®åº“éªŒè¯å®Œæˆï¼", "SUCCESS" if success else "WARNING")
        return success
        
    except psycopg2.Error as e:
        print_step(f"éªŒè¯å®‰è£…å¤±è´¥: {e}", "ERROR")
        return False

@dataclass
class DatabaseHealthReport:
    """æ•°æ®åº“å¥åº·æŠ¥å‘Š"""
    timestamp: str
    connection_status: str
    performance_metrics: Dict
    index_analysis: Dict
    query_analysis: Dict
    recommendations: List[str]
    overall_score: int

class DatabaseHealthChecker:
    """æ•°æ®åº“å¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
    
    def check_performance_metrics(self, cursor) -> Dict:
        """æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        try:
            # 1. æ•°æ®åº“å¤§å°å’Œå¢é•¿è¶‹åŠ¿
            cursor.execute("""
                SELECT 
                    pg_size_pretty(pg_database_size(current_database())) as db_size,
                    pg_database_size(current_database()) as db_size_bytes
            """)
            size_info = cursor.fetchone()
            metrics['database_size'] = {
                'human_readable': size_info[0],
                'bytes': size_info[1]
            }
            
            # 2. è¿æ¥ç»Ÿè®¡
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_connections,
                    COUNT(*) FILTER (WHERE state = 'active') as active_connections,
                    COUNT(*) FILTER (WHERE state = 'idle') as idle_connections,
                    COUNT(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction
                FROM pg_stat_activity 
                WHERE datname = current_database()
            """)
            conn_stats = cursor.fetchone()
            metrics['connections'] = {
                'total': conn_stats[0],
                'active': conn_stats[1],
                'idle': conn_stats[2],
                'idle_in_transaction': conn_stats[3]
            }
            
            # 3. ç¼“å­˜å‘½ä¸­ç‡
            cursor.execute("""
                SELECT 
                    ROUND(100.0 * sum(blks_hit) / NULLIF(sum(blks_hit) + sum(blks_read), 0), 2) as cache_hit_ratio,
                    sum(blks_read) as blocks_read,
                    sum(blks_hit) as blocks_hit
                FROM pg_stat_database 
                WHERE datname = current_database()
            """)
            cache_stats = cursor.fetchone()
            metrics['cache'] = {
                'hit_ratio': cache_stats[0] or 0.0,
                'blocks_read': cache_stats[1] or 0,
                'blocks_hit': cache_stats[2] or 0
            }
            
            # 4. äº‹åŠ¡ç»Ÿè®¡
            cursor.execute("""
                SELECT 
                    xact_commit,
                    xact_rollback,
                    deadlocks,
                    conflicts,
                    temp_files,
                    temp_bytes
                FROM pg_stat_database 
                WHERE datname = current_database()
            """)
            tx_stats = cursor.fetchone()
            metrics['transactions'] = {
                'commits': tx_stats[0] or 0,
                'rollbacks': tx_stats[1] or 0,
                'deadlocks': tx_stats[2] or 0,
                'conflicts': tx_stats[3] or 0,
                'temp_files': tx_stats[4] or 0,
                'temp_bytes': tx_stats[5] or 0
            }
            
            # 5. WALç»Ÿè®¡
            cursor.execute("""
                SELECT 
                    pg_wal_lsn_diff(pg_current_wal_lsn(), '0/0') / 1024 / 1024 as wal_size_mb,
                    pg_current_wal_lsn() as current_wal_lsn
            """)
            wal_stats = cursor.fetchone()
            metrics['wal'] = {
                'size_mb': round(wal_stats[0], 2),
                'current_lsn': str(wal_stats[1])
            }
            
        except Exception as e:
            metrics['error'] = str(e)
            
        return metrics
    
    def analyze_indexes(self, cursor) -> Dict:
        """åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
        analysis = {}
        
        try:
            # 1. æœªä½¿ç”¨çš„ç´¢å¼•
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan,
                    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
                FROM pg_stat_user_indexes 
                WHERE idx_scan = 0 
                AND schemaname = 'public'
                ORDER BY pg_relation_size(indexrelid) DESC
            """)
            unused_indexes = cursor.fetchall()
            analysis['unused_indexes'] = [
                {
                    'table': row[1],
                    'index': row[2],
                    'size': row[4]
                }
                for row in unused_indexes
            ]
            
            # 2. ä½æ•ˆç´¢å¼• (æ‰«ææ¬¡æ•°å°‘ä½†å ç”¨ç©ºé—´å¤§)
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan,
                    idx_tup_read,
                    idx_tup_fetch,
                    pg_size_pretty(pg_relation_size(indexrelid)) as index_size,
                    pg_relation_size(indexrelid) as index_bytes
                FROM pg_stat_user_indexes 
                WHERE schemaname = 'public'
                AND pg_relation_size(indexrelid) > 1024 * 1024  -- å¤§äº1MB
                AND idx_scan < 100  -- æ‰«ææ¬¡æ•°å°‘äº100æ¬¡
                ORDER BY pg_relation_size(indexrelid) DESC
            """)
            inefficient_indexes = cursor.fetchall()
            analysis['inefficient_indexes'] = [
                {
                    'table': row[1],
                    'index': row[2],
                    'scan_count': row[3],
                    'size': row[6],
                    'efficiency_score': round((row[3] or 0) / max(row[7] / (1024*1024), 1), 2)
                }
                for row in inefficient_indexes
            ]
            
            # 3. ç¼ºå¤±ç´¢å¼•å»ºè®® (åŸºäºæŸ¥è¯¢æ¨¡å¼)
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    seq_scan,
                    seq_tup_read,
                    idx_scan,
                    idx_tup_fetch,
                    CASE 
                        WHEN seq_scan > 0 THEN seq_tup_read / seq_scan 
                        ELSE 0 
                    END as avg_seq_read
                FROM pg_stat_user_tables 
                WHERE schemaname = 'public'
                AND seq_scan > idx_scan  -- é¡ºåºæ‰«æå¤šäºç´¢å¼•æ‰«æ
                ORDER BY seq_tup_read DESC
            """)
            seq_scan_heavy = cursor.fetchall()
            analysis['missing_index_candidates'] = [
                {
                    'table': row[1],
                    'seq_scans': row[2],
                    'seq_reads': row[3],
                    'avg_read_per_scan': round(row[6], 2),
                    'priority': 'HIGH' if row[6] > 1000 else 'MEDIUM'
                }
                for row in seq_scan_heavy if row[6] > 100
            ]
            
        except Exception as e:
            analysis['error'] = str(e)
            
        return analysis
    
    def analyze_queries(self, cursor) -> Dict:
        """åˆ†ææŸ¥è¯¢æ€§èƒ½"""
        analysis = {}
        
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†pg_stat_statements
            cursor.execute("""
                SELECT EXISTS (
                    SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements'
                );
            """)
            has_pg_stat_statements = cursor.fetchone()[0]
            
            if has_pg_stat_statements:
                # æœ€è€—æ—¶çš„æŸ¥è¯¢
                cursor.execute("""
                    SELECT 
                        LEFT(query, 100) as query_snippet,
                        calls,
                        total_exec_time,
                        mean_exec_time,
                        max_exec_time,
                        stddev_exec_time
                    FROM pg_stat_statements 
                    WHERE query NOT LIKE '%pg_stat_statements%'
                    ORDER BY total_exec_time DESC 
                    LIMIT 10
                """)
                slow_queries = cursor.fetchall()
                analysis['slow_queries'] = [
                    {
                        'query': row[0],
                        'calls': row[1],
                        'total_time': round(row[2], 2),
                        'mean_time': round(row[3], 2),
                        'max_time': round(row[4], 2)
                    }
                    for row in slow_queries
                ]
                
                # è°ƒç”¨é¢‘ç‡æœ€é«˜çš„æŸ¥è¯¢
                cursor.execute("""
                    SELECT 
                        LEFT(query, 100) as query_snippet,
                        calls,
                        total_exec_time,
                        mean_exec_time
                    FROM pg_stat_statements 
                    WHERE query NOT LIKE '%pg_stat_statements%'
                    ORDER BY calls DESC 
                    LIMIT 10
                """)
                frequent_queries = cursor.fetchall()
                analysis['frequent_queries'] = [
                    {
                        'query': row[0],
                        'calls': row[1],
                        'total_time': round(row[2], 2),
                        'mean_time': round(row[3], 2)
                    }
                    for row in frequent_queries
                ]
            else:
                analysis['pg_stat_statements'] = 'not_enabled'
                analysis['recommendation'] = 'Enable pg_stat_statements extension for query analysis'
            
            # å½“å‰æ´»è·ƒçš„é•¿æ—¶é—´è¿è¡ŒæŸ¥è¯¢
            cursor.execute("""
                SELECT 
                    pid,
                    now() - query_start as duration,
                    state,
                    LEFT(query, 100) as query_snippet
                FROM pg_stat_activity 
                WHERE state = 'active' 
                AND query_start < now() - interval '30 seconds'
                AND query NOT LIKE '%pg_stat_activity%'
                ORDER BY query_start ASC
            """)
            long_running = cursor.fetchall()
            analysis['long_running_queries'] = [
                {
                    'pid': row[0],
                    'duration': str(row[1]),
                    'state': row[2],
                    'query': row[3]
                }
                for row in long_running
            ]
            
        except Exception as e:
            analysis['error'] = str(e)
            
        return analysis
    
    def generate_recommendations(self, metrics: Dict, index_analysis: Dict, query_analysis: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # æ€§èƒ½ç›¸å…³å»ºè®®
        if 'cache' in metrics:
            cache_hit_ratio = metrics['cache']['hit_ratio']
            if cache_hit_ratio < 90:
                recommendations.append(f"âš ï¸ ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ ({cache_hit_ratio}%)ï¼Œå»ºè®®è°ƒæ•´shared_bufferså‚æ•°")
        
        # è¿æ¥ç›¸å…³å»ºè®®
        if 'connections' in metrics:
            idle_in_tx = metrics['connections']['idle_in_transaction']
            if idle_in_tx > 5:
                recommendations.append(f"âš ï¸ å‘ç° {idle_in_tx} ä¸ªç©ºé—²äº‹åŠ¡è¿æ¥ï¼Œå¯èƒ½å­˜åœ¨è¿æ¥æ³„æ¼")
        
        # ç´¢å¼•ç›¸å…³å»ºè®®
        if 'unused_indexes' in index_analysis:
            unused_count = len(index_analysis['unused_indexes'])
            if unused_count > 0:
                recommendations.append(f"ğŸ’¡ å‘ç° {unused_count} ä¸ªæœªä½¿ç”¨çš„ç´¢å¼•ï¼Œå»ºè®®æ¸…ç†ä»¥èŠ‚çœç©ºé—´")
        
        if 'missing_index_candidates' in index_analysis:
            missing_count = len(index_analysis['missing_index_candidates'])
            if missing_count > 0:
                recommendations.append(f"ğŸ’¡ å‘ç° {missing_count} ä¸ªè¡¨å¯èƒ½éœ€è¦æ·»åŠ ç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½")
        
        # æŸ¥è¯¢ç›¸å…³å»ºè®®
        if 'long_running_queries' in query_analysis:
            long_count = len(query_analysis['long_running_queries'])
            if long_count > 0:
                recommendations.append(f"âš ï¸ å‘ç° {long_count} ä¸ªé•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢ï¼Œå»ºè®®æ£€æŸ¥å’Œä¼˜åŒ–")
        
        if query_analysis.get('pg_stat_statements') == 'not_enabled':
            recommendations.append("ğŸ’¡ å»ºè®®å¯ç”¨pg_stat_statementsæ‰©å±•ä»¥è·å¾—æ›´è¯¦ç»†çš„æŸ¥è¯¢åˆ†æ")
        
        # WALç›¸å…³å»ºè®®
        if 'wal' in metrics:
            wal_size = metrics['wal']['size_mb']
            if wal_size > 1000:  # å¤§äº1GB
                recommendations.append(f"âš ï¸ WALå¤§å°è¾ƒå¤§ ({wal_size} MB)ï¼Œå»ºè®®æ£€æŸ¥checkpointé…ç½®")
        
        if not recommendations:
            recommendations.append("âœ… æ•°æ®åº“çŠ¶æ€è‰¯å¥½ï¼Œæœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜")
        
        return recommendations
    
    def calculate_health_score(self, metrics: Dict, index_analysis: Dict, query_analysis: Dict) -> int:
        """è®¡ç®—å¥åº·è¯„åˆ† (0-100)"""
        score = 100
        
        # ç¼“å­˜å‘½ä¸­ç‡å½±å“ (æœ€å¤šæ‰£30åˆ†)
        if 'cache' in metrics:
            cache_ratio = metrics['cache']['hit_ratio']
            if cache_ratio < 95:
                score -= min(30, (95 - cache_ratio) * 2)
        
        # ç©ºé—²äº‹åŠ¡è¿æ¥å½±å“ (æ¯ä¸ªæ‰£5åˆ†)
        if 'connections' in metrics:
            idle_in_tx = metrics['connections']['idle_in_transaction']
            score -= min(25, idle_in_tx * 5)
        
        # æœªä½¿ç”¨ç´¢å¼•å½±å“ (æ¯ä¸ªæ‰£2åˆ†)
        if 'unused_indexes' in index_analysis:
            unused_count = len(index_analysis['unused_indexes'])
            score -= min(20, unused_count * 2)
        
        # é•¿æ—¶é—´è¿è¡ŒæŸ¥è¯¢å½±å“ (æ¯ä¸ªæ‰£3åˆ†)
        if 'long_running_queries' in query_analysis:
            long_count = len(query_analysis['long_running_queries'])
            score -= min(15, long_count * 3)
        
        return max(0, score)
    
    def run_health_check(self) -> DatabaseHealthReport:
        """æ‰§è¡Œå®Œæ•´çš„å¥åº·æ£€æŸ¥"""
        print_step("å¼€å§‹æ•°æ®åº“å¥åº·æ£€æŸ¥...", "INFO")
        
        try:
            conn = psycopg2.connect(**self.config)
            cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
            
            # æ£€æŸ¥å„é¡¹æŒ‡æ ‡
            performance_metrics = self.check_performance_metrics(cursor)
            index_analysis = self.analyze_indexes(cursor)
            query_analysis = self.analyze_queries(cursor)
            
            # ç”Ÿæˆå»ºè®®
            recommendations = self.generate_recommendations(
                performance_metrics, index_analysis, query_analysis
            )
            
            # è®¡ç®—å¥åº·è¯„åˆ†
            health_score = self.calculate_health_score(
                performance_metrics, index_analysis, query_analysis
            )
            
            cursor.close()
            conn.close()
            
            # åˆ›å»ºå¥åº·æŠ¥å‘Š
            report = DatabaseHealthReport(
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                connection_status="connected",
                performance_metrics=performance_metrics,
                index_analysis=index_analysis,
                query_analysis=query_analysis,
                recommendations=recommendations,
                overall_score=health_score
            )
            
            return report
            
        except Exception as e:
            print_step(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}", "ERROR")
            return DatabaseHealthReport(
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                connection_status="failed",
                performance_metrics={},
                index_analysis={},
                query_analysis={},
                recommendations=[f"è¿æ¥å¤±è´¥: {e}"],
                overall_score=0
            )

def print_health_report(report: DatabaseHealthReport):
    """æ‰“å°å¥åº·æ£€æŸ¥æŠ¥å‘Š"""
    print_header("æ•°æ®åº“å¥åº·æ£€æŸ¥æŠ¥å‘Š")
    
    # æ•´ä½“è¯„åˆ†
    score_color = "SUCCESS" if report.overall_score >= 80 else "WARNING" if report.overall_score >= 60 else "ERROR"
    print_step(f"æ•´ä½“å¥åº·è¯„åˆ†: {report.overall_score}/100", score_color)
    
    # æ€§èƒ½æŒ‡æ ‡
    if report.performance_metrics:
        print_step("æ€§èƒ½æŒ‡æ ‡:", "INFO")
        metrics = report.performance_metrics
        
        if 'database_size' in metrics:
            print(f"  ğŸ“Š æ•°æ®åº“å¤§å°: {metrics['database_size']['human_readable']}")
        
        if 'connections' in metrics:
            conn = metrics['connections']
            print(f"  ğŸ”— è¿æ¥ç»Ÿè®¡: æ€»è®¡ {conn['total']}, æ´»è·ƒ {conn['active']}, ç©ºé—² {conn['idle']}")
            if conn['idle_in_transaction'] > 0:
                print(f"  âš ï¸  ç©ºé—²äº‹åŠ¡: {conn['idle_in_transaction']} ä¸ª")
        
        if 'cache' in metrics:
            print(f"  ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {metrics['cache']['hit_ratio']}%")
        
        if 'transactions' in metrics:
            tx = metrics['transactions']
            if tx['deadlocks'] > 0:
                print(f"  ğŸ”’ æ­»é”æ•°: {tx['deadlocks']}")
    
    # ç´¢å¼•åˆ†æ
    if report.index_analysis:
        print_step("ç´¢å¼•åˆ†æ:", "INFO")
        
        unused = report.index_analysis.get('unused_indexes', [])
        if unused:
            print(f"  ğŸ—‘ï¸  æœªä½¿ç”¨ç´¢å¼•: {len(unused)} ä¸ª")
            for idx in unused[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"    â€¢ {idx['table']}.{idx['index']} ({idx['size']})")
        
        missing = report.index_analysis.get('missing_index_candidates', [])
        if missing:
            print(f"  ğŸ’¡ å»ºè®®æ·»åŠ ç´¢å¼•çš„è¡¨: {len(missing)} ä¸ª")
            for table in missing[:3]:
                print(f"    â€¢ {table['table']} (ä¼˜å…ˆçº§: {table['priority']})")
    
    # æŸ¥è¯¢åˆ†æ
    if report.query_analysis:
        print_step("æŸ¥è¯¢åˆ†æ:", "INFO")
        
        long_running = report.query_analysis.get('long_running_queries', [])
        if long_running:
            print(f"  ğŸŒ é•¿æ—¶é—´è¿è¡ŒæŸ¥è¯¢: {len(long_running)} ä¸ª")
            for query in long_running[:2]:
                print(f"    â€¢ PID {query['pid']}: {query['duration']} - {query['query'][:50]}...")
        
        if 'slow_queries' in report.query_analysis:
            slow_count = len(report.query_analysis['slow_queries'])
            print(f"  ğŸ“ˆ æ…¢æŸ¥è¯¢ç»Ÿè®¡: {slow_count} æ¡è®°å½•")
    
    # ä¼˜åŒ–å»ºè®®
    if report.recommendations:
        print_step("ä¼˜åŒ–å»ºè®®:", "INFO")
        for i, rec in enumerate(report.recommendations, 1):
            print(f"  {i}. {rec}")
    
    print(f"\nğŸ“… æŠ¥å‘Šæ—¶é—´: {report.timestamp}")

def system_health_check():
    """æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    checker = DatabaseHealthChecker(REMOTE_DB_CONFIG)
    report = checker.run_health_check()
    
    # æ‰“å°æŠ¥å‘Š
    print_health_report(report)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_dir = Path("health_reports")
    report_dir.mkdir(exist_ok=True)
    
    report_file = report_dir / f"health_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(asdict(report), f, indent=2, ensure_ascii=False)
    
    print_step(f"å¥åº·æ£€æŸ¥æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}", "SUCCESS")
    
    return report.overall_score >= 70

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿œç¨‹PostgreSQLæ•°æ®åº“ç®¡ç†å·¥å…·")
    parser.add_argument('--mode', choices=['full', 'test', 'health', 'init'], default='full',
                       help='è¿è¡Œæ¨¡å¼: full(å®Œæ•´åˆå§‹åŒ–), test(ä»…æµ‹è¯•è¿æ¥), health(å¥åº·æ£€æŸ¥), init(ä»…åˆå§‹åŒ–)')
    parser.add_argument('--skip-confirmation', action='store_true',
                       help='è·³è¿‡ç¡®è®¤æç¤º')
    
    args = parser.parse_args()
    
    if args.mode == 'test':
        # ä»…æµ‹è¯•è¿æ¥
        print_header("æ•°æ®åº“è¿æ¥æµ‹è¯•")
        return test_connection()
    
    elif args.mode == 'health':
        # ä»…æ‰§è¡Œå¥åº·æ£€æŸ¥
        print_header("æ•°æ®åº“å¥åº·æ£€æŸ¥")
        return system_health_check()
    
    elif args.mode == 'init':
        # ä»…æ‰§è¡Œåˆå§‹åŒ–
        print_header("æ•°æ®åº“åˆå§‹åŒ–")
        
        # æµ‹è¯•è¿æ¥
        if not test_connection():
            print_step("è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’Œé…ç½®", "ERROR")
            return False
        
        # åˆ›å»ºå¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨
        if not create_enhanced_document_tables():
            print_step("å¢å¼ºç‰ˆè¡¨åˆ›å»ºå¤±è´¥", "ERROR")
            return False
        
        # æ‰§è¡Œæ•°æ®åº“åˆå§‹åŒ–
        sql_file_path = "database_complete.sql"
        if os.path.exists(sql_file_path):
            if not execute_sql_file(sql_file_path, confirm_required=not args.skip_confirmation):
                print_step("æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥", "WARNING")
        
        return True
    
    else:  # full mode
        print_header("è¿œç¨‹PostgreSQLæ•°æ®åº“è¿æ¥æµ‹è¯•å’Œå¢å¼ºç‰ˆåˆå§‹åŒ–")
        
        print("ğŸ¯ ç›®æ ‡æœåŠ¡å™¨ä¿¡æ¯:")
        print(f"  ğŸ“ åœ°å€: {REMOTE_DB_CONFIG['host']}:{REMOTE_DB_CONFIG['port']}")
        print(f"  ğŸ‘¤ ç”¨æˆ·: {REMOTE_DB_CONFIG['user']}")
        print(f"  ğŸ—„ï¸  æ•°æ®åº“: {REMOTE_DB_CONFIG['database']}")
        print("\nğŸ”§ å¢å¼ºç‰ˆåŠŸèƒ½:")
        print("  â€¢ å®Œæ•´çš„å‘é‡chunk IDè¿½è¸ª")
        print("  â€¢ ESæ–‡æ¡£åˆ†ç‰‡æ•°æ®å…³è”")
        print("  â€¢ ç»Ÿä¸€åˆ é™¤æ“ä½œæ”¯æŒ")
        print("  â€¢ è¯¦ç»†çš„å¤„ç†å†å²è®°å½•")
        print("  â€¢ ç³»ç»Ÿå¥åº·æ£€æŸ¥å’Œæ€§èƒ½ç›‘æ§")
        print("  â€¢ ç´¢å¼•ä½¿ç”¨åˆ†æå’Œä¼˜åŒ–å»ºè®®")
        
        # æ­¥éª¤1: æµ‹è¯•è¿æ¥
        if not test_connection():
            print_step("è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’Œé…ç½®", "ERROR")
            return False
        
        # æ­¥éª¤2: æ£€æŸ¥ç°æœ‰è¡¨
        table_count = check_existing_tables()
        if table_count == -1:
            print_step("æ— æ³•æ£€æŸ¥ç°æœ‰è¡¨ç»“æ„", "ERROR")
            return False
        
        # æ­¥éª¤3: åˆ›å»ºå¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨
        print_step("å¼€å§‹åˆ›å»ºå¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†è¡¨ç»“æ„...", "INFO")
        if not create_enhanced_document_tables():
            print_step("å¢å¼ºç‰ˆè¡¨åˆ›å»ºå¤±è´¥", "ERROR")
            return False
        
        # æ­¥éª¤4: æ‰§è¡Œæ•°æ®åº“åˆå§‹åŒ–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        sql_file_path = "database_complete.sql"
        if os.path.exists(sql_file_path):
            print_step("å‘ç°æ•°æ®åº“åˆå§‹åŒ–æ–‡ä»¶ï¼Œå‡†å¤‡æ‰§è¡Œ...", "INFO")
            if not execute_sql_file(sql_file_path, confirm_required=not args.skip_confirmation):
                print_step("æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥", "WARNING")
        else:
            print_step(f"æœªæ‰¾åˆ°æ•°æ®åº“åˆå§‹åŒ–æ–‡ä»¶: {sql_file_path} (è·³è¿‡)", "INFO")
        
        # æ­¥éª¤5: åˆ›å»ºæµ‹è¯•æ•°æ®
        if not create_test_data():
            print_step("åˆ›å»ºæµ‹è¯•æ•°æ®å¤±è´¥", "WARNING")
        
        # æ­¥éª¤6: éªŒè¯å®‰è£…
        if not verify_installation():
            print_step("æ•°æ®åº“éªŒè¯å¤±è´¥", "WARNING")
            return False
        
        # æ­¥éª¤7: æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥
        if not system_health_check():
            print_step("ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥", "WARNING")
            return False
        
        # æˆåŠŸå®Œæˆ
        print_header("å¢å¼ºç‰ˆæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    print_step("ğŸ‰ è¿œç¨‹PostgreSQLæ•°æ®åº“å¢å¼ºç‰ˆåˆå§‹åŒ–æˆåŠŸï¼", "SUCCESS")
    print_step("ğŸ“‹ æ–°å¢åŠŸèƒ½:", "INFO")
    print("  âœ… å®Œæ•´çš„æ–‡æ¡£chunkè¿½è¸ª")
    print("  âœ… å‘é‡æ•°æ®ç²¾ç¡®å…³è”")
    print("  âœ… ESæ–‡æ¡£åˆ†ç‰‡å…³è”")
    print("  âœ… ç»Ÿä¸€åˆ é™¤æ“ä½œæ”¯æŒ")
    print("  âœ… è¯¦ç»†çš„æ“ä½œå†å²è®°å½•")
    
    print_step("ğŸ”§ åç»­æ­¥éª¤:", "INFO")
    print("  1. æ›´æ–°åº”ç”¨ç¨‹åºä½¿ç”¨å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†å™¨")
    print("  2. é…ç½®ESå’Œå‘é‡æ•°æ®åº“è¿æ¥")
    print("  3. æµ‹è¯•ç»Ÿä¸€åˆ é™¤åŠŸèƒ½")
    print("  4. è¿ç§»ç°æœ‰æ–‡æ¡£æ•°æ®ï¼ˆå¦‚éœ€è¦ï¼‰")
    
    print_step("ğŸ”‘ é»˜è®¤ç®¡ç†å‘˜è´¦æˆ·:", "INFO")
    print("  ç”¨æˆ·å: admin")
    print("  é‚®ç®±: admin@zzdsj.com")
    print("  å¯†ç : admin123 (è¯·åŠæ—¶ä¿®æ”¹)")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print_step("\nç”¨æˆ·ä¸­æ–­æ“ä½œ", "INFO")
        sys.exit(1)
    except Exception as e:
        print_step(f"ç¨‹åºå¼‚å¸¸: {e}", "ERROR")
        sys.exit(1) 