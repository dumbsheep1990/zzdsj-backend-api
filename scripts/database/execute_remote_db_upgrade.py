#!/usr/bin/env python3
"""
åŸºäºè¿œç¨‹æ•°æ®åº“è¿æ¥çš„å¢å¼ºç‰ˆæ•°æ®åº“å‡çº§è„šæœ¬
ä½¿ç”¨æ˜¨å¤©çš„è¿œç¨‹PostgreSQLè¿æ¥é…ç½®æ‰§è¡Œæ•°æ®åº“è¡¨æ›´æ–°
ç¡®ä¿æ•°æ®åº“ç»“æ„å®Œå…¨ç¬¦åˆå¢å¼ºç‰ˆè¦æ±‚
"""

import os
import sys
import time
import logging
import psycopg2
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple
from psycopg2.extras import RealDictCursor

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è¿œç¨‹æ•°æ®åº“è¿æ¥é…ç½®ï¼ˆåŸºäºæ˜¨å¤©çš„æµ‹è¯•è„šæœ¬ï¼‰
REMOTE_DB_CONFIG = {
    'host': '167.71.85.231',
    'port': 5432,
    'user': 'zzdsj',
    'password': 'zzdsj123',
    'database': 'zzdsj'
}

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RemoteDatabaseUpgrader:
    """è¿œç¨‹æ•°æ®åº“å¢å¼ºç‰ˆå‡çº§å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‡çº§å™¨"""
        self.db_config = REMOTE_DB_CONFIG
        self.upgrade_results = []
        self.errors = []
        self.start_time = datetime.now()
        
    def _get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return psycopg2.connect(
            **self.db_config,
            cursor_factory=RealDictCursor
        )
    
    def log_operation(self, operation: str, success: bool, details: str = "", duration_ms: int = 0):
        """è®°å½•æ“ä½œç»“æœ"""
        result = {
            "operation": operation,
            "success": success,
            "details": details,
            "duration_ms": duration_ms,
            "timestamp": datetime.now().isoformat()
        }
        self.upgrade_results.append(result)
        
        status_icon = "âœ…" if success else "âŒ"
        print(f"[{datetime.now().strftime('%H:%M:%S')}] {status_icon} {operation}")
        if details:
            print(f"    â””â”€ {details}")
        if duration_ms > 0:
            print(f"    â±ï¸ è€—æ—¶: {duration_ms}ms")
            
        if not success:
            self.errors.append(f"{operation}: {details}")
    
    def test_remote_database_connection(self) -> bool:
        """æµ‹è¯•è¿œç¨‹æ•°æ®åº“è¿æ¥"""
        print("\nğŸ”— æµ‹è¯•è¿œç¨‹æ•°æ®åº“è¿æ¥...")
        print(f"ğŸ“ ç›®æ ‡æœåŠ¡å™¨: {self.db_config['host']}:{self.db_config['port']}")
        print(f"ğŸ‘¤ ç”¨æˆ·: {self.db_config['user']}")
        print(f"ğŸ—„ï¸ æ•°æ®åº“: {self.db_config['database']}")
        
        try:
            start_time = time.time()
            conn = self._get_db_connection()
            cursor = conn.cursor()
            
            # è·å–æ•°æ®åº“ç‰ˆæœ¬ä¿¡æ¯
            cursor.execute("SELECT version()")
            version = cursor.fetchone()['version']
            
            # è·å–æ•°æ®åº“åŸºæœ¬ä¿¡æ¯
            cursor.execute("SELECT current_database(), current_user, current_timestamp")
            db_info = cursor.fetchone()
            
            # æ£€æŸ¥æ•°æ®åº“æƒé™
            cursor.execute("""
                SELECT datname, has_database_privilege(current_user, datname, 'CREATE') as can_create
                FROM pg_database 
                WHERE datname = current_database()
            """)
            perm_info = cursor.fetchone()
            
            cursor.close()
            conn.close()
            
            duration_ms = int((time.time() - start_time) * 1000)
            
            self.log_operation(
                "è¿œç¨‹æ•°æ®åº“è¿æ¥æµ‹è¯•", True,
                f"PostgreSQLç‰ˆæœ¬: {version.split(',')[0]}", duration_ms
            )
            
            print(f"ğŸ“Š è¿æ¥ä¿¡æ¯:")
            print(f"   æ•°æ®åº“: {db_info['current_database']}")
            print(f"   ç”¨æˆ·: {db_info['current_user']}")
            print(f"   æ—¶é—´: {db_info['current_timestamp']}")
            print(f"   CREATEæƒé™: {'âœ…' if perm_info['can_create'] else 'âŒ'}")
            
            if not perm_info['can_create']:
                self.log_operation(
                    "æ•°æ®åº“æƒé™æ£€æŸ¥", False,
                    "ç”¨æˆ·ç¼ºå°‘CREATEæƒé™ï¼Œå¯èƒ½å½±å“è¡¨åˆ›å»º"
                )
                return False
            
            return True
            
        except Exception as e:
            self.log_operation(
                "è¿œç¨‹æ•°æ®åº“è¿æ¥æµ‹è¯•", False,
                f"è¿æ¥å¤±è´¥: {str(e)}"
            )
            return False
    
    def check_existing_remote_tables(self) -> Dict[str, bool]:
        """æ£€æŸ¥è¿œç¨‹æ•°æ®åº“çš„ç°æœ‰è¡¨ç»“æ„"""
        print("\nğŸ“Š æ£€æŸ¥è¿œç¨‹æ•°æ®åº“ç°æœ‰è¡¨ç»“æ„...")
        
        enhanced_tables = {
            'document_registry_enhanced': False,
            'document_chunks': False,
            'document_vectors_enhanced': False,
            'document_es_shards': False,
            'document_processing_history': False
        }
        
        try:
            conn = self._get_db_connection()
            cursor = conn.cursor()
            
            # æŸ¥è¯¢æ‰€æœ‰ç°æœ‰è¡¨
            cursor.execute("""
                SELECT tablename FROM pg_tables 
                WHERE schemaname = 'public'
                ORDER BY tablename
            """)
            existing_tables = [row['tablename'] for row in cursor.fetchall()]
            
            print(f"ğŸ“‹ å‘ç° {len(existing_tables)} ä¸ªç°æœ‰è¡¨:")
            for table in existing_tables:
                print(f"   â€¢ {table}")
            
            # æ£€æŸ¥å¢å¼ºç‰ˆè¡¨æ˜¯å¦å­˜åœ¨
            for table_name in enhanced_tables.keys():
                enhanced_tables[table_name] = table_name in existing_tables
                
                status_icon = "âœ…" if enhanced_tables[table_name] else "âŒ"
                print(f"   {status_icon} {table_name}: {'å­˜åœ¨' if enhanced_tables[table_name] else 'ä¸å­˜åœ¨'}")
            
            # ç»Ÿè®¡ç°æœ‰æ•°æ®
            total_records = 0
            for table in existing_tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) as count FROM {table}")
                    count = cursor.fetchone()['count']
                    total_records += count
                    if count > 0:
                        print(f"      â””â”€ {count} æ¡è®°å½•")
                except Exception as e:
                    print(f"      â””â”€ æŸ¥è¯¢å¤±è´¥: {str(e)[:30]}...")
            
            cursor.close()
            conn.close()
            
            existing_count = sum(enhanced_tables.values())
            total_count = len(enhanced_tables)
            
            self.log_operation(
                "è¿œç¨‹è¡¨ç»“æ„æ£€æŸ¥", True,
                f"å¢å¼ºç‰ˆè¡¨: {existing_count}/{total_count} å­˜åœ¨, æ€»è®°å½•æ•°: {total_records}"
            )
            
            return enhanced_tables
            
        except Exception as e:
            self.log_operation(
                "è¿œç¨‹è¡¨ç»“æ„æ£€æŸ¥", False,
                f"æ£€æŸ¥å¤±è´¥: {str(e)}"
            )
            return enhanced_tables
    
    def create_enhanced_document_registry(self) -> bool:
        """åˆ›å»ºå¢å¼ºç‰ˆæ–‡æ¡£æ³¨å†Œè¡¨"""
        print("\nğŸ“„ åˆ›å»ºå¢å¼ºç‰ˆæ–‡æ¡£æ³¨å†Œè¡¨...")
        
        sql = """
        CREATE TABLE IF NOT EXISTS document_registry_enhanced (
            file_id VARCHAR(36) PRIMARY KEY,
            filename VARCHAR(255) NOT NULL,
            content_type VARCHAR(100),
            file_size BIGINT NOT NULL,
            file_hash VARCHAR(64) NOT NULL,
            storage_backend VARCHAR(50) NOT NULL,
            storage_path VARCHAR(500),
            upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            kb_id VARCHAR(36),
            doc_id VARCHAR(36),
            user_id VARCHAR(36),
            metadata TEXT,
            status VARCHAR(20) DEFAULT 'uploaded',
            processing_status VARCHAR(20) DEFAULT 'pending',
            chunk_count INTEGER DEFAULT 0,
            vector_count INTEGER DEFAULT 0,
            es_doc_count INTEGER DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(file_hash)
        );
        """
        
        return self._execute_sql("åˆ›å»ºæ–‡æ¡£æ³¨å†Œè¡¨", sql)
    
    def create_document_chunks_table(self) -> bool:
        """åˆ›å»ºæ–‡æ¡£åˆ‡ç‰‡è¡¨"""
        print("\nğŸ”ª åˆ›å»ºæ–‡æ¡£åˆ‡ç‰‡è¡¨...")
        
        sql = """
        CREATE TABLE IF NOT EXISTS document_chunks (
            chunk_id VARCHAR(36) PRIMARY KEY,
            file_id VARCHAR(36),
            chunk_index INTEGER NOT NULL,
            chunk_text TEXT,
            chunk_size INTEGER,
            chunk_hash VARCHAR(64),
            chunk_metadata TEXT,
            processing_status VARCHAR(20) DEFAULT 'pending',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(file_id, chunk_index)
        );
        """
        
        return self._execute_sql("åˆ›å»ºæ–‡æ¡£åˆ‡ç‰‡è¡¨", sql)
    
    def create_vectors_enhanced_table(self) -> bool:
        """åˆ›å»ºå¢å¼ºç‰ˆå‘é‡å…³è”è¡¨"""
        print("\nğŸ§  åˆ›å»ºå¢å¼ºç‰ˆå‘é‡å…³è”è¡¨...")
        
        sql = """
        CREATE TABLE IF NOT EXISTS document_vectors_enhanced (
            id SERIAL PRIMARY KEY,
            file_id VARCHAR(36),
            chunk_id VARCHAR(36),
            vector_id VARCHAR(100) NOT NULL,
            vector_collection VARCHAR(100),
            vector_index VARCHAR(100),
            embedding_model VARCHAR(100),
            embedding_dimension INTEGER,
            vector_metadata TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(file_id, chunk_id, vector_id)
        );
        """
        
        return self._execute_sql("åˆ›å»ºå‘é‡å…³è”è¡¨", sql)
    
    def create_es_shards_table(self) -> bool:
        """åˆ›å»ºESåˆ†ç‰‡å…³è”è¡¨"""
        print("\nğŸ” åˆ›å»ºESåˆ†ç‰‡å…³è”è¡¨...")
        
        sql = """
        CREATE TABLE IF NOT EXISTS document_es_shards (
            id SERIAL PRIMARY KEY,
            file_id VARCHAR(36),
            chunk_id VARCHAR(36),
            es_index VARCHAR(100) NOT NULL,
            es_doc_id VARCHAR(100) NOT NULL,
            es_shard_id VARCHAR(50),
            es_routing VARCHAR(100),
            es_doc_type VARCHAR(50),
            es_metadata TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(es_index, es_doc_id)
        );
        """
        
        return self._execute_sql("åˆ›å»ºESåˆ†ç‰‡å…³è”è¡¨", sql)
    
    def create_processing_history_table(self) -> bool:
        """åˆ›å»ºå¤„ç†å†å²è¡¨"""
        print("\nğŸ“ˆ åˆ›å»ºå¤„ç†å†å²è¡¨...")
        
        sql = """
        CREATE TABLE IF NOT EXISTS document_processing_history (
            id SERIAL PRIMARY KEY,
            file_id VARCHAR(36),
            operation_type VARCHAR(50) NOT NULL,
            operation_status VARCHAR(20) NOT NULL,
            operation_details TEXT,
            error_message TEXT,
            started_at TIMESTAMP,
            completed_at TIMESTAMP,
            duration_ms INTEGER,
            operated_by VARCHAR(36),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        
        return self._execute_sql("åˆ›å»ºå¤„ç†å†å²è¡¨", sql)
    
    def add_foreign_key_constraints(self) -> bool:
        """æ·»åŠ å¤–é”®çº¦æŸ"""
        print("\nğŸ”— æ·»åŠ å¤–é”®çº¦æŸ...")
        
        constraints = [
            """
            ALTER TABLE document_chunks 
            ADD CONSTRAINT fk_chunks_file_id 
            FOREIGN KEY (file_id) REFERENCES document_registry_enhanced(file_id) 
            ON DELETE CASCADE
            """,
            """
            ALTER TABLE document_vectors_enhanced 
            ADD CONSTRAINT fk_vectors_file_id 
            FOREIGN KEY (file_id) REFERENCES document_registry_enhanced(file_id) 
            ON DELETE CASCADE
            """,
            """
            ALTER TABLE document_vectors_enhanced 
            ADD CONSTRAINT fk_vectors_chunk_id 
            FOREIGN KEY (chunk_id) REFERENCES document_chunks(chunk_id) 
            ON DELETE CASCADE
            """,
            """
            ALTER TABLE document_es_shards 
            ADD CONSTRAINT fk_es_file_id 
            FOREIGN KEY (file_id) REFERENCES document_registry_enhanced(file_id) 
            ON DELETE CASCADE
            """,
            """
            ALTER TABLE document_es_shards 
            ADD CONSTRAINT fk_es_chunk_id 
            FOREIGN KEY (chunk_id) REFERENCES document_chunks(chunk_id) 
            ON DELETE CASCADE
            """,
            """
            ALTER TABLE document_processing_history 
            ADD CONSTRAINT fk_history_file_id 
            FOREIGN KEY (file_id) REFERENCES document_registry_enhanced(file_id) 
            ON DELETE CASCADE
            """
        ]
        
        added_constraints = 0
        for constraint_sql in constraints:
            try:
                if self._execute_sql("æ·»åŠ å¤–é”®çº¦æŸ", constraint_sql, log_details=False):
                    added_constraints += 1
            except Exception as e:
                # å¤–é”®çº¦æŸå¯èƒ½å·²ç»å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
                logger.debug(f"æ·»åŠ å¤–é”®çº¦æŸæ—¶å¯èƒ½é‡åˆ°é‡å¤: {str(e)}")
        
        self.log_operation(
            "æ·»åŠ å¤–é”®çº¦æŸ", True,
            f"æˆåŠŸæ·»åŠ  {added_constraints}/{len(constraints)} ä¸ªçº¦æŸ"
        )
        
        return True
    
    def create_enhanced_indexes(self) -> bool:
        """åˆ›å»ºå¢å¼ºç‰ˆç´¢å¼•"""
        print("\nğŸ—ï¸ åˆ›å»ºå¢å¼ºç‰ˆç´¢å¼•...")
        
        indexes = [
            # æ–‡æ¡£æ³¨å†Œè¡¨ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_filename ON document_registry_enhanced(filename)",
            "CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_hash ON document_registry_enhanced(file_hash)",
            "CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_kb_id ON document_registry_enhanced(kb_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_doc_id ON document_registry_enhanced(doc_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_user_id ON document_registry_enhanced(user_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_status ON document_registry_enhanced(status)",
            "CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_proc_status ON document_registry_enhanced(processing_status)",
            "CREATE INDEX IF NOT EXISTS idx_doc_reg_enh_upload_time ON document_registry_enhanced(upload_time)",
            
            # æ–‡æ¡£åˆ‡ç‰‡è¡¨ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_doc_chunks_file_id ON document_chunks(file_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_chunks_index ON document_chunks(chunk_index)",
            "CREATE INDEX IF NOT EXISTS idx_doc_chunks_hash ON document_chunks(chunk_hash)",
            "CREATE INDEX IF NOT EXISTS idx_doc_chunks_status ON document_chunks(processing_status)",
            
            # å‘é‡æ•°æ®è¡¨ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_file_id ON document_vectors_enhanced(file_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_chunk_id ON document_vectors_enhanced(chunk_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_vector_id ON document_vectors_enhanced(vector_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_collection ON document_vectors_enhanced(vector_collection)",
            "CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_index ON document_vectors_enhanced(vector_index)",
            "CREATE INDEX IF NOT EXISTS idx_doc_vec_enh_model ON document_vectors_enhanced(embedding_model)",
            
            # ESåˆ†ç‰‡è¡¨ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_doc_es_file_id ON document_es_shards(file_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_es_chunk_id ON document_es_shards(chunk_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_es_index ON document_es_shards(es_index)",
            "CREATE INDEX IF NOT EXISTS idx_doc_es_doc_id ON document_es_shards(es_doc_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_es_shard_id ON document_es_shards(es_shard_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_es_routing ON document_es_shards(es_routing)",
            
            # å¤„ç†å†å²è¡¨ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_file_id ON document_processing_history(file_id)",
            "CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_op_type ON document_processing_history(operation_type)",
            "CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_status ON document_processing_history(operation_status)",
            "CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_started ON document_processing_history(started_at)",
            "CREATE INDEX IF NOT EXISTS idx_doc_proc_hist_user ON document_processing_history(operated_by)"
        ]
        
        successful_indexes = 0
        for index_sql in indexes:
            try:
                if self._execute_sql(f"ç´¢å¼•åˆ›å»º", index_sql, log_details=False):
                    successful_indexes += 1
            except Exception as e:
                logger.warning(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {index_sql} - {str(e)}")
        
        self.log_operation(
            "åˆ›å»ºå¢å¼ºç‰ˆç´¢å¼•", True,
            f"æˆåŠŸåˆ›å»º {successful_indexes}/{len(indexes)} ä¸ªç´¢å¼•"
        )
        
        return successful_indexes == len(indexes)
    
    def create_demo_data(self) -> bool:
        """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
        print("\nğŸ² åˆ›å»ºæ¼”ç¤ºæ•°æ®...")
        
        try:
            conn = self._get_db_connection()
            cursor = conn.cursor()
            
            # 1. åˆ›å»ºæµ‹è¯•æ–‡æ¡£
            demo_file_id = "demo_file_" + str(int(time.time()))
            cursor.execute("""
                INSERT INTO document_registry_enhanced 
                (file_id, filename, content_type, file_size, file_hash, storage_backend, 
                 storage_path, kb_id, doc_id, user_id, chunk_count, vector_count, es_doc_count)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (file_hash) DO NOTHING
            """, (
                demo_file_id, "demo_document.pdf", "application/pdf", 1024000, 
                "demo_hash_" + str(int(time.time())), "minio", "/demo/path/demo_document.pdf",
                "demo_kb_001", "demo_doc_001", "demo_user_001", 3, 3, 3
            ))
            
            # 2. åˆ›å»ºæ–‡æ¡£åˆ‡ç‰‡
            for i in range(3):
                chunk_id = f"demo_chunk_{i+1}_{int(time.time())}"
                cursor.execute("""
                    INSERT INTO document_chunks 
                    (chunk_id, file_id, chunk_index, chunk_text, chunk_size, chunk_hash)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """, (
                    chunk_id, demo_file_id, i, f"è¿™æ˜¯æ¼”ç¤ºæ–‡æ¡£çš„ç¬¬{i+1}ä¸ªåˆ‡ç‰‡å†…å®¹...", 
                    200 + i*50, f"chunk_hash_{i+1}_{int(time.time())}"
                ))
                
                # 3. åˆ›å»ºå‘é‡å…³è”
                cursor.execute("""
                    INSERT INTO document_vectors_enhanced 
                    (file_id, chunk_id, vector_id, vector_collection, embedding_model, embedding_dimension)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """, (
                    demo_file_id, chunk_id, f"vector_{i+1}_{int(time.time())}", 
                    "demo_collection", "text-embedding-ada-002", 1536
                ))
                
                # 4. åˆ›å»ºESåˆ†ç‰‡å…³è”
                cursor.execute("""
                    INSERT INTO document_es_shards 
                    (file_id, chunk_id, es_index, es_doc_id, es_shard_id)
                    VALUES (%s, %s, %s, %s, %s)
                """, (
                    demo_file_id, chunk_id, "demo_index", f"es_doc_{i+1}_{int(time.time())}", 
                    f"shard_{i+1}"
                ))
            
            # 5. åˆ›å»ºå¤„ç†å†å²
            cursor.execute("""
                INSERT INTO document_processing_history 
                (file_id, operation_type, operation_status, operation_details, 
                 started_at, completed_at, duration_ms, operated_by)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                demo_file_id, "demo_creation", "completed", 
                "æ¼”ç¤ºæ•°æ®åˆ›å»ºå®Œæˆ", datetime.now(), datetime.now(), 100, "system"
            ))
            
            conn.commit()
            cursor.close()
            conn.close()
            
            self.log_operation(
                "åˆ›å»ºæ¼”ç¤ºæ•°æ®", True,
                f"æ–‡æ¡£ID: {demo_file_id}, åŒ…å«3ä¸ªåˆ‡ç‰‡ã€3ä¸ªå‘é‡ã€3ä¸ªESåˆ†ç‰‡"
            )
            
            return True
            
        except Exception as e:
            self.log_operation(
                "åˆ›å»ºæ¼”ç¤ºæ•°æ®", False,
                f"åˆ›å»ºå¤±è´¥: {str(e)}"
            )
            return False
    
    def verify_remote_upgrade_results(self) -> bool:
        """éªŒè¯è¿œç¨‹å‡çº§ç»“æœ"""
        print("\nğŸ” éªŒè¯è¿œç¨‹å‡çº§ç»“æœ...")
        
        try:
            conn = self._get_db_connection()
            cursor = conn.cursor()
            
            # éªŒè¯è¡¨ç»“æ„
            tables_to_check = [
                'document_registry_enhanced',
                'document_chunks',
                'document_vectors_enhanced',
                'document_es_shards',
                'document_processing_history'
            ]
            
            verification_results = {}
            
            for table in tables_to_check:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' AND table_name = %s
                    )
                """, (table,))
                
                exists = cursor.fetchone()['exists']
                
                if exists:
                    # ç»Ÿè®¡è®°å½•æ•°é‡
                    cursor.execute(f"SELECT COUNT(*) as count FROM {table}")
                    count = cursor.fetchone()['count']
                    verification_results[table] = count
                    print(f"   âœ… {table}: {count} æ¡è®°å½•")
                else:
                    verification_results[table] = -1
                    print(f"   âŒ {table}: ä¸å­˜åœ¨")
            
            # æ£€æŸ¥ç´¢å¼•
            cursor.execute("""
                SELECT count(*) as index_count
                FROM pg_indexes
                WHERE schemaname = 'public'
                AND (indexname LIKE 'idx_doc_%')
            """)
            index_count = cursor.fetchone()['index_count']
            print(f"   ğŸ—ï¸ å¢å¼ºç‰ˆç´¢å¼•: {index_count} ä¸ª")
            
            cursor.close()
            conn.close()
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¡¨éƒ½å­˜åœ¨
            all_exist = all(count >= 0 for count in verification_results.values())
            
            self.log_operation(
                "è¿œç¨‹å‡çº§ç»“æœéªŒè¯", all_exist,
                f"è¡¨å­˜åœ¨çŠ¶æ€: {sum(1 for c in verification_results.values() if c >= 0)}/{len(tables_to_check)}, ç´¢å¼•: {index_count}"
            )
            
            return all_exist
            
        except Exception as e:
            self.log_operation(
                "è¿œç¨‹å‡çº§ç»“æœéªŒè¯", False,
                f"éªŒè¯å¤±è´¥: {str(e)}"
            )
            return False
    
    def _execute_sql(self, operation_name: str, sql: str, log_details: bool = True) -> bool:
        """æ‰§è¡ŒSQLè¯­å¥"""
        try:
            start_time = time.time()
            conn = self._get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute(sql)
            conn.commit()
            
            cursor.close()
            conn.close()
            
            duration_ms = int((time.time() - start_time) * 1000)
            
            if log_details:
                self.log_operation(operation_name, True, "æ‰§è¡ŒæˆåŠŸ", duration_ms)
            
            return True
            
        except Exception as e:
            if log_details:
                self.log_operation(operation_name, False, f"æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def execute_remote_full_upgrade(self) -> bool:
        """æ‰§è¡Œå®Œæ•´çš„è¿œç¨‹æ•°æ®åº“å‡çº§"""
        print("ğŸš€ å¼€å§‹æ‰§è¡Œè¿œç¨‹æ•°æ®åº“å¢å¼ºç‰ˆå‡çº§...")
        print(f"ğŸŒ ç›®æ ‡æœåŠ¡å™¨: {self.db_config['host']}:{self.db_config['port']}")
        print("="*80)
        
        upgrade_steps = [
            ("è¿œç¨‹æ•°æ®åº“è¿æ¥æµ‹è¯•", self.test_remote_database_connection),
            ("æ£€æŸ¥è¿œç¨‹ç°æœ‰è¡¨ç»“æ„", lambda: self.check_existing_remote_tables() is not None),
            ("åˆ›å»ºæ–‡æ¡£æ³¨å†Œè¡¨", self.create_enhanced_document_registry),
            ("åˆ›å»ºæ–‡æ¡£åˆ‡ç‰‡è¡¨", self.create_document_chunks_table),
            ("åˆ›å»ºå‘é‡å…³è”è¡¨", self.create_vectors_enhanced_table),
            ("åˆ›å»ºESåˆ†ç‰‡å…³è”è¡¨", self.create_es_shards_table),
            ("åˆ›å»ºå¤„ç†å†å²è¡¨", self.create_processing_history_table),
            ("æ·»åŠ å¤–é”®çº¦æŸ", self.add_foreign_key_constraints),
            ("åˆ›å»ºæ•°æ®åº“ç´¢å¼•", self.create_enhanced_indexes),
            ("åˆ›å»ºæ¼”ç¤ºæ•°æ®", self.create_demo_data),
            ("éªŒè¯è¿œç¨‹å‡çº§ç»“æœ", self.verify_remote_upgrade_results)
        ]
        
        successful_steps = 0
        for step_name, step_func in upgrade_steps:
            try:
                success = step_func()
                if success:
                    successful_steps += 1
                else:
                    print(f"\nâš ï¸ æ­¥éª¤å¤±è´¥: {step_name}")
            except Exception as e:
                print(f"\nâŒ æ­¥éª¤å¼‚å¸¸: {step_name} - {str(e)}")
                self.errors.append(f"{step_name}: {str(e)}")
        
        overall_success = successful_steps == len(upgrade_steps)
        
        # ç”Ÿæˆå‡çº§æŠ¥å‘Š
        self._generate_remote_upgrade_report(overall_success)
        
        return overall_success
    
    def _generate_remote_upgrade_report(self, overall_success: bool):
        """ç”Ÿæˆè¿œç¨‹å‡çº§æŠ¥å‘Š"""
        end_time = datetime.now()
        total_duration = int((end_time - self.start_time).total_seconds() * 1000)
        
        print("\n" + "="*80)
        print("ğŸ“Š è¿œç¨‹æ•°æ®åº“å¢å¼ºç‰ˆå‡çº§æŠ¥å‘Š")
        print("="*80)
        
        print(f"\nğŸŒ è¿œç¨‹æœåŠ¡å™¨ä¿¡æ¯:")
        print(f"   åœ°å€: {self.db_config['host']}:{self.db_config['port']}")
        print(f"   æ•°æ®åº“: {self.db_config['database']}")
        print(f"   ç”¨æˆ·: {self.db_config['user']}")
        
        print(f"\nâ±ï¸ å‡çº§ç»Ÿè®¡:")
        print(f"   å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   æ€»è€—æ—¶: {total_duration}ms")
        print(f"   æ€»æ“ä½œæ•°: {len(self.upgrade_results)}")
        
        successful_ops = sum(1 for r in self.upgrade_results if r["success"])
        print(f"   æˆåŠŸæ“ä½œ: {successful_ops}")
        print(f"   å¤±è´¥æ“ä½œ: {len(self.upgrade_results) - successful_ops}")
        
        # æ˜¾ç¤ºæ“ä½œè¯¦æƒ…
        print(f"\nğŸ“‹ æ“ä½œè¯¦æƒ…:")
        for result in self.upgrade_results:
            status_icon = "âœ…" if result["success"] else "âŒ"
            print(f"   {status_icon} {result['operation']}")
            if result["details"]:
                print(f"      â””â”€ {result['details']}")
        
        # æ˜¾ç¤ºé”™è¯¯
        if self.errors:
            print(f"\nâŒ é”™è¯¯åˆ—è¡¨:")
            for error in self.errors:
                print(f"   â€¢ {error}")
        
        # æ€»ç»“
        print(f"\n{'='*80}")
        if overall_success:
            print("ğŸ‰ è¿œç¨‹æ•°æ®åº“å¢å¼ºç‰ˆå‡çº§æˆåŠŸ!")
            print("ğŸ’¡ æ‰€æœ‰å¢å¼ºç‰ˆè¡¨ç»“æ„å·²åœ¨è¿œç¨‹æœåŠ¡å™¨åˆ›å»ºï¼Œç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªã€‚")
            print("ğŸ“ æ¥ä¸‹æ¥å¯ä»¥:")
            print("   1. ä½¿ç”¨å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†å™¨è¿æ¥è¿œç¨‹æ•°æ®åº“")
            print("   2. é…ç½®åº”ç”¨ç¨‹åºä½¿ç”¨è¿œç¨‹æ•°æ®åº“è¿æ¥")
            print("   3. è¿è¡Œå®Œæ•´ç³»ç»Ÿæµ‹è¯•éªŒè¯è¿œç¨‹è¿æ¥")
        else:
            print("âŒ è¿œç¨‹æ•°æ®åº“å¢å¼ºç‰ˆå‡çº§éƒ¨åˆ†å¤±è´¥!")
            print("ğŸ’¡ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å’Œç½‘ç»œè¿æ¥ï¼Œç„¶åé‡è¯•å¤±è´¥çš„æ­¥éª¤ã€‚")
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºæ”¿çŸ¥è¯†åº“è¿œç¨‹æ•°æ®åº“å¢å¼ºç‰ˆè‡ªåŠ¨åŒ–å‡çº§å·¥å…·")
    print("åŸºäºæ˜¨å¤©çš„è¿œç¨‹æ•°æ®åº“è¿æ¥é…ç½®")
    print("åŒ…å«ï¼šæ–‡æ¡£æ³¨å†Œå¢å¼º + åˆ‡ç‰‡è¿½è¸ª + å‘é‡å…³è” + ESåˆ†ç‰‡å…³è” + å¤„ç†å†å²")
    print("="*80)
    
    print(f"ğŸŒ ç›®æ ‡è¿œç¨‹æœåŠ¡å™¨ä¿¡æ¯:")
    print(f"   åœ°å€: {REMOTE_DB_CONFIG['host']}:{REMOTE_DB_CONFIG['port']}")
    print(f"   æ•°æ®åº“: {REMOTE_DB_CONFIG['database']}")
    print(f"   ç”¨æˆ·: {REMOTE_DB_CONFIG['user']}")
    
    try:
        upgrader = RemoteDatabaseUpgrader()
        success = upgrader.execute_remote_full_upgrade()
        
        return success
        
    except Exception as e:
        print(f"\nâŒ è¿œç¨‹å‡çº§è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        return False


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¿œç¨‹å‡çº§")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1) 