#!/usr/bin/env python3
"""
æ•°æ®åº“è¿›åº¦ç›‘æ§ç³»ç»Ÿ - å¢å¼ºç‰ˆ
æä¾›å…¨é¢çš„æ•°æ®åº“çŠ¶æ€ç›‘æ§ã€æ€§èƒ½åˆ†æã€å‘Šè­¦å’ŒæŠ¥å‘ŠåŠŸèƒ½
"""

import psycopg2
import psycopg2.extras
from datetime import datetime, timedelta
import time
import sys
import json
import os
from pathlib import Path
import argparse
from typing import Dict, List, Optional, Tuple
import threading
import signal
from dataclasses import dataclass, asdict
from enum import Enum

# è¿œç¨‹æ•°æ®åº“è¿æ¥é…ç½®
REMOTE_DB_CONFIG = {
    'host': '167.71.85.231',
    'port': 5432,
    'user': 'zzdsj',
    'password': 'zzdsj123',
    'database': 'zzdsj'
}

class AlertLevel(Enum):
    """å‘Šè­¦çº§åˆ«"""
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

@dataclass
class DatabaseMetrics:
    """æ•°æ®åº“æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    table_count: int
    total_records: int
    database_size: str
    connection_count: int
    active_queries: int
    slow_queries: int
    cache_hit_ratio: float
    disk_usage: Dict[str, float]
    memory_usage: float
    cpu_usage: float
    status: str

@dataclass
class TableInfo:
    """è¡¨ä¿¡æ¯æ•°æ®ç±»"""
    name: str
    row_count: int
    size: str
    indexes: int
    last_vacuum: Optional[str]
    last_analyze: Optional[str]

class DatabaseMonitor:
    """æ•°æ®åº“ç›‘æ§å™¨"""
    
    def __init__(self, config: Dict = None):
        self.config = config or REMOTE_DB_CONFIG
        self.monitoring = False
        self.metrics_history = []
        self.alerts = []
        self.last_check_time = None
        
    def get_connection(self) -> Optional[psycopg2.extensions.connection]:
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            conn = psycopg2.connect(**self.config)
            return conn
        except Exception as e:
            self._add_alert(AlertLevel.ERROR, f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return None
    
    def _add_alert(self, level: AlertLevel, message: str):
        """æ·»åŠ å‘Šè­¦"""
        alert = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'level': level.value,
            'message': message
        }
        self.alerts.append(alert)
        print(f"ğŸš¨ [{level.value}] {message}")
    
    def check_database_health(self) -> Optional[DatabaseMetrics]:
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶å†µ"""
        conn = self.get_connection()
        if not conn:
            return None
        
        try:
            cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # 1. åŸºç¡€è¡¨ç»Ÿè®¡
            cursor.execute("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = 'public';
            """)
            table_count = cursor.fetchone()[0]
            
            # 2. æ€»è®°å½•æ•°
            cursor.execute("""
                SELECT schemaname, tablename FROM pg_tables 
                WHERE schemaname = 'public';
            """)
            tables = cursor.fetchall()
            
            total_records = 0
            for table in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table['tablename']};")
                    count = cursor.fetchone()[0]
                    total_records += count
                except:
                    pass
            
            # 3. æ•°æ®åº“å¤§å°
            cursor.execute("""
                SELECT pg_size_pretty(pg_database_size(current_database()));
            """)
            database_size = cursor.fetchone()[0]
            
            # 4. è¿æ¥æ•°ç»Ÿè®¡
            cursor.execute("""
                SELECT COUNT(*) FROM pg_stat_activity 
                WHERE datname = current_database();
            """)
            connection_count = cursor.fetchone()[0]
            
            # 5. æ´»è·ƒæŸ¥è¯¢æ•°
            cursor.execute("""
                SELECT COUNT(*) FROM pg_stat_activity 
                WHERE state = 'active' AND datname = current_database();
            """)
            active_queries = cursor.fetchone()[0]
            
            # 6. æ…¢æŸ¥è¯¢æ•° (è¶…è¿‡1ç§’)
            cursor.execute("""
                SELECT COUNT(*) FROM pg_stat_activity 
                WHERE state = 'active' 
                AND datname = current_database()
                AND now() - query_start > interval '1 second';
            """)
            slow_queries = cursor.fetchone()[0]
            
            # 7. ç¼“å­˜å‘½ä¸­ç‡
            cursor.execute("""
                SELECT 
                    ROUND(
                        100.0 * (sum(blks_hit) / NULLIF(sum(blks_hit) + sum(blks_read), 0)), 2
                    ) as cache_hit_ratio
                FROM pg_stat_database 
                WHERE datname = current_database();
            """)
            cache_hit_ratio = cursor.fetchone()[0] or 0.0
            
            # 8. ç£ç›˜ä½¿ç”¨æƒ…å†µ
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size_pretty,
                    pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
                FROM pg_tables 
                WHERE schemaname = 'public'
                ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
                LIMIT 10;
            """)
            disk_stats = cursor.fetchall()
            disk_usage = {
                row['tablename']: round(row['size_bytes'] / (1024*1024), 2)  # MB
                for row in disk_stats
            }
            
            # 9. å†…å­˜ä½¿ç”¨ (ä¼°ç®—)
            cursor.execute("SELECT setting FROM pg_settings WHERE name = 'shared_buffers';")
            shared_buffers = cursor.fetchone()[0]
            memory_usage = self._parse_memory_setting(shared_buffers)
            
            # 10. CPUä½¿ç”¨ (é€šè¿‡æ´»è·ƒè¿æ¥ä¼°ç®—)
            cpu_usage = min(100.0, (active_queries / max(connection_count, 1)) * 100)
            
            # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
            metrics = DatabaseMetrics(
                timestamp=timestamp,
                table_count=table_count,
                total_records=total_records,
                database_size=database_size,
                connection_count=connection_count,
                active_queries=active_queries,
                slow_queries=slow_queries,
                cache_hit_ratio=cache_hit_ratio,
                disk_usage=disk_usage,
                memory_usage=memory_usage,
                cpu_usage=cpu_usage,
                status="healthy"
            )
            
            # å¥åº·çŠ¶å†µè¯„ä¼°
            self._evaluate_health(metrics)
            
            cursor.close()
            conn.close()
            
            return metrics
            
        except Exception as e:
            self._add_alert(AlertLevel.ERROR, f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return None
    
    def _parse_memory_setting(self, setting: str) -> float:
        """è§£æPostgreSQLå†…å­˜è®¾ç½®"""
        try:
            if 'MB' in setting:
                return float(setting.replace('MB', ''))
            elif 'GB' in setting:
                return float(setting.replace('GB', '')) * 1024
            elif 'kB' in setting:
                return float(setting.replace('kB', '')) / 1024
            else:
                # é»˜è®¤å•ä½æ˜¯8kBå—
                return float(setting) * 8 / 1024
        except:
            return 0.0
    
    def _evaluate_health(self, metrics: DatabaseMetrics):
        """è¯„ä¼°æ•°æ®åº“å¥åº·çŠ¶å†µ"""
        # æ£€æŸ¥æ…¢æŸ¥è¯¢
        if metrics.slow_queries > 5:
            self._add_alert(AlertLevel.WARNING, f"æ£€æµ‹åˆ° {metrics.slow_queries} ä¸ªæ…¢æŸ¥è¯¢")
        
        # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
        if metrics.cache_hit_ratio < 80:
            self._add_alert(AlertLevel.WARNING, f"ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {metrics.cache_hit_ratio}%")
        
        # æ£€æŸ¥è¿æ¥æ•°
        if metrics.connection_count > 100:
            self._add_alert(AlertLevel.WARNING, f"è¿æ¥æ•°è¿‡å¤š: {metrics.connection_count}")
        
        # æ£€æŸ¥æ´»è·ƒæŸ¥è¯¢
        if metrics.active_queries > 20:
            self._add_alert(AlertLevel.ERROR, f"æ´»è·ƒæŸ¥è¯¢æ•°è¿‡å¤š: {metrics.active_queries}")
    
    def get_table_details(self) -> List[TableInfo]:
        """è·å–è¯¦ç»†çš„è¡¨ä¿¡æ¯"""
        conn = self.get_connection()
        if not conn:
            return []
        
        try:
            cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
            
            # è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯
            cursor.execute("""
                SELECT 
                    t.tablename,
                    t.schemaname,
                    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                    (SELECT COUNT(*) FROM information_schema.table_constraints 
                     WHERE table_name = t.tablename AND constraint_type = 'PRIMARY KEY') as pk_count,
                    s.n_tup_ins as inserts,
                    s.n_tup_upd as updates,
                    s.n_tup_del as deletes,
                    s.last_vacuum,
                    s.last_autovacuum,
                    s.last_analyze,
                    s.last_autoanalyze
                FROM pg_tables t
                LEFT JOIN pg_stat_user_tables s ON s.relname = t.tablename
                WHERE t.schemaname = 'public'
                ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
            """)
            
            tables_data = cursor.fetchall()
            table_infos = []
            
            for row in tables_data:
                # è·å–è¡Œæ•°
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {row['tablename']};")
                    row_count = cursor.fetchone()[0]
                except:
                    row_count = 0
                
                # è·å–ç´¢å¼•æ•°
                cursor.execute("""
                    SELECT COUNT(*) FROM pg_indexes 
                    WHERE tablename = %s AND schemaname = 'public';
                """, (row['tablename'],))
                index_count = cursor.fetchone()[0]
                
                table_info = TableInfo(
                    name=row['tablename'],
                    row_count=row_count,
                    size=row['size'],
                    indexes=index_count,
                    last_vacuum=str(row['last_vacuum']) if row['last_vacuum'] else None,
                    last_analyze=str(row['last_analyze']) if row['last_analyze'] else None
                )
                table_infos.append(table_info)
            
            cursor.close()
            conn.close()
            
            return table_infos
            
        except Exception as e:
            self._add_alert(AlertLevel.ERROR, f"è·å–è¡¨è¯¦æƒ…å¤±è´¥: {e}")
            return []
    
    def generate_report(self, metrics: DatabaseMetrics, tables: List[TableInfo]) -> str:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        report = f"""
{'='*80}
ğŸ“Š æ•°æ®åº“ç›‘æ§æŠ¥å‘Š - {metrics.timestamp}
{'='*80}

ğŸ¥ å¥åº·çŠ¶å†µæ€»è§ˆ:
  çŠ¶æ€: {metrics.status.upper()}
  æ•°æ®åº“å¤§å°: {metrics.database_size}
  ç¼“å­˜å‘½ä¸­ç‡: {metrics.cache_hit_ratio}%
  
ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:
  è¿æ¥æ•°: {metrics.connection_count}
  æ´»è·ƒæŸ¥è¯¢: {metrics.active_queries}
  æ…¢æŸ¥è¯¢: {metrics.slow_queries}
  å†…å­˜ä½¿ç”¨: {metrics.memory_usage:.1f} MB
  CPUä½¿ç”¨ç‡: {metrics.cpu_usage:.1f}%

ğŸ“‹ æ•°æ®ç»Ÿè®¡:
  è¡¨æ•°é‡: {metrics.table_count}
  æ€»è®°å½•æ•°: {metrics.total_records:,}

ğŸ“Š è¡¨ç©ºé—´ä½¿ç”¨ (Top 5):"""
        
        # è¡¨ç©ºé—´ä½¿ç”¨æ’åº
        sorted_tables = sorted(tables, key=lambda x: x.row_count, reverse=True)[:5]
        for i, table in enumerate(sorted_tables, 1):
            report += f"\n  {i:2}. {table.name:25} | {table.row_count:>10,} è¡Œ | {table.size:>10} | {table.indexes} ç´¢å¼•"
        
        if self.alerts:
            report += f"\n\nğŸš¨ å‘Šè­¦ä¿¡æ¯ ({len(self.alerts)} æ¡):"
            for alert in self.alerts[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5æ¡å‘Šè­¦
                report += f"\n  [{alert['level']:8}] {alert['timestamp']} - {alert['message']}"
        
        report += f"\n\n{'='*80}\n"
        return report
    
    def save_metrics(self, metrics: DatabaseMetrics):
        """ä¿å­˜ç›‘æ§æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        metrics_dir = Path("monitoring_data")
        metrics_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        metrics_file = metrics_dir / f"db_metrics_{datetime.now().strftime('%Y%m%d')}.json"
        
        metrics_data = asdict(metrics)
        
        # è¯»å–ç°æœ‰æ•°æ®
        if metrics_file.exists():
            with open(metrics_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        else:
            existing_data = []
        
        # æ·»åŠ æ–°æ•°æ®
        existing_data.append(metrics_data)
        
        # ä¿å­˜æ•°æ®
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, indent=2, ensure_ascii=False)
    
    def start_monitoring(self, interval: int = 60):
        """å¼€å§‹æŒç»­ç›‘æ§"""
        print(f"ğŸ”„ å¼€å§‹æ•°æ®åº“ç›‘æ§ï¼Œæ£€æŸ¥é—´éš”: {interval} ç§’")
        print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§\n")
        
        self.monitoring = True
        
        def signal_handler(signum, frame):
            print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢ç›‘æ§...")
            self.monitoring = False
        
        signal.signal(signal.SIGINT, signal_handler)
        
        while self.monitoring:
            try:
                # æ‰§è¡Œå¥åº·æ£€æŸ¥
                metrics = self.check_database_health()
                if metrics:
                    tables = self.get_table_details()
                    
                    # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
                    report = self.generate_report(metrics, tables)
                    print(report)
                    
                    # ä¿å­˜æŒ‡æ ‡
                    self.save_metrics(metrics)
                    self.metrics_history.append(metrics)
                    
                    # ä¿ç•™æœ€è¿‘100æ¡è®°å½•
                    if len(self.metrics_history) > 100:
                        self.metrics_history = self.metrics_history[-100:]
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                if self.monitoring:
                    time.sleep(interval)
                    
            except Exception as e:
                self._add_alert(AlertLevel.ERROR, f"ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(5)  # å‡ºé”™åçŸ­æš‚ç­‰å¾…
        
        print("ğŸ“´ ç›‘æ§å·²åœæ­¢")

def quick_check():
    """å¿«é€Ÿæ£€æŸ¥æ•°æ®åº“çŠ¶æ€"""
    monitor = DatabaseMonitor()
    
    print("ğŸ” æ­£åœ¨æ‰§è¡Œå¿«é€Ÿæ•°æ®åº“æ£€æŸ¥...")
    metrics = monitor.check_database_health()
    
    if metrics:
        tables = monitor.get_table_details()
        report = monitor.generate_report(metrics, tables)
        print(report)
        return True
    else:
        print("âŒ å¿«é€Ÿæ£€æŸ¥å¤±è´¥")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®åº“ç›‘æ§ç³»ç»Ÿ")
    parser.add_argument('--mode', choices=['quick', 'monitor'], default='quick',
                       help='è¿è¡Œæ¨¡å¼: quick(å¿«é€Ÿæ£€æŸ¥) æˆ– monitor(æŒç»­ç›‘æ§)')
    parser.add_argument('--interval', type=int, default=60,
                       help='ç›‘æ§é—´éš”(ç§’)ï¼Œé»˜è®¤60ç§’')
    
    args = parser.parse_args()
    
    if args.mode == 'quick':
        quick_check()
    elif args.mode == 'monitor':
        monitor = DatabaseMonitor()
        monitor.start_monitoring(args.interval)

if __name__ == "__main__":
    main() 