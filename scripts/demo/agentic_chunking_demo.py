#!/usr/bin/env python3
"""
Agenticæ–‡æ¡£åˆ‡åˆ†åŠŸèƒ½æ¼”ç¤ºè„šæœ¬
å±•ç¤ºåŸºäºAgnoæ¡†æ¶çš„æ™ºèƒ½æ–‡æ¡£åˆ‡åˆ†èƒ½åŠ›
"""

import asyncio
import sys
import os
from pathlib import Path
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.tools.advanced.agentic_chunking import (
    AgenticDocumentChunker,
    AgenticChunkingConfig,
    AgenticChunkingStrategy,
    create_agentic_chunker,
    agentic_chunk_text
)

from app.tools.advanced.agentic_chunking_integration import (
    get_agentic_chunking_manager,
    smart_chunk_text,
    get_chunking_recommendations
)

class AgenticChunkingDemo:
    """Agenticåˆ‡åˆ†æ¼”ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤º"""
        self.sample_texts = self._prepare_sample_texts()
        print("ğŸš€ Agenticæ–‡æ¡£åˆ‡åˆ†æ¼”ç¤ºåˆå§‹åŒ–å®Œæˆ")
    
    def _prepare_sample_texts(self) -> dict:
        """å‡†å¤‡ç¤ºä¾‹æ–‡æœ¬"""
        return {
            "technical_doc": """
# APIæ–‡æ¡£ç¤ºä¾‹

## ç”¨æˆ·è®¤è¯æ¥å£

### POST /api/auth/login
ç”¨äºç”¨æˆ·ç™»å½•è®¤è¯ã€‚

**è¯·æ±‚å‚æ•°ï¼š**
- username: ç”¨æˆ·åï¼ˆå¿…å¡«ï¼‰
- password: å¯†ç ï¼ˆå¿…å¡«ï¼‰

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "data": {
    "token": "eyJhbGciOiJIUzI1NiIs...",
    "user": {
      "id": 1,
      "username": "admin"
    }
  }
}
```

## æ•°æ®æŸ¥è¯¢æ¥å£

### GET /api/data/users
è·å–ç”¨æˆ·åˆ—è¡¨ã€‚

**æŸ¥è¯¢å‚æ•°ï¼š**
- page: é¡µç ï¼ˆå¯é€‰ï¼Œé»˜è®¤1ï¼‰
- limit: æ¯é¡µæ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤10ï¼‰

**å“åº”æ ¼å¼ï¼š**
```json
{
  "success": true,
  "data": {
    "users": [...],
    "total": 100,
    "page": 1
  }
}
```
""",
            
            "academic_paper": """
æ‘˜è¦

æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨åŠå…¶å‘å±•å‰æ™¯ã€‚é€šè¿‡æ·±åº¦å­¦ä¹ ç®—æ³•çš„åº”ç”¨ï¼ŒåŒ»ç–—AIç³»ç»Ÿåœ¨å›¾åƒè¯†åˆ«ã€ç–¾ç—…é¢„æµ‹å’Œæ²»ç–—æ–¹æ¡ˆæ¨èæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚

1. å¼•è¨€

äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—å¥åº·é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ã€‚æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯ä¸ºåŒ»ç–—è¯Šæ–­å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ã€‚

2. ç›¸å…³å·¥ä½œ

2.1 å›¾åƒè¯Šæ–­æŠ€æœ¯
å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAIç³»ç»Ÿåœ¨æŸäº›ç–¾ç—…çš„è¯Šæ–­å‡†ç¡®ç‡å·²ç»è¾¾åˆ°æˆ–è¶…è¿‡ä¸“ä¸šåŒ»ç”Ÿçš„æ°´å¹³ã€‚

2.2 è‡ªç„¶è¯­è¨€å¤„ç†åœ¨åŒ»ç–—ä¸­çš„åº”ç”¨
é€šè¿‡åˆ†æç”µå­ç—…å†å’ŒåŒ»å­¦æ–‡çŒ®ï¼ŒNLPæŠ€æœ¯èƒ½å¤Ÿè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­å†³ç­–ï¼Œæé«˜åŒ»ç–—æœåŠ¡æ•ˆç‡ã€‚

3. æ–¹æ³•è®º

æœ¬ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆå›¾åƒæ•°æ®å’Œæ–‡æœ¬æ•°æ®ï¼Œæ„å»ºäº†ç»¼åˆæ€§çš„åŒ»ç–—è¯Šæ–­ç³»ç»Ÿã€‚

4. å®éªŒç»“æœ

å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°95.2%ã€‚

5. ç»“è®º

AIæŠ€æœ¯åœ¨åŒ»ç–—è¯Šæ–­é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œä½†ä»éœ€è¦è§£å†³æ•°æ®éšç§ã€ç®—æ³•å¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ã€‚
""",
            
            "conversation": """
å®¢æœ: æ‚¨å¥½ï¼æ¬¢è¿å’¨è¯¢æˆ‘ä»¬çš„åœ¨çº¿å®¢æœï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ

ç”¨æˆ·: ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ä½ ä»¬çš„äº§å“ä¿ä¿®æ”¿ç­–ã€‚

å®¢æœ: å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†ä»‹ç»ã€‚æˆ‘ä»¬çš„äº§å“æ ¹æ®ä¸åŒç±»åˆ«æœ‰ä¸åŒçš„ä¿ä¿®æœŸï¼š
- ç”µå­äº§å“ï¼š1å¹´ä¿ä¿®
- å®¶ç”µäº§å“ï¼š2å¹´ä¿ä¿®  
- æ•°ç é…ä»¶ï¼š6ä¸ªæœˆä¿ä¿®

ç”¨æˆ·: é‚£å¦‚æœäº§å“åœ¨ä¿ä¿®æœŸå†…å‡ºç°è´¨é‡é—®é¢˜æ€ä¹ˆåŠï¼Ÿ

å®¢æœ: å¦‚æœæ˜¯è´¨é‡é—®é¢˜ï¼Œæˆ‘ä»¬æä¾›ä»¥ä¸‹æœåŠ¡ï¼š
1. å…è´¹ç»´ä¿®
2. å¦‚æœæ— æ³•ç»´ä¿®ï¼Œå¯ä»¥å…è´¹æ›´æ¢åŒå‹å·äº§å“
3. å¦‚æœæ²¡æœ‰åŒå‹å·ï¼Œå¯ä»¥æ›´æ¢åŒä»·å€¼çš„å…¶ä»–äº§å“

ç”¨æˆ·: éœ€è¦æä¾›ä»€ä¹ˆææ–™å—ï¼Ÿ

å®¢æœ: éœ€è¦æ‚¨æä¾›ï¼š
- è´­ä¹°å‡­è¯ï¼ˆå‘ç¥¨æˆ–è®¢å•æˆªå›¾ï¼‰
- äº§å“åºåˆ—å·
- é—®é¢˜æè¿°å’Œç…§ç‰‡

ç”¨æˆ·: æ˜ç™½äº†ï¼Œè°¢è°¢ï¼

å®¢æœ: ä¸å®¢æ°”ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶è”ç³»æˆ‘ä»¬ã€‚ç¥æ‚¨ç”Ÿæ´»æ„‰å¿«ï¼
""",
            
            "news_article": """
ç§‘æŠ€å‰æ²¿ï¼šé‡å­è®¡ç®—æœºå–å¾—é‡å¤§çªç ´

æ®æœ€æ–°æŠ¥é“ï¼ŒIBMå…¬å¸å®£å¸ƒå…¶é‡å­è®¡ç®—æœºåœ¨å¤„ç†ç‰¹å®šç®—æ³•æ–¹é¢å®ç°äº†é‡å¤§çªç ´ï¼Œé‡å­ä¼˜åŠ¿å¾—åˆ°è¿›ä¸€æ­¥éªŒè¯ã€‚

é‡å­è®¡ç®—çš„å‘å±•å†ç¨‹

é‡å­è®¡ç®—æŠ€æœ¯çš„å‘å±•å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª80å¹´ä»£ã€‚1982å¹´ï¼Œç‰©ç†å­¦å®¶ç†æŸ¥å¾·Â·è´¹æ›¼é¦–æ¬¡æå‡ºäº†é‡å­è®¡ç®—çš„æ¦‚å¿µã€‚æ­¤åï¼Œè¿™ä¸€é¢†åŸŸç»å†äº†å‡ ä¸ªé‡è¦çš„å‘å±•é˜¶æ®µã€‚

æŠ€æœ¯çªç ´çš„æ„ä¹‰

æ­¤æ¬¡çªç ´çš„æ„ä¹‰åœ¨äºï¼šé¦–å…ˆï¼Œå®ƒè¯æ˜äº†é‡å­è®¡ç®—æœºåœ¨æŸäº›ç‰¹å®šé—®é¢˜ä¸Šç¡®å®èƒ½å¤Ÿè¶…è¶Šç»å…¸è®¡ç®—æœºã€‚å…¶æ¬¡ï¼Œè¿™ä¸ºæœªæ¥çš„å•†ä¸šåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚

åº”ç”¨å‰æ™¯å±•æœ›

ä¸“å®¶é¢„æµ‹ï¼Œé‡å­è®¡ç®—å°†åœ¨ä»¥ä¸‹é¢†åŸŸäº§ç”Ÿé‡å¤§å½±å“ï¼š
- å¯†ç å­¦å’Œç½‘ç»œå®‰å…¨
- è¯ç‰©ç ”å‘å’Œåˆ†å­æ¨¡æ‹Ÿ
- é‡‘èå»ºæ¨¡å’Œé£é™©åˆ†æ
- äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ 

æŒ‘æˆ˜ä¸å›°éš¾

å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œé‡å­è®¡ç®—ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼šé‡å­æ¯”ç‰¹çš„ç¨³å®šæ€§é—®é¢˜ã€é‡å­çº é”™æŠ€æœ¯çš„å®Œå–„ã€ä»¥åŠå¤§è§„æ¨¡å•†ä¸šåŒ–çš„æˆæœ¬é—®é¢˜ã€‚

ç»“è¯­

é‡å­è®¡ç®—æŠ€æœ¯çš„å‘å±•å‰æ™¯å¹¿é˜”ï¼Œä½†ä»éœ€è¦æŒç»­çš„ç ”ç©¶æŠ•å…¥å’ŒæŠ€æœ¯åˆ›æ–°ã€‚
"""
        }
    
    async def demo_basic_chunking(self):
        """æ¼”ç¤ºåŸºç¡€åˆ‡åˆ†åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ“ åŸºç¡€Agenticåˆ‡åˆ†æ¼”ç¤º")
        print("="*60)
        
        strategy = AgenticChunkingStrategy.SEMANTIC_BOUNDARY
        sample_text = self.sample_texts["academic_paper"]
        
        print(f"âœ¨ ä½¿ç”¨ç­–ç•¥: {strategy.value}")
        print(f"ğŸ“„ æ–‡æ¡£é•¿åº¦: {len(sample_text)} å­—ç¬¦")
        
        try:
            # åˆ›å»ºåŸºç¡€é…ç½®
            config = AgenticChunkingConfig(
                strategy=strategy,
                max_chunk_size=1000,
                min_chunk_size=100,
                language="zh"
            )
            
            chunker = AgenticDocumentChunker(config)
            result = await chunker.chunk_document(sample_text)
            
            print(f"âœ… åˆ‡åˆ†å®Œæˆ!")
            print(f"ğŸ“Š ç”Ÿæˆå—æ•°: {result.total_chunks}")
            print(f"ğŸ“ å—å¤§å°èŒƒå›´: {min(result.chunk_sizes)} - {max(result.chunk_sizes)} å­—ç¬¦")
            
            # æ˜¾ç¤ºåˆ‡åˆ†ç»“æœ
            for i, chunk in enumerate(result.chunks):
                print(f"\nğŸ“‹ å— {i+1} ({len(chunk.content)} å­—ç¬¦):")
                print("-" * 40)
                preview = chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content
                print(preview)
                
        except Exception as e:
            print(f"âŒ åˆ‡åˆ†å¤±è´¥: {str(e)}")
    
    async def demo_strategy_comparison(self):
        """æ¼”ç¤ºä¸åŒç­–ç•¥çš„å¯¹æ¯”"""
        print("\n" + "="*60)
        print("ğŸ” ç­–ç•¥å¯¹æ¯”æ¼”ç¤º")
        print("="*60)
        
        sample_text = self.sample_texts["technical_doc"]
        strategies = [
            AgenticChunkingStrategy.SEMANTIC_BOUNDARY,
            AgenticChunkingStrategy.TOPIC_TRANSITION,
            AgenticChunkingStrategy.TECHNICAL_DOCUMENT
        ]
        
        results = {}
        
        for strategy in strategies:
            print(f"\nğŸš€ æµ‹è¯•ç­–ç•¥: {strategy.value}")
            
            try:
                config = AgenticChunkingConfig(
                    strategy=strategy,
                    max_chunk_size=800,
                    language="zh"
                )
                
                chunker = AgenticDocumentChunker(config)
                result = await chunker.chunk_document(sample_text)
                
                results[strategy.value] = result
                
                print(f"  ğŸ“Š å—æ•°: {result.total_chunks}")
                print(f"  ğŸ“ å¹³å‡å¤§å°: {sum(result.chunk_sizes)/len(result.chunk_sizes):.0f} å­—ç¬¦")
                
            except Exception as e:
                print(f"  âŒ ç­–ç•¥ {strategy.value} å¤±è´¥: {str(e)}")
        
        if results:
            best_strategy = max(results.items(), key=lambda x: x[1].total_chunks)
            print(f"\nğŸ† æœ€å¤šå—æ•°ç­–ç•¥: {best_strategy[0]} ({best_strategy[1].total_chunks} ä¸ªå—)")
    
    async def demo_smart_chunking(self):
        """æ¼”ç¤ºæ™ºèƒ½åˆ‡åˆ†åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ§  æ™ºèƒ½åˆ‡åˆ†æ¼”ç¤º")
        print("="*60)
        
        test_cases = [
            ("technical_doc", "æŠ€æœ¯æ–‡æ¡£"),
            ("conversation", "å¯¹è¯è®°å½•"),
            ("news_article", "æ–°é—»æ–‡ç« "),
            ("academic_paper", "å­¦æœ¯è®ºæ–‡")
        ]
        
        for text_key, description in test_cases:
            print(f"\nğŸ“ å¤„ç† {description}...")
            content = self.sample_texts[text_key]
            
            try:
                # è·å–åˆ‡åˆ†å»ºè®®
                recommendations = get_chunking_recommendations(content[:500])  # ä½¿ç”¨å‰500å­—ç¬¦åˆ†æ
                print(f"  ğŸ” æ¨èå·¥å…·: {recommendations['recommended_tool']}")
                print(f"  ğŸ“ˆ å†…å®¹å¤æ‚åº¦: {recommendations['content_features']['complexity']}")
                
                # æ‰§è¡Œæ™ºèƒ½åˆ‡åˆ†
                result = await smart_chunk_text(content, content_type=description)
                
                print(f"  âœ… åˆ‡åˆ†å®Œæˆ: {result.total_chunks} ä¸ªå—")
                print(f"  ğŸ“ å¤§å°èŒƒå›´: {min(result.chunk_sizes)} - {max(result.chunk_sizes)} å­—ç¬¦")
                
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {str(e)}")
    
    async def demo_custom_configuration(self):
        """æ¼”ç¤ºè‡ªå®šä¹‰é…ç½®"""
        print("\n" + "="*60)
        print("âš™ï¸ è‡ªå®šä¹‰é…ç½®æ¼”ç¤º")
        print("="*60)
        
        # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
        custom_config = AgenticChunkingConfig(
            strategy=AgenticChunkingStrategy.SEMANTIC_BOUNDARY,
            max_chunk_size=1500,
            min_chunk_size=300,
            chunk_overlap=100,
            llm_model="gpt-4o-mini",
            language="zh",
            semantic_threshold=0.8,
            quality_threshold=0.85
        )
        
        print("ğŸ› ï¸ è‡ªå®šä¹‰é…ç½®å‚æ•°:")
        config_dict = custom_config.to_dict()
        for key, value in config_dict.items():
            print(f"  {key}: {value}")
        
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºåˆ‡åˆ†å™¨
        chunker = AgenticDocumentChunker(custom_config)
        
        sample_text = self.sample_texts["news_article"]
        
        try:
            result = await chunker.chunk_document(sample_text)
            
            print(f"\nâœ… è‡ªå®šä¹‰é…ç½®åˆ‡åˆ†å®Œæˆ!")
            print(f"ğŸ“Š ç”Ÿæˆ {result.total_chunks} ä¸ªå—")
            
            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            for i, chunk in enumerate(result.chunks):
                print(f"\nğŸ“‹ å— {i+1}:")
                print(f"  ğŸ“ å¤§å°: {len(chunk.content)} å­—ç¬¦")
                print(f"  ğŸ·ï¸ å…ƒæ•°æ®: {chunk.metadata}")
                
        except Exception as e:
            print(f"âŒ è‡ªå®šä¹‰é…ç½®åˆ‡åˆ†å¤±è´¥: {str(e)}")
    
    async def demo_batch_processing(self):
        """æ¼”ç¤ºæ‰¹é‡å¤„ç†"""
        print("\n" + "="*60)
        print("ğŸ”„ æ‰¹é‡å¤„ç†æ¼”ç¤º")
        print("="*60)
        
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        documents = []
        for text_key, content in self.sample_texts.items():
            documents.append({
                "content": content,
                "content_type": text_key,
                "metadata": {
                    "source": f"demo_{text_key}",
                    "timestamp": datetime.now().isoformat()
                }
            })
        
        print(f"ğŸ“¦ å‡†å¤‡å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£")
        
        try:
            manager = get_agentic_chunking_manager()
            results = await manager.batch_chunk_documents(
                documents, 
                auto_select=True,
                max_workers=3
            )
            
            print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
            
            # ç»Ÿè®¡ç»“æœ
            total_chunks = sum(result.total_chunks for result in results)
            successful_docs = sum(1 for result in results if result.total_chunks > 0)
            
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  æˆåŠŸæ–‡æ¡£: {successful_docs}/{len(documents)}")
            print(f"  æ€»å—æ•°: {total_chunks}")
            print(f"  å¹³å‡æ¯æ–‡æ¡£: {total_chunks/successful_docs:.1f} å—")
            
            # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„ç»“æœ
            for i, (doc, result) in enumerate(zip(documents, results)):
                content_type = doc["content_type"]
                print(f"\nğŸ“„ æ–‡æ¡£ {i+1} ({content_type}):")
                print(f"  å—æ•°: {result.total_chunks}")
                if result.chunk_sizes:
                    print(f"  å¹³å‡å—å¤§å°: {sum(result.chunk_sizes)/len(result.chunk_sizes):.0f} å­—ç¬¦")
                
                # æ˜¾ç¤ºä½¿ç”¨çš„ç­–ç•¥
                if "strategy" in result.processing_metadata:
                    print(f"  ä½¿ç”¨ç­–ç•¥: {result.processing_metadata['strategy']}")
            
            # æ˜¾ç¤ºç®¡ç†å™¨ç»Ÿè®¡
            stats = manager.get_stats()
            print(f"\nğŸ“ˆ ç®¡ç†å™¨ç»Ÿè®¡:")
            for key, value in stats.items():
                print(f"  {key}: {value}")
                
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
    
    async def demo_quality_analysis(self):
        """æ¼”ç¤ºè´¨é‡åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ“Š è´¨é‡åˆ†ææ¼”ç¤º")
        print("="*60)
        
        sample_text = self.sample_texts["academic_paper"]
        
        # ä½¿ç”¨é«˜è´¨é‡é…ç½®
        config = AgenticChunkingConfig(
            strategy=AgenticChunkingStrategy.SEMANTIC_BOUNDARY,
            max_chunk_size=1200,
            min_chunk_size=200,
            quality_threshold=0.8,
            semantic_threshold=0.75
        )
        
        chunker = AgenticDocumentChunker(config)
        
        try:
            result = await chunker.chunk_document(sample_text)
            
            print(f"âœ… è´¨é‡åˆ†æå®Œæˆ!")
            print(f"ğŸ“Š æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ€»å—æ•°: {result.total_chunks}")
            print(f"  å¹³å‡è´¨é‡: {result.processing_metadata.get('average_quality', 0):.3f}")
            
            # è¯¦ç»†è´¨é‡åˆ†æ
            if "quality_scores" in result.processing_metadata:
                quality_scores = result.processing_metadata["quality_scores"]
                
                print(f"\nğŸ¯ è´¨é‡ç»´åº¦åˆ†æ:")
                
                # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
                dimensions = ["semantic_coherence", "size_appropriateness", 
                            "boundary_naturalness", "structure_preservation"]
                
                for dim in dimensions:
                    avg_score = sum(score[dim] for score in quality_scores) / len(quality_scores)
                    print(f"  {dim}: {avg_score:.3f}")
                
                # æ˜¾ç¤ºæœ€ä½³å’Œæœ€å·®å—
                overall_scores = [score["overall_score"] for score in quality_scores]
                best_idx = overall_scores.index(max(overall_scores))
                worst_idx = overall_scores.index(min(overall_scores))
                
                print(f"\nğŸ† æœ€ä½³å— (å— {best_idx+1}, åˆ†æ•°: {overall_scores[best_idx]:.3f}):")
                best_chunk_preview = result.chunks[best_idx].content[:150] + "..."
                print(f"  {best_chunk_preview}")
                
                print(f"\nâš ï¸ æœ€å·®å— (å— {worst_idx+1}, åˆ†æ•°: {overall_scores[worst_idx]:.3f}):")
                worst_chunk_preview = result.chunks[worst_idx].content[:150] + "..."
                print(f"  {worst_chunk_preview}")
                
        except Exception as e:
            print(f"âŒ è´¨é‡åˆ†æå¤±è´¥: {str(e)}")
    
    async def run_all_demos(self):
        """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
        print("ğŸ‰ Agenticæ–‡æ¡£åˆ‡åˆ†å…¨åŠŸèƒ½æ¼”ç¤º")
        print("åŸºäºAgnoæ¡†æ¶çš„æ™ºèƒ½æ–‡æ¡£åˆ‡åˆ†ç³»ç»Ÿ")
        print("=" * 60)
        
        demos = [
            ("åŸºç¡€åˆ‡åˆ†åŠŸèƒ½", self.demo_basic_chunking),
            ("ç­–ç•¥å¯¹æ¯”", self.demo_strategy_comparison),
            ("æ™ºèƒ½åˆ‡åˆ†", self.demo_smart_chunking),
            ("è‡ªå®šä¹‰é…ç½®", self.demo_custom_configuration),
            ("æ‰¹é‡å¤„ç†", self.demo_batch_processing),
            ("è´¨é‡åˆ†æ", self.demo_quality_analysis)
        ]
        
        for demo_name, demo_func in demos:
            try:
                print(f"\nğŸš€ å¼€å§‹æ¼”ç¤º: {demo_name}")
                await demo_func()
                print(f"âœ… {demo_name} æ¼”ç¤ºå®Œæˆ")
            except Exception as e:
                print(f"âŒ {demo_name} æ¼”ç¤ºå¤±è´¥: {str(e)}")
            
            # ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç»§ç»­
            input("\næŒ‰ Enter é”®ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")
        
        print("\nğŸŠ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("æ„Ÿè°¢ä½¿ç”¨ Agenticæ–‡æ¡£åˆ‡åˆ†ç³»ç»Ÿ")

async def main():
    """ä¸»å‡½æ•°"""
    demo = AgenticChunkingDemo()
    
    print("è¯·é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("1. è¿è¡Œæ‰€æœ‰æ¼”ç¤º")
    print("2. åŸºç¡€åˆ‡åˆ†åŠŸèƒ½")
    print("3. ç­–ç•¥å¯¹æ¯”")
    print("4. æ™ºèƒ½åˆ‡åˆ†")
    print("5. è‡ªå®šä¹‰é…ç½®")
    print("6. æ‰¹é‡å¤„ç†")
    print("7. è´¨é‡åˆ†æ")
    
    try:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-7): ").strip()
        
        if choice == "1":
            await demo.run_all_demos()
        elif choice == "2":
            await demo.demo_basic_chunking()
        elif choice == "3":
            await demo.demo_strategy_comparison()
        elif choice == "4":
            await demo.demo_smart_chunking()
        elif choice == "5":
            await demo.demo_custom_configuration()
        elif choice == "6":
            await demo.demo_batch_processing()
        elif choice == "7":
            await demo.demo_quality_analysis()
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºå·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main()) 