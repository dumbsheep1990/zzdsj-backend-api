#!/usr/bin/env python3
"""
æ™ºèƒ½é¡µé¢è§£æå·¥å…·ç»¼åˆæ¼”ç¤º
å¯¹æ¯”Crawl4AIå’ŒBrowser Useä¸¤å¥—æ–¹æ¡ˆçš„åŠŸèƒ½å’Œé€‚ç”¨åœºæ™¯
"""

import asyncio
import json
import time
from typing import Dict, Any, List
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯¼å…¥ä¸¤å¥—MCPå®¢æˆ·ç«¯
from app.frameworks.fastmcp.integrations.providers.crawl4ai_llamaindex import Crawl4AILlamaIndexClient
from app.frameworks.fastmcp.integrations.providers.browser_use_llamaindex import BrowserUseLlamaIndexClient
from app.frameworks.fastmcp.integrations.registry import ExternalMCPService

class IntelligentPageAnalysisDemo:
    """æ™ºèƒ½é¡µé¢è§£ææ¼”ç¤ºç±»"""
    
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–æ¼”ç¤ºç±»
        
        å‚æ•°:
            api_key: APIå¯†é’¥
        """
        self.api_key = api_key
        self.crawl4ai_client = None
        self.browser_use_client = None
        self._setup_clients()
    
    def _setup_clients(self):
        """è®¾ç½®ä¸¤ä¸ªå®¢æˆ·ç«¯"""
        # Crawl4AIå®¢æˆ·ç«¯é…ç½®
        crawl4ai_service = ExternalMCPService(
            id="crawl4ai_intelligence",
            name="Crawl4AIæ™ºèƒ½è§£æå™¨",
            description="åŸºäºCrawl4AIçš„é«˜æ€§èƒ½æ™ºèƒ½é¡µé¢è§£æå·¥å…·",
            url="local://crawl4ai",
            provider="crawl4ai",
            capabilities=["tools", "resources"],
            extra_config={
                "model": {
                    "provider": "openai",
                    "name": "gpt-4o",
                    "temperature": 0.1
                }
            }
        )
        
        # Browser Useå®¢æˆ·ç«¯é…ç½®
        browser_use_service = ExternalMCPService(
            id="browser_use_intelligence",
            name="Browser Useæ™ºèƒ½æµè§ˆå™¨",
            description="åŸºäºBrowser Useçš„æ™ºèƒ½æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·",
            url="local://browser-use",
            provider="browser_use",
            capabilities=["tools", "resources", "chat"],
            extra_config={
                "model": {
                    "provider": "openai",
                    "name": "gpt-4o",
                    "temperature": 0.3
                }
            }
        )
        
        # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
        self.crawl4ai_client = Crawl4AILlamaIndexClient(crawl4ai_service, self.api_key)
        self.browser_use_client = BrowserUseLlamaIndexClient(browser_use_service, self.api_key)
    
    async def demo_performance_comparison(self):
        """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”"""
        print("\n" + "="*60)
        print("ğŸš€ æ€§èƒ½å¯¹æ¯”æ¼”ç¤ºï¼šCrawl4AI vs Browser Use")
        print("="*60)
        
        test_url = "https://news.ycombinator.com/"  # ç¤ºä¾‹æ–°é—»ç«™ç‚¹
        
        print(f"æµ‹è¯•URL: {test_url}")
        print("\næ­£åœ¨æµ‹è¯•Crawl4AIæ€§èƒ½...")
        
        # æµ‹è¯•Crawl4AI
        start_time = time.time()
        crawl4ai_result = await self.crawl4ai_client.call_tool(
            tool_name="advanced_page_intelligence",
            parameters={
                "url": test_url,
                "intelligence_level": "standard",
                "content_types": ["text", "links"],
                "analysis_depth": "medium"
            }
        )
        crawl4ai_time = time.time() - start_time
        
        print(f"âœ… Crawl4AIå®Œæˆæ—¶é—´: {crawl4ai_time:.2f}ç§’")
        
        print("\næ­£åœ¨æµ‹è¯•Browser Useæ€§èƒ½...")
        
        # æµ‹è¯•Browser Use
        start_time = time.time()
        browser_use_result = await self.browser_use_client.call_tool(
            tool_name="intelligent_page_analysis",
            parameters={
                "url": test_url,
                "analysis_goals": ["content", "structure"],
                "depth_level": "standard"
            }
        )
        browser_use_time = time.time() - start_time
        
        print(f"âœ… Browser Useå®Œæˆæ—¶é—´: {browser_use_time:.2f}ç§’")
        
        # æ€§èƒ½åˆ†æ
        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
        print(f"  Crawl4AI: {crawl4ai_time:.2f}ç§’")
        print(f"  Browser Use: {browser_use_time:.2f}ç§’")
        print(f"  é€Ÿåº¦å·®å¼‚: {abs(crawl4ai_time - browser_use_time):.2f}ç§’")
        
        if crawl4ai_time < browser_use_time:
            print(f"  ğŸ† Crawl4AI å¿« {(browser_use_time/crawl4ai_time - 1)*100:.1f}%")
        else:
            print(f"  ğŸ† Browser Use å¿« {(crawl4ai_time/browser_use_time - 1)*100:.1f}%")
        
        return {
            "crawl4ai": {"time": crawl4ai_time, "result": crawl4ai_result},
            "browser_use": {"time": browser_use_time, "result": browser_use_result}
        }
    
    async def demo_content_extraction_comparison(self):
        """æ¼”ç¤ºå†…å®¹æå–å¯¹æ¯”"""
        print("\n" + "="*60)
        print("ğŸ“„ å†…å®¹æå–å¯¹æ¯”æ¼”ç¤º")
        print("="*60)
        
        test_url = "https://www.wikipedia.org/wiki/Artificial_intelligence"
        extraction_rules = [
            "æå–æ–‡ç« çš„ä¸»è¦å®šä¹‰å’Œæ¦‚å¿µ",
            "è¯†åˆ«é‡è¦çš„å†å²æ—¶é—´ç‚¹å’Œäº‹ä»¶",
            "æå–ç›¸å…³çš„æŠ€æœ¯æœ¯è¯­å’Œä¸“ä¸šè¯æ±‡"
        ]
        
        print(f"æµ‹è¯•URL: {test_url}")
        print(f"æå–è§„åˆ™: {len(extraction_rules)}ä¸ª")
        
        # Crawl4AIç»“æ„åŒ–æŒ–æ˜
        print("\næ­£åœ¨ä½¿ç”¨Crawl4AIè¿›è¡Œç»“æ„åŒ–å†…å®¹æŒ–æ˜...")
        crawl4ai_result = await self.crawl4ai_client.call_tool(
            tool_name="structural_content_mining",
            parameters={
                "url": test_url,
                "target_structures": ["definitions", "history", "terminology"],
                "semantic_rules": extraction_rules
            }
        )
        
        # Browser Useæ™ºèƒ½æå–
        print("æ­£åœ¨ä½¿ç”¨Browser Useè¿›è¡Œæ™ºèƒ½å†…å®¹æå–...")
        browser_use_result = await self.browser_use_client.call_tool(
            tool_name="smart_content_extraction",
            parameters={
                "url": test_url,
                "extraction_rules": extraction_rules,
                "output_format": "structured"
            }
        )
        
        # ç»“æœå¯¹æ¯”
        print("\nğŸ“‹ å†…å®¹æå–ç»“æœå¯¹æ¯”:")
        
        if crawl4ai_result["status"] == "success":
            extracted_content = crawl4ai_result["data"].get("extracted_content", {})
            print(f"  Crawl4AIæå–é¡¹ç›®æ•°: {len(extracted_content) if isinstance(extracted_content, list) else 'N/A'}")
            print(f"  å†…å®¹é•¿åº¦: {crawl4ai_result['data'].get('raw_content_length', 0)} å­—ç¬¦")
        else:
            print(f"  Crawl4AIå¤±è´¥: {crawl4ai_result.get('error', 'Unknown error')}")
        
        if browser_use_result["status"] == "success":
            extracted_data = browser_use_result["data"].get("extracted_data", {})
            print(f"  Browser Useæå–è§„åˆ™æ•°: {len(extracted_data)}")
            total_confidence = sum(item.get("confidence", 0) for item in extracted_data.values())
            avg_confidence = total_confidence / len(extracted_data) if extracted_data else 0
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}")
        else:
            print(f"  Browser Useå¤±è´¥: {browser_use_result.get('error', 'Unknown error')}")
        
        return {
            "crawl4ai": crawl4ai_result,
            "browser_use": browser_use_result
        }
    
    async def demo_batch_processing_capability(self):
        """æ¼”ç¤ºæ‰¹é‡å¤„ç†èƒ½åŠ›"""
        print("\n" + "="*60)
        print("âš¡ æ‰¹é‡å¤„ç†èƒ½åŠ›æ¼”ç¤º")
        print("="*60)
        
        test_urls = [
            "https://example.com",
            "https://httpbin.org/html",
            "https://jsonplaceholder.typicode.com"
        ]
        
        print(f"æµ‹è¯•URLæ•°é‡: {len(test_urls)}")
        
        # Crawl4AIæ‰¹é‡å¤„ç†
        print("\næ­£åœ¨ä½¿ç”¨Crawl4AIè¿›è¡Œæ‰¹é‡æ™ºèƒ½çˆ¬å–...")
        start_time = time.time()
        crawl4ai_batch_result = await self.crawl4ai_client.call_tool(
            tool_name="batch_intelligent_crawling",
            parameters={
                "urls": test_urls,
                "crawl_strategy": "parallel",
                "max_concurrent": 2,
                "unified_analysis": True
            }
        )
        crawl4ai_batch_time = time.time() - start_time
        
        # Browser Useå¤šé¡µé¢åˆ†æ
        print("æ­£åœ¨ä½¿ç”¨Browser Useè¿›è¡Œå¤šé¡µé¢åˆ†æ...")
        start_time = time.time()
        browser_use_multi_result = await self.browser_use_client.call_tool(
            tool_name="multi_page_intelligence",
            parameters={
                "start_url": test_urls[0],  # ä»ç¬¬ä¸€ä¸ªURLå¼€å§‹
                "navigation_strategy": "auto",
                "max_pages": len(test_urls),
                "analysis_focus": "content_similarity"
            }
        )
        browser_use_multi_time = time.time() - start_time
        
        # æ‰¹é‡å¤„ç†ç»“æœåˆ†æ
        print(f"\nâš¡ æ‰¹é‡å¤„ç†ç»“æœ:")
        print(f"  Crawl4AIæ‰¹é‡å¤„ç†æ—¶é—´: {crawl4ai_batch_time:.2f}ç§’")
        print(f"  Browser Useå¤šé¡µé¢åˆ†ææ—¶é—´: {browser_use_multi_time:.2f}ç§’")
        
        if crawl4ai_batch_result["status"] == "success":
            data = crawl4ai_batch_result["data"]
            print(f"  Crawl4AIæˆåŠŸå¤„ç†: {data.get('successful_crawls', 0)}/{data.get('total_urls', 0)} ä¸ªURL")
            print(f"  ç»Ÿä¸€åˆ†æ: {'âœ…' if data.get('unified_analysis') else 'âŒ'}")
        
        if browser_use_multi_result["status"] == "success":
            data = browser_use_multi_result["data"]
            print(f"  Browser Useåˆ†æé¡µé¢æ•°: {data.get('pages_analyzed', 0)}")
            print(f"  è·¨é¡µé¢åˆ†æ: {'âœ…' if data.get('cross_page_analysis') else 'âŒ'}")
        
        return {
            "crawl4ai": {"time": crawl4ai_batch_time, "result": crawl4ai_batch_result},
            "browser_use": {"time": browser_use_multi_time, "result": browser_use_multi_result}
        }
    
    async def demo_dynamic_content_handling(self):
        """æ¼”ç¤ºåŠ¨æ€å†…å®¹å¤„ç†"""
        print("\n" + "="*60)
        print("ğŸ­ åŠ¨æ€å†…å®¹å¤„ç†æ¼”ç¤º")
        print("="*60)
        
        # é€‰æ‹©ä¸€ä¸ªæœ‰åŠ¨æ€å†…å®¹çš„æµ‹è¯•ç«™ç‚¹
        dynamic_url = "https://httpbin.org/delay/2"  # æœ‰å»¶è¿Ÿçš„ç«™ç‚¹æ¨¡æ‹ŸåŠ¨æ€åŠ è½½
        
        print(f"æµ‹è¯•åŠ¨æ€URL: {dynamic_url}")
        
        # Crawl4AIåŠ¨æ€å†…å®¹åˆ†æ
        print("\næ­£åœ¨ä½¿ç”¨Crawl4AIåˆ†æåŠ¨æ€å†…å®¹...")
        crawl4ai_dynamic_result = await self.crawl4ai_client.call_tool(
            tool_name="dynamic_content_analysis",
            parameters={
                "url": dynamic_url,
                "interaction_script": "await new Promise(resolve => setTimeout(resolve, 3000));",
                "analysis_triggers": ["scroll", "wait"]
            }
        )
        
        # Browser Useè¡¨å•äº¤äº’ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        print("æ­£åœ¨ä½¿ç”¨Browser Useè¿›è¡Œè‡ªé€‚åº”äº¤äº’...")
        browser_use_adaptive_result = await self.browser_use_client.call_tool(
            tool_name="adaptive_form_interaction",
            parameters={
                "url": dynamic_url,
                "form_intent": "æµ‹è¯•åŠ¨æ€å†…å®¹å“åº”",
                "auto_submit": False,
                "result_analysis": True
            }
        )
        
        # åŠ¨æ€å†…å®¹å¤„ç†ç»“æœ
        print(f"\nğŸ­ åŠ¨æ€å†…å®¹å¤„ç†ç»“æœ:")
        
        if crawl4ai_dynamic_result["status"] == "success":
            data = crawl4ai_dynamic_result["data"]
            print(f"  Crawl4AIåŠ¨æ€åˆ†æ: âœ…")
            print(f"  è§¦å‘å™¨: {data.get('interaction_triggers', [])}")
            extracted = data.get('extracted_dynamic_content', {})
            print(f"  åŠ¨æ€å†…å®¹ç±»å‹: {len(extracted) if isinstance(extracted, dict) else 'N/A'}")
        else:
            print(f"  Crawl4AIåŠ¨æ€åˆ†æ: âŒ {crawl4ai_dynamic_result.get('error', '')}")
        
        if browser_use_adaptive_result["status"] == "success":
            data = browser_use_adaptive_result["data"]
            print(f"  Browser Useè‡ªé€‚åº”äº¤äº’: âœ…")
            print(f"  è¡¨å•æ„å›¾: {data.get('form_intent', 'N/A')}")
            analysis = data.get('analysis_results', {})
            print(f"  ç»“æœåˆ†æ: {'âœ…' if analysis else 'âŒ'}")
        else:
            print(f"  Browser Useè‡ªé€‚åº”äº¤äº’: âŒ {browser_use_adaptive_result.get('error', '')}")
        
        return {
            "crawl4ai": crawl4ai_dynamic_result,
            "browser_use": browser_use_adaptive_result
        }
    
    async def demo_semantic_search_capability(self):
        """æ¼”ç¤ºè¯­ä¹‰æœç´¢èƒ½åŠ›"""
        print("\n" + "="*60)
        print("ğŸ” è¯­ä¹‰æœç´¢èƒ½åŠ›æ¼”ç¤º")
        print("="*60)
        
        test_url = "https://en.wikipedia.org/wiki/Machine_learning"
        search_queries = [
            "æœºå™¨å­¦ä¹ çš„å®šä¹‰å’Œæ ¸å¿ƒæ¦‚å¿µ",
            "ä¸»è¦çš„æœºå™¨å­¦ä¹ ç®—æ³•ç±»å‹",
            "æœºå™¨å­¦ä¹ çš„å®é™…åº”ç”¨åœºæ™¯"
        ]
        
        print(f"æµ‹è¯•URL: {test_url}")
        print(f"æœç´¢æŸ¥è¯¢: {len(search_queries)}ä¸ª")
        
        # Crawl4AIè¯­ä¹‰æœç´¢
        print("\næ­£åœ¨ä½¿ç”¨Crawl4AIè¿›è¡Œè¯­ä¹‰æœç´¢æå–...")
        crawl4ai_semantic_result = await self.crawl4ai_client.call_tool(
            tool_name="semantic_search_extraction",
            parameters={
                "url": test_url,
                "search_queries": search_queries,
                "extraction_goals": ["å®šä¹‰æå–", "åˆ†ç±»æ€»ç»“", "åº”ç”¨æ¡ˆä¾‹"],
                "similarity_threshold": 0.75
            }
        )
        
        # Browser Useæ™ºèƒ½é¡µé¢åˆ†æï¼ˆæ·±åº¦æ¨¡å¼ï¼‰
        print("æ­£åœ¨ä½¿ç”¨Browser Useè¿›è¡Œæ·±åº¦æ™ºèƒ½åˆ†æ...")
        browser_use_intelligent_result = await self.browser_use_client.call_tool(
            tool_name="intelligent_page_analysis",
            parameters={
                "url": test_url,
                "analysis_goals": ["content", "structure", "metadata"],
                "depth_level": "deep"
            }
        )
        
        # è¯­ä¹‰æœç´¢ç»“æœå¯¹æ¯”
        print(f"\nğŸ” è¯­ä¹‰æœç´¢ç»“æœ:")
        
        if crawl4ai_semantic_result["status"] == "success":
            data = crawl4ai_semantic_result["data"]
            search_results = data.get("search_results", {})
            extraction_results = data.get("extraction_results", {})
            print(f"  Crawl4AIè¯­ä¹‰æœç´¢: âœ…")
            print(f"  æœç´¢ç»“æœæ•°: {len(search_results)}")
            print(f"  æå–ç›®æ ‡æ•°: {len(extraction_results)}")
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            confidences = [result.get("confidence", 0) for result in search_results.values()]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0
            print(f"  å¹³å‡æœç´¢ç½®ä¿¡åº¦: {avg_confidence:.2f}")
        else:
            print(f"  Crawl4AIè¯­ä¹‰æœç´¢: âŒ {crawl4ai_semantic_result.get('error', '')}")
        
        if browser_use_intelligent_result["status"] == "success":
            data = browser_use_intelligent_result["data"]
            analysis_results = data.get("analysis_results", {})
            print(f"  Browser Useæ·±åº¦åˆ†æ: âœ…")
            print(f"  åˆ†æç»´åº¦æ•°: {len(analysis_results)}")
            
            if "semantic_analysis" in analysis_results:
                print(f"  è¯­ä¹‰åˆ†æ: âœ…")
            if "entity_extraction" in analysis_results:
                print(f"  å®ä½“æå–: âœ…")
        else:
            print(f"  Browser Useæ·±åº¦åˆ†æ: âŒ {browser_use_intelligent_result.get('error', '')}")
        
        return {
            "crawl4ai": crawl4ai_semantic_result,
            "browser_use": browser_use_intelligent_result
        }
    
    async def demo_tool_capabilities_overview(self):
        """æ¼”ç¤ºå·¥å…·èƒ½åŠ›æ¦‚è§ˆ"""
        print("\n" + "="*60)
        print("ğŸ› ï¸  å·¥å…·èƒ½åŠ›æ¦‚è§ˆ")
        print("="*60)
        
        # è·å–Crawl4AIå·¥å…·åˆ—è¡¨
        crawl4ai_tools = await self.crawl4ai_client.get_tools()
        
        # è·å–Browser Useå·¥å…·åˆ—è¡¨
        browser_use_tools = await self.browser_use_client.get_tools()
        
        print("ğŸ”§ Crawl4AIå·¥å…·é›†:")
        for i, tool in enumerate(crawl4ai_tools, 1):
            print(f"  {i}. {tool['name']}")
            print(f"     æè¿°: {tool['description']}")
            required_params = tool['parameters'].get('required', [])
            print(f"     å¿…éœ€å‚æ•°: {', '.join(required_params)}")
            print()
        
        print("ğŸ¯ Browser Useå·¥å…·é›†:")
        for i, tool in enumerate(browser_use_tools, 1):
            print(f"  {i}. {tool['name']}")
            print(f"     æè¿°: {tool['description']}")
            required_params = tool['parameters'].get('required', [])
            print(f"     å¿…éœ€å‚æ•°: {', '.join(required_params)}")
            print()
        
        # èƒ½åŠ›å¯¹æ¯”
        print("ğŸ“Š èƒ½åŠ›å¯¹æ¯”æ€»ç»“:")
        print(f"  Crawl4AIå·¥å…·æ•°é‡: {len(crawl4ai_tools)}")
        print(f"  Browser Useå·¥å…·æ•°é‡: {len(browser_use_tools)}")
        
        crawl4ai_features = set()
        browser_use_features = set()
        
        for tool in crawl4ai_tools:
            if "batch" in tool['name']:
                crawl4ai_features.add("æ‰¹é‡å¤„ç†")
            if "dynamic" in tool['name']:
                crawl4ai_features.add("åŠ¨æ€å†…å®¹")
            if "semantic" in tool['name']:
                crawl4ai_features.add("è¯­ä¹‰æœç´¢")
            if "structural" in tool['name']:
                crawl4ai_features.add("ç»“æ„åŒ–æŒ–æ˜")
        
        for tool in browser_use_tools:
            if "multi" in tool['name']:
                browser_use_features.add("å¤šé¡µé¢åˆ†æ")
            if "adaptive" in tool['name']:
                browser_use_features.add("è‡ªé€‚åº”äº¤äº’")
            if "intelligent" in tool['name']:
                browser_use_features.add("æ™ºèƒ½åˆ†æ")
        
        print(f"\n  Crawl4AIç‰¹è‰²åŠŸèƒ½: {', '.join(crawl4ai_features)}")
        print(f"  Browser Useç‰¹è‰²åŠŸèƒ½: {', '.join(browser_use_features)}")
        
        return {
            "crawl4ai_tools": crawl4ai_tools,
            "browser_use_tools": browser_use_tools,
            "feature_comparison": {
                "crawl4ai": list(crawl4ai_features),
                "browser_use": list(browser_use_features)
            }
        }
    
    def print_recommendations(self):
        """æ‰“å°ä½¿ç”¨å»ºè®®"""
        print("\n" + "="*60)
        print("ğŸ’¡ ä½¿ç”¨å»ºè®®å’Œæœ€ä½³å®è·µ")
        print("="*60)
        
        print("ğŸš€ Crawl4AIé€‚ç”¨åœºæ™¯:")
        print("  âœ… å¤§è§„æ¨¡æ‰¹é‡ç½‘é¡µçˆ¬å–")
        print("  âœ… é«˜æ€§èƒ½è¦æ±‚çš„åœºæ™¯")
        print("  âœ… ç»“æ„åŒ–æ•°æ®æå–")
        print("  âœ… é™æ€å†…å®¹ä¸ºä¸»çš„ç½‘ç«™")
        print("  âœ… APIåŒ–çš„è‡ªåŠ¨åŒ–æµç¨‹")
        
        print("\nğŸ¯ Browser Useé€‚ç”¨åœºæ™¯:")
        print("  âœ… å¤æ‚äº¤äº’å¼ç½‘ç«™")
        print("  âœ… éœ€è¦æ¨¡æ‹Ÿäººç±»è¡Œä¸º")
        print("  âœ… è¡¨å•è‡ªåŠ¨åŒ–å¡«å†™")
        print("  âœ… å•é¡µåº”ç”¨(SPA)å¤„ç†")
        print("  âœ… éœ€è¦æ·±åº¦é¡µé¢åˆ†æ")
        
        print("\nâš–ï¸  é€‰æ‹©å»ºè®®:")
        print("  ğŸ“Š æ•°æ®é‡å¤§ã€é€Ÿåº¦ä¼˜å…ˆ â†’ é€‰æ‹©Crawl4AI")
        print("  ğŸ­ äº¤äº’å¤æ‚ã€ç²¾åº¦ä¼˜å…ˆ â†’ é€‰æ‹©Browser Use")
        print("  ğŸ”„ æ··åˆåœºæ™¯ â†’ ä¸¤è€…ç»“åˆä½¿ç”¨")
        
        print("\nğŸ›¡ï¸  æ³¨æ„äº‹é¡¹:")
        print("  âš ï¸  éµå®ˆç½‘ç«™robots.txtè§„åˆ™")
        print("  âš ï¸  æ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œé¿å…è¿‡è½½")
        print("  âš ï¸  å¤„ç†åçˆ¬è™«æœºåˆ¶")
        print("  âš ï¸  æ³¨æ„æ•°æ®éšç§å’Œåˆè§„æ€§")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½é¡µé¢è§£æå·¥å…·ç»¼åˆæ¼”ç¤º")
    print("Crawl4AI vs Browser Use å…¨é¢å¯¹æ¯”")
    print("="*60)
    
    # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…APIå¯†é’¥
    API_KEY = "your-openai-api-key-here"
    
    if API_KEY == "your-openai-api-key-here":
        print("âš ï¸  è¯·å…ˆè®¾ç½®ä½ çš„APIå¯†é’¥ï¼")
        print("ä¿®æ”¹ä»£ç ä¸­çš„ API_KEY å˜é‡ä¸ºä½ çš„å®é™…OpenAI APIå¯†é’¥")
        return
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = IntelligentPageAnalysisDemo(API_KEY)
    
    try:
        # å·¥å…·èƒ½åŠ›æ¦‚è§ˆ
        await demo.demo_tool_capabilities_overview()
        
        # æ€§èƒ½å¯¹æ¯”
        await demo.demo_performance_comparison()
        
        # å†…å®¹æå–å¯¹æ¯”
        await demo.demo_content_extraction_comparison()
        
        # æ‰¹é‡å¤„ç†èƒ½åŠ›
        await demo.demo_batch_processing_capability()
        
        # åŠ¨æ€å†…å®¹å¤„ç†
        await demo.demo_dynamic_content_handling()
        
        # è¯­ä¹‰æœç´¢èƒ½åŠ›
        await demo.demo_semantic_search_capability()
        
        # ä½¿ç”¨å»ºè®®
        demo.print_recommendations()
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("ä¸¤å¥—å·¥å…·å„æœ‰ä¼˜åŠ¿ï¼Œå¯æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©ä½¿ç”¨ã€‚")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºç¨‹åºå·²åœæ­¢")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºç¨‹åºå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        if demo.crawl4ai_client:
            await demo.crawl4ai_client.close()
        if demo.browser_use_client:
            await demo.browser_use_client.close()

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())