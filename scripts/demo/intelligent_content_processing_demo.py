#!/usr/bin/env python3
"""
æ™ºèƒ½å†…å®¹å¤„ç†æ¼”ç¤ºè„šæœ¬
å±•ç¤ºmarkitdownæ¡†æ¶é›†æˆã€æ™ºèƒ½çˆ¬è™«å’Œå†…å®¹åˆ†æåŠŸèƒ½
"""

import sys
import os
import asyncio
import json
import time
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from app.tools.advanced.content.markitdown_adapter import get_markitdown_adapter
from app.tools.advanced.search.enhanced_web_crawler import get_enhanced_web_crawler
from app.tools.advanced.search.intelligent_crawler_scheduler import IntelligentCrawlerScheduler


class ContentProcessingDemo:
    """æ™ºèƒ½å†…å®¹å¤„ç†æ¼”ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤º"""
        self.markitdown_adapter = None
        self.web_crawler = None
        self.crawler_scheduler = None
        
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        print("ğŸš€ åˆå§‹åŒ–æ™ºèƒ½å†…å®¹å¤„ç†ç»„ä»¶...")
        
        try:
            # åˆå§‹åŒ–MarkItDowné€‚é…å™¨
            self.markitdown_adapter = get_markitdown_adapter()
            await self.markitdown_adapter.initialize()
            print("âœ… MarkItDowné€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–å¢å¼ºç½‘é¡µçˆ¬è™«
            self.web_crawler = get_enhanced_web_crawler()
            await self.web_crawler.initialize()
            print("âœ… å¢å¼ºç½‘é¡µçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–æ™ºèƒ½çˆ¬è™«è°ƒåº¦å™¨
            self.crawler_scheduler = IntelligentCrawlerScheduler()
            await self.crawler_scheduler.initialize()
            print("âœ… æ™ºèƒ½çˆ¬è™«è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
            
            print("ğŸ‰ æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–æˆåŠŸï¼\n")
            
        except Exception as e:
            print(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def demo_markitdown_conversion(self):
        """æ¼”ç¤ºMarkItDownå†…å®¹è½¬æ¢"""
        print("=" * 60)
        print("ğŸ“„ MarkItDownå†…å®¹è½¬æ¢æ¼”ç¤º")
        print("=" * 60)
        
        # æµ‹è¯•HTMLè½¬æ¢
        html_content = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>æµ‹è¯•é¡µé¢</title>
            <meta name="description" content="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é¡µé¢">
        </head>
        <body>
            <h1>ä¸»æ ‡é¢˜</h1>
            <p>è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ï¼ŒåŒ…å«äº†ä¸€äº›<strong>é‡è¦</strong>çš„ä¿¡æ¯ã€‚</p>
            
            <h2>å­æ ‡é¢˜</h2>
            <ul>
                <li>åˆ—è¡¨é¡¹ç›®1</li>
                <li>åˆ—è¡¨é¡¹ç›®2</li>
                <li>åˆ—è¡¨é¡¹ç›®3</li>
            </ul>
            
            <h3>æ•°æ®è¡¨æ ¼</h3>
            <table>
                <tr>
                    <th>å§“å</th>
                    <th>å¹´é¾„</th>
                    <th>èŒä¸š</th>
                </tr>
                <tr>
                    <td>å¼ ä¸‰</td>
                    <td>25</td>
                    <td>å·¥ç¨‹å¸ˆ</td>
                </tr>
                <tr>
                    <td>æå››</td>
                    <td>30</td>
                    <td>è®¾è®¡å¸ˆ</td>
                </tr>
            </table>
            
            <p>è¿™æ˜¯æœ€åä¸€æ®µå†…å®¹ï¼ŒåŒ…å«ä¸€ä¸ª<a href="https://example.com">å¤–éƒ¨é“¾æ¥</a>ã€‚</p>
        </body>
        </html>
        """
        
        print("ğŸ”„ è½¬æ¢HTMLå†…å®¹ä¸ºMarkdown...")
        
        try:
            start_time = time.time()
            result = self.markitdown_adapter.convert_to_markdown(
                html_content, 
                "html", 
                "https://example.com/test"
            )
            end_time = time.time()
            
            print(f"â±ï¸  è½¬æ¢è€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"âœ… è½¬æ¢æˆåŠŸ: {result['conversion_success']}")
            print(f"ğŸ“ æå–æ ‡é¢˜: {result.get('title', 'N/A')}")
            print(f"ğŸ“Š å…ƒæ•°æ®: {len(result.get('metadata', {}))} é¡¹")
            
            print("\nğŸ“„ è½¬æ¢åçš„Markdownå†…å®¹:")
            print("-" * 40)
            print(result.get('markdown', 'è½¬æ¢å¤±è´¥')[:500] + "...")
            print("-" * 40)
            
            if result.get('metadata'):
                print("\nğŸ“Š æå–çš„å…ƒæ•°æ®:")
                for key, value in result['metadata'].items():
                    print(f"  â€¢ {key}: {value}")
            
        except Exception as e:
            print(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}")
        
        print("\n")
    
    async def demo_enhanced_web_crawling(self):
        """æ¼”ç¤ºå¢å¼ºç½‘é¡µçˆ¬è™«"""
        print("=" * 60)
        print("ğŸ•·ï¸  å¢å¼ºç½‘é¡µçˆ¬è™«æ¼”ç¤º")
        print("=" * 60)
        
        # æµ‹è¯•URLåˆ—è¡¨
        test_urls = [
            "https://httpbin.org/html",
            "https://httpbin.org/json",
            "https://example.com"
        ]
        
        print(f"ğŸ¯ å‡†å¤‡çˆ¬å– {len(test_urls)} ä¸ªæµ‹è¯•URL...")
        
        for i, url in enumerate(test_urls, 1):
            print(f"\nğŸ“¡ [{i}/{len(test_urls)}] çˆ¬å–: {url}")
            
            try:
                start_time = time.time()
                result = await self.web_crawler.crawl_url(url)
                end_time = time.time()
                
                print(f"â±ï¸  çˆ¬å–è€—æ—¶: {end_time - start_time:.2f}ç§’")
                print(f"ğŸ“Š çŠ¶æ€: {result.get('status', 'unknown')}")
                
                if result.get('status') == 'success':
                    quality = result.get('quality_analysis', {})
                    print(f"ğŸ¯ è´¨é‡è¯„åˆ†: {quality.get('overall_score', 0):.2f}")
                    print(f"ğŸ“ è´¨é‡ç­‰çº§: {quality.get('quality_level', 'unknown')}")
                    
                    extracted = result.get('extracted_data', {})
                    if extracted:
                        print(f"ğŸ“„ æå–æ ‡é¢˜: {extracted.get('title', 'N/A')}")
                        print(f"ğŸ”— é“¾æ¥æ•°é‡: {extracted.get('links', {}).get('total_links', 0)}")
                        
                        if extracted.get('structured_data'):
                            print(f"ğŸ“Š ç»“æ„åŒ–æ•°æ®: {len(extracted['structured_data'])} é¡¹")
                
                elif result.get('status') == 'failed':
                    print(f"âŒ çˆ¬å–å¤±è´¥: {result.get('error', 'unknown error')}")
                
                elif result.get('status') == 'low_quality':
                    quality = result.get('quality_analysis', {})
                    print(f"âš ï¸  è´¨é‡ä¸è¾¾æ ‡: {quality.get('overall_score', 0):.2f}")
                    recommendations = quality.get('recommendations', [])
                    if recommendations:
                        print("ğŸ’¡ æ”¹è¿›å»ºè®®:")
                        for rec in recommendations:
                            print(f"   â€¢ {rec}")
                
            except Exception as e:
                print(f"âŒ çˆ¬å–å¼‚å¸¸: {str(e)}")
        
        print("\n")
    
    async def demo_intelligent_crawler_scheduler(self):
        """æ¼”ç¤ºæ™ºèƒ½çˆ¬è™«è°ƒåº¦å™¨"""
        print("=" * 60)
        print("ğŸ§  æ™ºèƒ½çˆ¬è™«è°ƒåº¦å™¨æ¼”ç¤º")
        print("=" * 60)
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„é¡µé¢
        test_cases = [
            {
                "url": "https://httpbin.org/html",
                "description": "ç®€å•HTMLé¡µé¢"
            },
            {
                "url": "https://httpbin.org/json",
                "description": "JSON APIå“åº”"
            },
            {
                "url": "https://example.com",
                "description": "ç¤ºä¾‹ç½‘ç«™"
            }
        ]
        
        print(f"ğŸ¯ å‡†å¤‡ä½¿ç”¨æ™ºèƒ½è°ƒåº¦å™¨åˆ†æ {len(test_cases)} ä¸ªé¡µé¢...")
        
        for i, test_case in enumerate(test_cases, 1):
            url = test_case["url"]
            description = test_case["description"]
            
            print(f"\nğŸ” [{i}/{len(test_cases)}] åˆ†æ: {description}")
            print(f"ğŸŒ URL: {url}")
            
            try:
                start_time = time.time()
                
                # ä½¿ç”¨æ™ºèƒ½è°ƒåº¦å™¨è¿›è¡Œåˆ†æ
                result = await self.crawler_scheduler.intelligent_crawl(url)
                
                end_time = time.time()
                
                print(f"â±ï¸  åˆ†æè€—æ—¶: {end_time - start_time:.2f}ç§’")
                print(f"ğŸ“Š çŠ¶æ€: {result.get('status', 'unknown')}")
                
                if result.get('crawler_used'):
                    print(f"ğŸ¤– ä½¿ç”¨çš„çˆ¬è™«: {result['crawler_used']}")
                
                if result.get('content_analysis'):
                    analysis = result['content_analysis']
                    print(f"ğŸ“ å†…å®¹é•¿åº¦: {analysis.get('content_length', 0)} å­—ç¬¦")
                    print(f"ğŸ¯ å¤æ‚åº¦è¯„åˆ†: {analysis.get('complexity_score', 0):.2f}")
                    
                    if analysis.get('markdown_content'):
                        markdown_preview = analysis['markdown_content'][:200] + "..."
                        print(f"ğŸ“„ Markdowné¢„è§ˆ: {markdown_preview}")
                
                if result.get('quality_assessment'):
                    quality = result['quality_assessment']
                    print(f"âœ¨ æ•´ä½“è´¨é‡: {quality.get('overall_score', 0):.2f}")
                    
                    recommendations = quality.get('recommendations', [])
                    if recommendations:
                        print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                        for rec in recommendations[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                            print(f"   â€¢ {rec}")
                
            except Exception as e:
                print(f"âŒ åˆ†æå¼‚å¸¸: {str(e)}")
        
        print("\n")
    
    async def demo_batch_processing(self):
        """æ¼”ç¤ºæ‰¹é‡å¤„ç†åŠŸèƒ½"""
        print("=" * 60)
        print("ğŸ“¦ æ‰¹é‡å†…å®¹å¤„ç†æ¼”ç¤º")
        print("=" * 60)
        
        # æ‰¹é‡æµ‹è¯•URL
        batch_urls = [
            "https://httpbin.org/html",
            "https://httpbin.org/xml",
            "https://httpbin.org/json",
            "https://example.com"
        ]
        
        print(f"ğŸ¯ æ‰¹é‡å¤„ç† {len(batch_urls)} ä¸ªURL...")
        
        try:
            start_time = time.time()
            
            # ä½¿ç”¨å¢å¼ºçˆ¬è™«è¿›è¡Œæ‰¹é‡å¤„ç†
            results = await self.web_crawler.crawl_urls_batch(batch_urls)
            
            end_time = time.time()
            
            print(f"â±ï¸  æ‰¹é‡å¤„ç†è€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
            
            # ç»Ÿè®¡ç»“æœ
            status_counts = {}
            quality_scores = []
            
            for result in results:
                status = result.get('status', 'unknown')
                status_counts[status] = status_counts.get(status, 0) + 1
                
                if result.get('quality_analysis', {}).get('overall_score'):
                    quality_scores.append(result['quality_analysis']['overall_score'])
            
            print("\nğŸ“ˆ çŠ¶æ€åˆ†å¸ƒ:")
            for status, count in status_counts.items():
                print(f"   â€¢ {status}: {count} ä¸ª")
            
            if quality_scores:
                avg_quality = sum(quality_scores) / len(quality_scores)
                print(f"\nğŸ¯ å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.2f}")
                print(f"ğŸ† æœ€é«˜è´¨é‡è¯„åˆ†: {max(quality_scores):.2f}")
                print(f"â¬‡ï¸  æœ€ä½è´¨é‡è¯„åˆ†: {min(quality_scores):.2f}")
            
            # æ˜¾ç¤ºæˆåŠŸå¤„ç†çš„è¯¦ç»†ä¿¡æ¯
            successful_results = [r for r in results if r.get('status') == 'success']
            if successful_results:
                print(f"\nâœ… æˆåŠŸå¤„ç†çš„é¡µé¢è¯¦æƒ…:")
                for i, result in enumerate(successful_results, 1):
                    extracted = result.get('extracted_data', {})
                    print(f"   [{i}] {result.get('url', 'N/A')}")
                    print(f"       æ ‡é¢˜: {extracted.get('title', 'N/A')}")
                    print(f"       è´¨é‡: {result.get('quality_analysis', {}).get('overall_score', 0):.2f}")
        
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
        
        print("\n")
    
    async def demo_content_comparison(self):
        """æ¼”ç¤ºå†…å®¹å¤„ç†å¯¹æ¯”"""
        print("=" * 60)
        print("âš–ï¸  å†…å®¹å¤„ç†æ–¹æ³•å¯¹æ¯”æ¼”ç¤º")
        print("=" * 60)
        
        test_url = "https://httpbin.org/html"
        
        print(f"ğŸ¯ ä½¿ç”¨ä¸åŒæ–¹æ³•å¤„ç†åŒä¸€URL: {test_url}")
        
        # æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨MarkItDown
        print("\nğŸ“„ æ–¹æ³•1: ç›´æ¥MarkItDownè½¬æ¢")
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.get(test_url) as response:
                    html_content = await response.text()
            
            start_time = time.time()
            markitdown_result = self.markitdown_adapter.convert_to_markdown(html_content, "html", test_url)
            end_time = time.time()
            
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
            print(f"âœ… è½¬æ¢æˆåŠŸ: {markitdown_result['conversion_success']}")
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(markitdown_result.get('markdown', ''))}")
            
        except Exception as e:
            print(f"âŒ MarkItDownè½¬æ¢å¤±è´¥: {str(e)}")
        
        # æ–¹æ³•2ï¼šå¢å¼ºç½‘é¡µçˆ¬è™«
        print("\nğŸ•·ï¸  æ–¹æ³•2: å¢å¼ºç½‘é¡µçˆ¬è™«")
        try:
            start_time = time.time()
            crawler_result = await self.web_crawler.crawl_url(test_url)
            end_time = time.time()
            
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ“Š å¤„ç†çŠ¶æ€: {crawler_result.get('status')}")
            print(f"ğŸ¯ è´¨é‡è¯„åˆ†: {crawler_result.get('quality_analysis', {}).get('overall_score', 0):.2f}")
            
            extracted = crawler_result.get('extracted_data', {})
            if extracted:
                print(f"ğŸ“„ æå–ä¿¡æ¯: {len(extracted)} é¡¹")
        
        except Exception as e:
            print(f"âŒ å¢å¼ºçˆ¬è™«å¤„ç†å¤±è´¥: {str(e)}")
        
        # æ–¹æ³•3ï¼šæ™ºèƒ½è°ƒåº¦å™¨
        print("\nğŸ§  æ–¹æ³•3: æ™ºèƒ½è°ƒåº¦å™¨")
        try:
            start_time = time.time()
            scheduler_result = await self.crawler_scheduler.intelligent_crawl(test_url)
            end_time = time.time()
            
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ“Š å¤„ç†çŠ¶æ€: {scheduler_result.get('status')}")
            print(f"ğŸ¤– é€‰æ‹©çš„æ–¹æ³•: {scheduler_result.get('crawler_used', 'N/A')}")
            
            if scheduler_result.get('quality_assessment'):
                quality = scheduler_result['quality_assessment']
                print(f"âœ¨ æ•´ä½“è´¨é‡: {quality.get('overall_score', 0):.2f}")
        
        except Exception as e:
            print(f"âŒ æ™ºèƒ½è°ƒåº¦å™¨å¤„ç†å¤±è´¥: {str(e)}")
        
        print("\n")
    
    def print_performance_summary(self):
        """æ‰“å°æ€§èƒ½æ€»ç»“"""
        print("=" * 60)
        print("ğŸ“Š æ¼”ç¤ºæ€»ç»“")
        print("=" * 60)
        
        print("ğŸ‰ æ™ºèƒ½å†…å®¹å¤„ç†æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ä¸»è¦åŠŸèƒ½äº®ç‚¹:")
        print("   â€¢ âœ… MarkItDownæ¡†æ¶é›†æˆ - æ”¯æŒå¤šç§æ ¼å¼è½¬æ¢")
        print("   â€¢ âœ… å¢å¼ºç½‘é¡µçˆ¬è™« - æ™ºèƒ½å†…å®¹è´¨é‡åˆ†æ")
        print("   â€¢ âœ… æ™ºèƒ½è°ƒåº¦å™¨ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¤„ç†æ–¹æ¡ˆ")
        print("   â€¢ âœ… æ‰¹é‡å¤„ç† - é«˜æ•ˆå¹¶å‘å¤„ç†å¤šä¸ªURL")
        print("   â€¢ âœ… è´¨é‡è¯„ä¼° - å…¨æ–¹ä½å†…å®¹è´¨é‡åˆ†æ")
        
        print("\nğŸ”§ æŠ€æœ¯ç‰¹æ€§:")
        print("   â€¢ å¼‚æ­¥å¤„ç†æ¶æ„")
        print("   â€¢ æ™ºèƒ½é”™è¯¯æ¢å¤")
        print("   â€¢ å¯é…ç½®è´¨é‡é˜ˆå€¼")
        print("   â€¢ ç»“æ„åŒ–æ•°æ®æå–")
        print("   â€¢ å¤šæ ¼å¼å†…å®¹æ”¯æŒ")
        
        print("\nğŸš€ é€‚ç”¨åœºæ™¯:")
        print("   â€¢ ç½‘é¡µå†…å®¹é‡‡é›†ä¸åˆ†æ")
        print("   â€¢ æ–‡æ¡£æ ¼å¼è½¬æ¢ä¸æ¸…æ´—")
        print("   â€¢ çŸ¥è¯†åº“å†…å®¹é¢„å¤„ç†")
        print("   â€¢ æœç´¢ç»“æœè´¨é‡è¯„ä¼°")
        print("   â€¢ æ‰¹é‡å†…å®¹å¤„ç†ä»»åŠ¡")
        
        print("\nğŸ“ å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒé¡¹ç›®æ–‡æ¡£æˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚")
    
    async def run_full_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        try:
            await self.initialize()
            
            # è¿è¡Œå„ä¸ªæ¼”ç¤ºæ¨¡å—
            await self.demo_markitdown_conversion()
            await self.demo_enhanced_web_crawling()
            await self.demo_intelligent_crawler_scheduler()
            await self.demo_batch_processing()
            await self.demo_content_comparison()
            
            # æ‰“å°æ€»ç»“
            self.print_performance_summary()
            
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {str(e)}")
            raise
        finally:
            # æ¸…ç†èµ„æº
            if self.web_crawler:
                await self.web_crawler.close()
    
    async def run_interactive_demo(self):
        """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
        await self.initialize()
        
        while True:
            print("\n" + "=" * 60)
            print("ğŸ® æ™ºèƒ½å†…å®¹å¤„ç†äº¤äº’å¼æ¼”ç¤º")
            print("=" * 60)
            print("è¯·é€‰æ‹©æ¼”ç¤ºé¡¹ç›®:")
            print("1. MarkItDownå†…å®¹è½¬æ¢")
            print("2. å¢å¼ºç½‘é¡µçˆ¬è™«")
            print("3. æ™ºèƒ½çˆ¬è™«è°ƒåº¦å™¨")
            print("4. æ‰¹é‡å¤„ç†")
            print("5. æ–¹æ³•å¯¹æ¯”")
            print("6. è¿è¡Œå®Œæ•´æ¼”ç¤º")
            print("0. é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-6): ").strip()
            
            try:
                if choice == "1":
                    await self.demo_markitdown_conversion()
                elif choice == "2":
                    await self.demo_enhanced_web_crawling()
                elif choice == "3":
                    await self.demo_intelligent_crawler_scheduler()
                elif choice == "4":
                    await self.demo_batch_processing()
                elif choice == "5":
                    await self.demo_content_comparison()
                elif choice == "6":
                    await self.run_full_demo()
                    break
                elif choice == "0":
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æ™ºèƒ½å†…å®¹å¤„ç†æ¼”ç¤ºï¼")
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")
                    
            except Exception as e:
                print(f"âŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {str(e)}")
                print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚")
        
        # æ¸…ç†èµ„æº
        if self.web_crawler:
            await self.web_crawler.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½å†…å®¹å¤„ç†æ¼”ç¤ºç³»ç»Ÿï¼")
    print("æœ¬ç³»ç»Ÿé›†æˆäº†markitdownæ¡†æ¶ã€å¢å¼ºç½‘é¡µçˆ¬è™«å’Œæ™ºèƒ½è°ƒåº¦å™¨ã€‚")
    
    demo = ContentProcessingDemo()
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
        if mode == "full":
            print("\nğŸš€ è¿è¡Œå®Œæ•´æ¼”ç¤º...")
            await demo.run_full_demo()
        elif mode == "interactive":
            print("\nğŸ® å¯åŠ¨äº¤äº’å¼æ¼”ç¤º...")
            await demo.run_interactive_demo()
        else:
            print(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}")
            print("æ”¯æŒçš„æ¨¡å¼: full, interactive")
    else:
        print("\nğŸ® é»˜è®¤å¯åŠ¨äº¤äº’å¼æ¼”ç¤º...")
        await demo.run_interactive_demo()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œæ¼”ç¤ºç»“æŸã€‚")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {str(e)}")
        sys.exit(1) 