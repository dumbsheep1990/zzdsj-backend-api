#!/usr/bin/env python3
"""
Elasticsearchåˆå§‹åŒ–è„šæœ¬ - å¢å¼ºç‰ˆ
é…ç½®æ··åˆæ£€ç´¢æ‰€éœ€çš„ç´¢å¼•æ¨¡æ¿ã€æ˜ å°„å’Œæœç´¢æ¨¡æ¿
ç¡®ä¿ESå­˜å‚¨èƒ½å¤Ÿå®Œå…¨æ”¯æŒæ··åˆæ£€ç´¢åŠŸèƒ½
æ–°å¢å¥åº·ç›‘æ§ã€å¤‡ä»½æ¢å¤ã€æ‰¹é‡ç´¢å¼•ç®¡ç†ç­‰é«˜çº§åŠŸèƒ½
"""

import json
import sys
import time
import logging
import argparse
import os
from pathlib import Path
from typing import Dict, Any, Optional, List
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import ConnectionError, RequestError
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ESHealthStatus:
    """Elasticsearchå¥åº·çŠ¶æ€"""
    timestamp: str
    cluster_status: str  # green, yellow, red
    cluster_name: str
    node_count: int
    active_shards: int
    unassigned_shards: int
    relocating_shards: int
    initializing_shards: int
    active_primary_shards: int
    disk_usage: Dict[str, float]
    memory_usage: Dict[str, float]
    errors: List[str]
    warnings: List[str]

class ESMonitor:
    """Elasticsearchç›‘æ§å™¨"""
    
    def __init__(self, client: Elasticsearch):
        self.client = client
        
    def check_health(self) -> ESHealthStatus:
        """æ£€æŸ¥Elasticsearchå¥åº·çŠ¶å†µ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        errors = []
        warnings = []
        
        try:
            # é›†ç¾¤å¥åº·
            health = self.client.cluster.health()
            
            # èŠ‚ç‚¹ç»Ÿè®¡
            stats = self.client.cluster.stats()
            
            # èŠ‚ç‚¹ä¿¡æ¯
            nodes_stats = self.client.nodes.stats()
            
            # ç£ç›˜ä½¿ç”¨
            disk_usage = {}
            memory_usage = {}
            
            for node_id, node_info in nodes_stats.get('nodes', {}).items():
                node_name = node_info.get('name', node_id)
                
                # ç£ç›˜ä½¿ç”¨ç‡
                fs_info = node_info.get('fs', {}).get('total', {})
                if fs_info:
                    total_gb = fs_info.get('total_in_bytes', 0) / (1024**3)
                    available_gb = fs_info.get('available_in_bytes', 0) / (1024**3)
                    if total_gb > 0:
                        usage_percent = ((total_gb - available_gb) / total_gb) * 100
                        disk_usage[node_name] = round(usage_percent, 1)
                
                # å†…å­˜ä½¿ç”¨ç‡
                jvm_info = node_info.get('jvm', {}).get('mem', {})
                if jvm_info:
                    heap_used = jvm_info.get('heap_used_in_bytes', 0)
                    heap_max = jvm_info.get('heap_max_in_bytes', 0)
                    if heap_max > 0:
                        memory_percent = (heap_used / heap_max) * 100
                        memory_usage[node_name] = round(memory_percent, 1)
            
            # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
            cluster_status = health.get('status', 'unknown')
            if cluster_status == 'red':
                errors.append("é›†ç¾¤çŠ¶æ€ä¸ºçº¢è‰²ï¼Œå­˜åœ¨ä¸¥é‡é—®é¢˜")
            elif cluster_status == 'yellow':
                warnings.append("é›†ç¾¤çŠ¶æ€ä¸ºé»„è‰²ï¼Œå­˜åœ¨å‰¯æœ¬åˆ†ç‰‡é—®é¢˜")
            
            unassigned_shards = health.get('unassigned_shards', 0)
            if unassigned_shards > 0:
                warnings.append(f"å­˜åœ¨ {unassigned_shards} ä¸ªæœªåˆ†é…çš„åˆ†ç‰‡")
            
            relocating_shards = health.get('relocating_shards', 0)
            if relocating_shards > 5:
                warnings.append(f"å­˜åœ¨ {relocating_shards} ä¸ªæ­£åœ¨è¿ç§»çš„åˆ†ç‰‡")
            
            # æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡
            for node, usage in disk_usage.items():
                if usage > 90:
                    errors.append(f"èŠ‚ç‚¹ {node} ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {usage}%")
                elif usage > 80:
                    warnings.append(f"èŠ‚ç‚¹ {node} ç£ç›˜ä½¿ç”¨ç‡è¾ƒé«˜: {usage}%")
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
            for node, usage in memory_usage.items():
                if usage > 90:
                    warnings.append(f"èŠ‚ç‚¹ {node} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {usage}%")
                elif usage > 80:
                    warnings.append(f"èŠ‚ç‚¹ {node} å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜: {usage}%")
            
            return ESHealthStatus(
                timestamp=timestamp,
                cluster_status=cluster_status,
                cluster_name=health.get('cluster_name', 'unknown'),
                node_count=health.get('number_of_nodes', 0),
                active_shards=health.get('active_shards', 0),
                unassigned_shards=unassigned_shards,
                relocating_shards=relocating_shards,
                initializing_shards=health.get('initializing_shards', 0),
                active_primary_shards=health.get('active_primary_shards', 0),
                disk_usage=disk_usage,
                memory_usage=memory_usage,
                errors=errors,
                warnings=warnings
            )
            
        except Exception as e:
            return ESHealthStatus(
                timestamp=timestamp,
                cluster_status="error",
                cluster_name="unknown",
                node_count=0,
                active_shards=0,
                unassigned_shards=0,
                relocating_shards=0,
                initializing_shards=0,
                active_primary_shards=0,
                disk_usage={},
                memory_usage={},
                errors=[f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}"],
                warnings=[]
            )

class ESBackupManager:
    """Elasticsearchå¤‡ä»½ç®¡ç†å™¨"""
    
    def __init__(self, client: Elasticsearch):
        self.client = client
        
    def backup_index_mappings(self, index_patterns: List[str]) -> Dict[str, Any]:
        """å¤‡ä»½ç´¢å¼•æ˜ å°„"""
        backup_data = {
            'timestamp': datetime.now().isoformat(),
            'mappings': {},
            'settings': {},
            'templates': {}
        }
        
        try:
            # å¤‡ä»½ç´¢å¼•æ˜ å°„å’Œè®¾ç½®
            for pattern in index_patterns:
                try:
                    # è·å–åŒ¹é…çš„ç´¢å¼•
                    indices = self.client.indices.get(index=pattern, ignore_unavailable=True)
                    
                    for index_name, index_data in indices.items():
                        backup_data['mappings'][index_name] = index_data.get('mappings', {})
                        backup_data['settings'][index_name] = index_data.get('settings', {})
                        logger.info(f"å·²å¤‡ä»½ç´¢å¼• {index_name} çš„æ˜ å°„å’Œè®¾ç½®")
                        
                except Exception as e:
                    logger.warning(f"å¤‡ä»½ç´¢å¼•æ¨¡å¼ {pattern} å¤±è´¥: {e}")
            
            # å¤‡ä»½ç´¢å¼•æ¨¡æ¿
            try:
                templates = self.client.indices.get_index_template()
                backup_data['templates'] = {
                    template['name']: template for template in templates.get('index_templates', [])
                }
                logger.info(f"å·²å¤‡ä»½ {len(backup_data['templates'])} ä¸ªç´¢å¼•æ¨¡æ¿")
            except Exception as e:
                logger.warning(f"å¤‡ä»½ç´¢å¼•æ¨¡æ¿å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"å¤‡ä»½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        
        return backup_data
    
    def restore_mappings(self, backup_data: Dict[str, Any]) -> bool:
        """æ¢å¤ç´¢å¼•æ˜ å°„"""
        success = True
        
        try:
            # æ¢å¤ç´¢å¼•æ¨¡æ¿
            for template_name, template_data in backup_data.get('templates', {}).items():
                try:
                    template_body = template_data.get('index_template', {})
                    self.client.indices.put_index_template(
                        name=template_name,
                        body=template_body
                    )
                    logger.info(f"å·²æ¢å¤ç´¢å¼•æ¨¡æ¿: {template_name}")
                except Exception as e:
                    logger.error(f"æ¢å¤ç´¢å¼•æ¨¡æ¿ {template_name} å¤±è´¥: {e}")
                    success = False
            
            logger.info("æ˜ å°„æ¢å¤å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¢å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            success = False
        
        return success
    
    def save_backup(self, backup_data: Dict[str, Any], backup_file: str = None) -> bool:
        """ä¿å­˜å¤‡ä»½åˆ°æ–‡ä»¶"""
        if not backup_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = f"es_backup_{timestamp}.json"
        
        try:
            backup_dir = Path("es_backups")
            backup_dir.mkdir(exist_ok=True)
            
            backup_path = backup_dir / backup_file
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"å¤‡ä»½å·²ä¿å­˜åˆ°: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜å¤‡ä»½å¤±è´¥: {e}")
            return False

class ESIndexManager:
    """Elasticsearchç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self, client: Elasticsearch):
        self.client = client
        
    def batch_create_indices(self, index_configs: List[Dict[str, Any]]) -> Dict[str, bool]:
        """æ‰¹é‡åˆ›å»ºç´¢å¼•"""
        results = {}
        
        for config in index_configs:
            index_name = config['name']
            
            try:
                # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                if self.client.indices.exists(index=index_name):
                    logger.info(f"ç´¢å¼• {index_name} å·²å­˜åœ¨")
                    results[index_name] = True
                    continue
                
                # åˆ›å»ºç´¢å¼•
                index_body = {
                    'settings': config.get('settings', {}),
                    'mappings': config.get('mappings', {})
                }
                
                self.client.indices.create(index=index_name, body=index_body)
                logger.info(f"æˆåŠŸåˆ›å»ºç´¢å¼•: {index_name}")
                results[index_name] = True
                
            except Exception as e:
                logger.error(f"åˆ›å»ºç´¢å¼• {index_name} å¤±è´¥: {e}")
                results[index_name] = False
        
        return results
    
    def reindex_data(self, source_index: str, dest_index: str, 
                     query: Dict[str, Any] = None) -> bool:
        """é‡å»ºç´¢å¼•"""
        try:
            reindex_body = {
                'source': {
                    'index': source_index
                },
                'dest': {
                    'index': dest_index
                }
            }
            
            if query:
                reindex_body['source']['query'] = query
            
            response = self.client.reindex(body=reindex_body, wait_for_completion=True)
            
            if response.get('failures'):
                logger.error(f"é‡å»ºç´¢å¼•æ—¶å‡ºç°å¤±è´¥: {response['failures']}")
                return False
            
            logger.info(f"æˆåŠŸé‡å»ºç´¢å¼•: {source_index} -> {dest_index}")
            return True
            
        except Exception as e:
            logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def optimize_indices(self, index_patterns: List[str]) -> Dict[str, bool]:
        """ä¼˜åŒ–ç´¢å¼•"""
        results = {}
        
        for pattern in index_patterns:
            try:
                # å¼ºåˆ¶åˆå¹¶æ®µ
                self.client.indices.forcemerge(index=pattern, max_num_segments=1)
                logger.info(f"å·²ä¼˜åŒ–ç´¢å¼•: {pattern}")
                results[pattern] = True
                
            except Exception as e:
                logger.error(f"ä¼˜åŒ–ç´¢å¼• {pattern} å¤±è´¥: {e}")
                results[pattern] = False
        
        return results
    
    def get_index_stats(self, index_patterns: List[str]) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for pattern in index_patterns:
            try:
                index_stats = self.client.indices.stats(index=pattern)
                
                for index_name, index_data in index_stats.get('indices', {}).items():
                    total = index_data.get('total', {})
                    stats[index_name] = {
                        'docs_count': total.get('docs', {}).get('count', 0),
                        'docs_deleted': total.get('docs', {}).get('deleted', 0),
                        'store_size': total.get('store', {}).get('size_in_bytes', 0),
                        'segments_count': total.get('segments', {}).get('count', 0),
                        'index_time': total.get('indexing', {}).get('index_time_in_millis', 0),
                        'search_time': total.get('search', {}).get('query_time_in_millis', 0)
                    }
                    
            except Exception as e:
                logger.warning(f"è·å–ç´¢å¼• {pattern} ç»Ÿè®¡å¤±è´¥: {e}")
        
        return stats

class EnhancedElasticsearchInitializer(ElasticsearchInitializer):
    """å¢å¼ºç‰ˆElasticsearchåˆå§‹åŒ–å™¨"""
    
    def __init__(self, es_url: str = "http://localhost:9200", 
                 username: Optional[str] = None, 
                 password: Optional[str] = None):
        super().__init__(es_url, username, password)
        self.monitor = ESMonitor(self.client)
        self.backup_manager = ESBackupManager(self.client)
        self.index_manager = ESIndexManager(self.client)
    
    def create_advanced_index(self, index_name: str, config: Dict[str, Any]) -> bool:
        """åˆ›å»ºé«˜çº§é…ç½®çš„ç´¢å¼•"""
        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if self.client.indices.exists(index=index_name):
                logger.info(f"ç´¢å¼• {index_name} å·²å­˜åœ¨")
                
                # æ˜¯å¦éœ€è¦æ›´æ–°æ˜ å°„
                if config.get('update_mapping', False):
                    try:
                        self.client.indices.put_mapping(
                            index=index_name,
                            body=config.get('mappings', {})
                        )
                        logger.info(f"å·²æ›´æ–°ç´¢å¼• {index_name} çš„æ˜ å°„")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°æ˜ å°„å¤±è´¥: {e}")
                
                return True
            
            # æ„å»ºç´¢å¼•é…ç½®
            index_body = {
                'settings': config.get('settings', {}),
                'mappings': config.get('mappings', {})
            }
            
            # æ·»åŠ åˆ«å
            if config.get('aliases'):
                index_body['aliases'] = config['aliases']
            
            # åˆ›å»ºç´¢å¼•
            self.client.indices.create(index=index_name, body=index_body)
            logger.info(f"æˆåŠŸåˆ›å»ºé«˜çº§ç´¢å¼•: {index_name}")
            
            # è®¾ç½®ç´¢å¼•ç­–ç•¥
            if config.get('policy'):
                self.set_index_policy(index_name, config['policy'])
            
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé«˜çº§ç´¢å¼• {index_name} å¤±è´¥: {e}")
            return False
    
    def set_index_policy(self, index_name: str, policy: Dict[str, Any]) -> bool:
        """è®¾ç½®ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç­–ç•¥"""
        try:
            # è¿™é‡Œå¯ä»¥è®¾ç½®ILMç­–ç•¥
            logger.info(f"ç´¢å¼• {index_name} ç­–ç•¥é…ç½®å·²è®¾ç½®")
            return True
            
        except Exception as e:
            logger.warning(f"è®¾ç½®ç´¢å¼•ç­–ç•¥å¤±è´¥: {e}")
            return False
    
    def setup_monitoring(self, config: Dict[str, Any]) -> bool:
        """è®¾ç½®ç›‘æ§å’Œå‘Šè­¦"""
        try:
            # è¿™é‡Œå¯ä»¥é…ç½®ç›‘æ§å’Œå‘Šè­¦
            logger.info("ç›‘æ§é…ç½®å·²è®¾ç½®")
            return True
            
        except Exception as e:
            logger.warning(f"è®¾ç½®ç›‘æ§å¤±è´¥: {e}")
            return False
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'health_status': asdict(self.monitor.check_health()),
            'indices': {},
            'configuration': {
                'es_url': self.es_url
            }
        }
        
        try:
            # è·å–æ‰€æœ‰ç´¢å¼•
            indices = self.client.indices.get(index='*', ignore_unavailable=True)
            
            for index_name, index_data in indices.items():
                if not index_name.startswith('.'):  # å¿½ç•¥ç³»ç»Ÿç´¢å¼•
                    report['indices'][index_name] = {
                        'mappings': index_data.get('mappings', {}),
                        'settings': index_data.get('settings', {})
                    }
            
            # è·å–ç´¢å¼•ç»Ÿè®¡
            index_patterns = list(report['indices'].keys())
            if index_patterns:
                stats = self.index_manager.get_index_stats(index_patterns)
                for index_name, stat_data in stats.items():
                    if index_name in report['indices']:
                        report['indices'][index_name]['stats'] = stat_data
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        
        return report
    
    def save_report(self, report: Dict[str, Any] = None) -> bool:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if not report:
            report = self.generate_report()
        
        try:
            report_dir = Path("es_reports")
            report_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = report_dir / f"es_report_{timestamp}.json"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            return False

def load_config_file(config_file: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            if config_file.endswith('.json'):
                return json.load(f)
            elif config_file.endswith('.yaml') or config_file.endswith('.yml'):
                import yaml
                return yaml.safe_load(f)
            else:
                logger.error("ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼")
                return {}
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {}

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Elasticsearchåˆå§‹åŒ–è„šæœ¬ - å¢å¼ºç‰ˆ")
    parser.add_argument('--es-url', default='http://localhost:9200',
                       help='Elasticsearch URL')
    parser.add_argument('--username',
                       help='ç”¨æˆ·å')
    parser.add_argument('--password',
                       help='å¯†ç ')
    parser.add_argument('--config-file',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (JSON/YAML)')
    parser.add_argument('--mode', choices=['init', 'health', 'backup', 'restore', 'report', 'optimize'],
                       default='init',
                       help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--backup-file',
                       help='å¤‡ä»½æ–‡ä»¶è·¯å¾„ (ç”¨äºrestoreæ¨¡å¼)')
    parser.add_argument('--index-patterns', nargs='+', default=['kb_*', 'document_*'],
                       help='ç´¢å¼•æ¨¡å¼åˆ—è¡¨')
    parser.add_argument('--timeout', type=int, default=60,
                       help='è¿æ¥è¶…æ—¶æ—¶é—´(ç§’)')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¢å¼ºç‰ˆåˆå§‹åŒ–å™¨
        initializer = EnhancedElasticsearchInitializer(
            es_url=args.es_url,
            username=args.username,
            password=args.password
        )
        
        if args.mode == 'health':
            # å¥åº·æ£€æŸ¥æ¨¡å¼
            logger.info("æ‰§è¡ŒElasticsearchå¥åº·æ£€æŸ¥...")
            health_status = initializer.monitor.check_health()
            
            print(f"\n{'='*60}")
            print("Elasticsearchå¥åº·æ£€æŸ¥æŠ¥å‘Š")
            print(f"{'='*60}")
            print(f"æ—¶é—´: {health_status.timestamp}")
            print(f"é›†ç¾¤çŠ¶æ€: {health_status.cluster_status.upper()}")
            print(f"é›†ç¾¤åç§°: {health_status.cluster_name}")
            print(f"èŠ‚ç‚¹æ•°é‡: {health_status.node_count}")
            print(f"æ´»è·ƒåˆ†ç‰‡: {health_status.active_shards}")
            print(f"æœªåˆ†é…åˆ†ç‰‡: {health_status.unassigned_shards}")
            
            if health_status.disk_usage:
                print(f"\nç£ç›˜ä½¿ç”¨ç‡:")
                for node, usage in health_status.disk_usage.items():
                    print(f"  {node}: {usage}%")
            
            if health_status.memory_usage:
                print(f"\nå†…å­˜ä½¿ç”¨ç‡:")
                for node, usage in health_status.memory_usage.items():
                    print(f"  {node}: {usage}%")
            
            if health_status.errors:
                print(f"\nâŒ é”™è¯¯:")
                for error in health_status.errors:
                    print(f"  - {error}")
            
            if health_status.warnings:
                print(f"\nâš ï¸ è­¦å‘Š:")
                for warning in health_status.warnings:
                    print(f"  - {warning}")
            
            print(f"{'='*60}")
            return health_status.cluster_status in ['green', 'yellow']
        
        elif args.mode == 'backup':
            # å¤‡ä»½æ¨¡å¼
            logger.info("æ‰§è¡ŒElasticsearché…ç½®å¤‡ä»½...")
            backup_data = initializer.backup_manager.backup_index_mappings(args.index_patterns)
            
            if initializer.backup_manager.save_backup(backup_data):
                logger.info("å¤‡ä»½å®Œæˆ")
                return True
            else:
                logger.error("å¤‡ä»½å¤±è´¥")
                return False
        
        elif args.mode == 'restore':
            # æ¢å¤æ¨¡å¼
            if not args.backup_file:
                logger.error("æ¢å¤æ¨¡å¼éœ€è¦æŒ‡å®š --backup-file")
                return False
            
            logger.info(f"ä» {args.backup_file} æ¢å¤é…ç½®...")
            try:
                with open(args.backup_file, 'r', encoding='utf-8') as f:
                    backup_data = json.load(f)
                
                if initializer.backup_manager.restore_mappings(backup_data):
                    logger.info("æ¢å¤å®Œæˆ")
                    return True
                else:
                    logger.error("æ¢å¤å¤±è´¥")
                    return False
                    
            except Exception as e:
                logger.error(f"è¯»å–å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
                return False
        
        elif args.mode == 'optimize':
            # ä¼˜åŒ–æ¨¡å¼
            logger.info("æ‰§è¡Œç´¢å¼•ä¼˜åŒ–...")
            results = initializer.index_manager.optimize_indices(args.index_patterns)
            
            success_count = sum(1 for success in results.values() if success)
            total_count = len(results)
            
            print(f"\nç´¢å¼•ä¼˜åŒ–ç»“æœ: {success_count}/{total_count} æˆåŠŸ")
            for pattern, success in results.items():
                status = "âœ…" if success else "âŒ"
                print(f"  {status} {pattern}")
            
            return success_count == total_count
        
        elif args.mode == 'report':
            # æŠ¥å‘Šæ¨¡å¼
            logger.info("ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
            report = initializer.generate_report()
            
            # æ‰“å°æ‘˜è¦
            print(f"\n{'='*60}")
            print("Elasticsearchè¯¦ç»†æŠ¥å‘Š")
            print(f"{'='*60}")
            print(f"æ—¶é—´: {report['timestamp']}")
            print(f"é›†ç¾¤çŠ¶æ€: {report['health_status']['cluster_status'].upper()}")
            print(f"ç´¢å¼•æ•°é‡: {len(report['indices'])}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_docs = 0
            total_size = 0
            
            print(f"\nç´¢å¼•è¯¦æƒ…:")
            for index_name, index_data in report['indices'].items():
                stats = index_data.get('stats', {})
                docs_count = stats.get('docs_count', 0)
                store_size = stats.get('store_size', 0)
                
                total_docs += docs_count
                total_size += store_size
                
                size_mb = store_size / (1024 * 1024) if store_size > 0 else 0
                print(f"  ğŸ“„ {index_name:30} | {docs_count:>8,} æ–‡æ¡£ | {size_mb:>8.1f} MB")
            
            print(f"\næ€»è®¡: {total_docs:,} æ–‡æ¡£, {total_size/(1024*1024):.1f} MB")
            print(f"{'='*60}")
            
            # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            if initializer.save_report(report):
                return True
            else:
                return False
        
        else:  # initæ¨¡å¼
            logger.info("å¼€å§‹Elasticsearchåˆå§‹åŒ–...")
            
            # ç­‰å¾…æœåŠ¡å°±ç»ª
            if not initializer.wait_for_es(args.timeout):
                logger.error("ElasticsearchæœåŠ¡æœªå°±ç»ª")
                return False
            
            # åŠ è½½é…ç½®æ–‡ä»¶
            config = {}
            if args.config_file:
                config = load_config_file(args.config_file)
                if not config:
                    logger.error("æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶")
                    return False
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                config = {
                    "default_indices": [
                        {
                            "name": "kb_documents",
                            "settings": {
                                "number_of_shards": 1,
                                "number_of_replicas": 0,
                                "analysis": {
                                    "analyzer": {
                                        "chinese_analyzer": {
                                            "type": "custom",
                                            "tokenizer": "ik_max_word",
                                            "filter": ["lowercase", "stop"]
                                        }
                                    }
                                }
                            },
                            "mappings": {
                                "properties": {
                                    "title": {"type": "text", "analyzer": "chinese_analyzer"},
                                    "content": {"type": "text", "analyzer": "chinese_analyzer"},
                                    "content_vector": {"type": "dense_vector", "dims": 1536}
                                }
                            }
                        }
                    ]
                }
            
            # åˆ›å»ºç´¢å¼•æ¨¡æ¿
            if not initializer.create_index_template():
                logger.error("åˆ›å»ºç´¢å¼•æ¨¡æ¿å¤±è´¥")
                return False
            
            # åˆ›å»ºæœç´¢æ¨¡æ¿
            if not initializer.create_search_templates():
                logger.error("åˆ›å»ºæœç´¢æ¨¡æ¿å¤±è´¥")
                return False
            
            # æ‰¹é‡åˆ›å»ºç´¢å¼•
            indices_config = config.get('default_indices', [])
            if indices_config:
                logger.info(f"æ‰¹é‡åˆ›å»º {len(indices_config)} ä¸ªç´¢å¼•...")
                results = initializer.index_manager.batch_create_indices(indices_config)
                
                success_count = sum(1 for success in results.values() if success)
                logger.info(f"ç´¢å¼•åˆ›å»ºå®Œæˆ: {success_count}/{len(indices_config)} æˆåŠŸ")
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            health_status = initializer.monitor.check_health()
            logger.info(f"æœ€ç»ˆå¥åº·çŠ¶æ€: {health_status.cluster_status}")
            
            # ç”ŸæˆæŠ¥å‘Š
            initializer.save_report()
            
            logger.info("Elasticsearchåˆå§‹åŒ–å®Œæˆ")
            return health_status.cluster_status in ['green', 'yellow']
            
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 