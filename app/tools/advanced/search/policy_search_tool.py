"""
æ”¿ç­–æ£€ç´¢å·¥å…·æ¨¡å—
åŸºäºå±‚çº§é—¨æˆ·æ£€ç´¢çš„æ”¿ç­–æ–‡æ¡£æœç´¢å·¥å…·ï¼Œæ”¯æŒåœ°æ–¹â†’çœçº§â†’æœç´¢å¼•æ“çš„æ™ºèƒ½æ£€ç´¢ç­–ç•¥
ç°å·²é›†æˆæ™ºèƒ½çˆ¬å–è°ƒåº¦å™¨ï¼Œå¯è‡ªåŠ¨è§£ææœç´¢ç»“æœé¡µé¢å†…å®¹
"""

import logging
import asyncio
import aiohttp
import re
from typing import Dict, List, Any, Optional, Union, Tuple
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field

from llama_index.core.tools import BaseTool, FunctionTool
from llama_index.core.schema import Document
from app.frameworks.fastmcp.tools import register_tool
from app.tools.base.search.search_tool import get_search_tool
from core.system_config.config_manager import SystemConfigManager
from app.models.database import get_db

# å¯¼å…¥æ™ºèƒ½çˆ¬å–è°ƒåº¦å™¨
from .intelligent_crawler_scheduler import (
    get_crawler_scheduler, 
    smart_crawl_url, 
    smart_crawl_urls,
    CrawlTask,
    PageComplexity
)

logger = logging.getLogger(__name__)


class SearchLevel(Enum):
    """æ£€ç´¢å±‚çº§æšä¸¾"""
    LOCAL = "local"          # åœ°æ–¹é—¨æˆ·
    PROVINCIAL = "provincial"  # çœçº§é—¨æˆ·
    SEARCH_ENGINE = "search_engine"  # æœç´¢å¼•æ“


@dataclass
class PortalConfig:
    """é—¨æˆ·é…ç½®ç±»"""
    name: str
    base_url: str
    search_endpoint: str
    search_params: Dict[str, str]
    result_selector: str = ""
    encoding: str = "utf-8"
    timeout: int = 30
    max_results: int = 10


class PolicySearchConfig(BaseModel):
    """æ”¿ç­–æ£€ç´¢é…ç½®æ¨¡å‹"""
    region_name: str = Field(..., description="åœ°åŒºåç§°")
    local_portal: Optional[PortalConfig] = Field(None, description="åœ°æ–¹é—¨æˆ·é…ç½®")
    provincial_portal: Optional[PortalConfig] = Field(None, description="çœçº§é—¨æˆ·é…ç½®")
    search_strategy: str = Field("auto", description="æ£€ç´¢ç­–ç•¥ï¼šauto|local_only|provincial_only|search_only")
    quality_threshold: float = Field(0.6, description="ç»“æœè´¨é‡é˜ˆå€¼")
    max_retries: int = Field(2, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
    # æ–°å¢æ™ºèƒ½çˆ¬å–é…ç½®
    enable_intelligent_crawling: bool = Field(True, description="æ˜¯å¦å¯ç”¨æ™ºèƒ½çˆ¬å–")
    crawl_search_results: bool = Field(True, description="æ˜¯å¦çˆ¬å–æœç´¢ç»“æœé¡µé¢")
    crawl_detail_pages: bool = Field(False, description="æ˜¯å¦çˆ¬å–è¯¦æƒ…é¡µé¢")
    max_crawl_pages: int = Field(5, description="æœ€å¤§çˆ¬å–é¡µé¢æ•°")


class PolicySearchResult(BaseModel):
    """æ”¿ç­–æ£€ç´¢ç»“æœæ¨¡å‹"""
    title: str
    url: str
    content: str
    published_date: Optional[str] = None
    source: str
    search_level: SearchLevel
    relevance_score: float = 0.0
    policy_type: Optional[str] = None
    department: Optional[str] = None
    # æ–°å¢æ™ºèƒ½è§£æç»“æœ
    intelligent_analysis: Optional[Dict[str, Any]] = None
    content_quality_score: float = 0.0
    extraction_method: str = "traditional"  # traditional, intelligent_crawl


class PolicySearchTool(BaseTool):
    """
    æ”¿ç­–æ£€ç´¢å·¥å…·
    æ”¯æŒåŸºäºå±‚çº§é—¨æˆ·çš„æ”¿ç­–æ–‡æ¡£æ£€ç´¢ï¼Œä»åœ°æ–¹é—¨æˆ·å¼€å§‹ï¼Œé€çº§å‘ä¸Šæœç´¢
    ç°å·²é›†æˆæ™ºèƒ½çˆ¬å–è°ƒåº¦å™¨ï¼Œå¯è‡ªåŠ¨è§£ææœç´¢ç»“æœé¡µé¢å†…å®¹
    """
    
    def __init__(self, name: str = "policy_search"):
        """åˆå§‹åŒ–æ”¿ç­–æ£€ç´¢å·¥å…·"""
        super().__init__(name=name)
        self.description = (
            "æ”¿ç­–æ£€ç´¢å·¥å…·ï¼šæ”¯æŒæ™ºèƒ½åˆ†å±‚æ£€ç´¢ï¼Œä¼˜å…ˆä½¿ç”¨åœ°æ–¹æ”¿åºœé—¨æˆ·ç½‘ç«™ï¼Œ"
            "ç„¶åçœçº§é—¨æˆ·ï¼Œæœ€åä½¿ç”¨æœç´¢å¼•æ“ã€‚é›†æˆæ™ºèƒ½çˆ¬å–è°ƒåº¦å™¨ï¼Œ"
            "å¯è‡ªåŠ¨è§£æé¡µé¢å†…å®¹ï¼Œæä¾›æ›´é«˜è´¨é‡çš„ç»“æœã€‚"
        )
        self.search_tool = get_search_tool()
        self.session = None
        self.crawler_scheduler = None
        self.config_manager = None
        self._init_default_portals()
    
    async def _initialize_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        if self.crawler_scheduler is None:
            self.crawler_scheduler = get_crawler_scheduler()
            await self.crawler_scheduler.initialize()
        
        if self.config_manager is None:
            db = next(get_db())
            self.config_manager = SystemConfigManager(db)
    
    def _init_default_portals(self):
        """åˆå§‹åŒ–é»˜è®¤é—¨æˆ·é…ç½®"""
        self.default_portals = {
            "å…­ç›˜æ°´": PortalConfig(
                name="å…­ç›˜æ°´å¸‚äººæ°‘æ”¿åºœ",
                base_url="https://www.gzlps.gov.cn",
                search_endpoint="/so/search.shtml",
                search_params={
                    "tenantId": "30",
                    "tenantIds": "",
                    "configTenantId": "",
                    "searchWord": "{query}"
                },
                result_selector=".search-result-item",
                max_results=10
            ),
            "è´µå·": PortalConfig(
                name="è´µå·çœäººæ°‘æ”¿åºœ",
                base_url="https://www.guizhou.gov.cn",
                search_endpoint="/so/search.shtml",
                search_params={
                    "tenantId": "186",
                    "tenantIds": "",
                    "configTenantId": "",
                    "searchWord": "{query}"
                },
                result_selector=".search-result-item",
                max_results=10
            )
        }
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å–HTTPä¼šè¯"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
            )
        return self.session
    
    async def _search_portal(
        self, 
        portal: PortalConfig, 
        query: str, 
        search_level: SearchLevel,
        enable_intelligent_crawling: bool = True
    ) -> List[PolicySearchResult]:
        """åœ¨æŒ‡å®šé—¨æˆ·è¿›è¡Œæœç´¢"""
        try:
            session = await self._get_session()
            
            # æ„å»ºæœç´¢URL
            search_params = {}
            for key, value in portal.search_params.items():
                if "{query}" in value:
                    search_params[key] = value.format(query=query)
                else:
                    search_params[key] = value
            
            search_url = urljoin(portal.base_url, portal.search_endpoint)
            if search_params:
                search_url += "?" + urlencode(search_params)
            
            logger.info(f"æœç´¢URL: {search_url}")
            
            # å‘é€æœç´¢è¯·æ±‚
            async with session.get(search_url) as response:
                if response.status == 200:
                    content = await response.text(encoding=portal.encoding)
                    
                    # ä¼ ç»Ÿè§£æ
                    traditional_results = await self._parse_portal_results(
                        content, portal, search_level, query
                    )
                    
                    # å¦‚æœå¯ç”¨æ™ºèƒ½çˆ¬å–ï¼Œåˆ™è¿›è¡Œæ™ºèƒ½è§£æ
                    if enable_intelligent_crawling and traditional_results:
                        enhanced_results = await self._enhance_results_with_intelligent_crawling(
                            traditional_results, search_url, query
                        )
                        return enhanced_results
                    
                    return traditional_results
                else:
                    logger.warning(f"é—¨æˆ·æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"é—¨æˆ·æœç´¢å‡ºé”™: {str(e)}")
            return []
    
    async def _enhance_results_with_intelligent_crawling(
        self,
        traditional_results: List[PolicySearchResult],
        search_url: str,
        query: str
    ) -> List[PolicySearchResult]:
        """ä½¿ç”¨æ™ºèƒ½çˆ¬å–å¢å¼ºæœç´¢ç»“æœ"""
        try:
            await self._initialize_components()
            
            # é¦–å…ˆå¯¹æœç´¢ç»“æœé¡µé¢è¿›è¡Œæ™ºèƒ½åˆ†æ
            crawl_result = await smart_crawl_url(
                url=search_url,
                task_type="content_extraction",
                extraction_rules=[
                    "æå–æ”¿ç­–æ–‡æ¡£æ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦å’Œå‘å¸ƒæ—¥æœŸ",
                    "è¯†åˆ«æ”¿ç­–ç±»å‹å’Œå‘å¸ƒéƒ¨é—¨",
                    "æå–ç›¸å…³åº¦å’Œè´¨é‡è¯„åˆ†ä¿¡æ¯"
                ],
                analysis_goals=["æ”¿ç­–æ–‡æ¡£åˆ—è¡¨", "å…ƒæ•°æ®", "ç›¸å…³æ€§åˆ†æ"]
            )
            
            enhanced_results = []
            
            if crawl_result.success and crawl_result.data:
                # å°†æ™ºèƒ½çˆ¬å–çš„ç»“æœä¸ä¼ ç»Ÿç»“æœè¿›è¡Œèåˆ
                intelligent_data = crawl_result.data.get("data", {})
                
                for result in traditional_results:
                    enhanced_result = result.copy()
                    enhanced_result.intelligent_analysis = intelligent_data
                    enhanced_result.content_quality_score = crawl_result.content_quality_score
                    enhanced_result.extraction_method = "intelligent_crawl"
                    
                    # å°è¯•ä»æ™ºèƒ½åˆ†æä¸­æå–æ›´å‡†ç¡®çš„ä¿¡æ¯
                    if "extracted_content" in intelligent_data:
                        extracted = intelligent_data["extracted_content"]
                        if isinstance(extracted, dict):
                            # æ›´æ–°æ”¿ç­–ç±»å‹
                            if "policy_type" in extracted:
                                enhanced_result.policy_type = extracted["policy_type"]
                            
                            # æ›´æ–°å‘å¸ƒéƒ¨é—¨
                            if "department" in extracted:
                                enhanced_result.department = extracted["department"]
                            
                            # æ›´æ–°å‘å¸ƒæ—¥æœŸ
                            if "published_date" in extracted:
                                enhanced_result.published_date = extracted["published_date"]
                            
                            # æé«˜ç›¸å…³æ€§è¯„åˆ†
                            if enhanced_result.relevance_score < 0.8:
                                enhanced_result.relevance_score = min(
                                    enhanced_result.relevance_score + 0.2,
                                    1.0
                                )
                    
                    enhanced_results.append(enhanced_result)
                
                logger.info(f"æ™ºèƒ½çˆ¬å–å¢å¼ºäº† {len(enhanced_results)} ä¸ªæœç´¢ç»“æœ")
                return enhanced_results
            else:
                logger.warning(f"æ™ºèƒ½çˆ¬å–å¤±è´¥: {crawl_result.error}")
                # æ™ºèƒ½çˆ¬å–å¤±è´¥ï¼Œè¿”å›ä¼ ç»Ÿç»“æœ
                for result in traditional_results:
                    result.extraction_method = "traditional"
                return traditional_results
                
        except Exception as e:
            logger.error(f"æ™ºèƒ½çˆ¬å–å¢å¼ºå¤±è´¥: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›ä¼ ç»Ÿç»“æœ
            for result in traditional_results:
                result.extraction_method = "traditional"
            return traditional_results
    
    async def _parse_portal_results(
        self, 
        html_content: str, 
        portal: PortalConfig, 
        search_level: SearchLevel,
        query: str
    ) -> List[PolicySearchResult]:
        """è§£æé—¨æˆ·æœç´¢ç»“æœ"""
        results = []
        
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æœç´¢ç»“æœ
            # è¿™é‡Œä½¿ç”¨é€šç”¨çš„HTMLç»“æ„è§£æï¼Œå®é™…éƒ¨ç½²æ—¶å¯èƒ½éœ€è¦é’ˆå¯¹å…·ä½“ç½‘ç«™è°ƒæ•´
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_pattern = r'<h3[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></h3>'
            matches = re.findall(title_pattern, html_content, re.IGNORECASE | re.DOTALL)
            
            # æå–æ‘˜è¦
            summary_pattern = r'<p[^>]*class="[^"]*summary[^"]*"[^>]*>([^<]*)</p>'
            summaries = re.findall(summary_pattern, html_content, re.IGNORECASE | re.DOTALL)
            
            # æå–æ—¥æœŸ
            date_pattern = r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            dates = re.findall(date_pattern, html_content)
            
            for i, (url, title) in enumerate(matches[:portal.max_results]):
                # æ¸…ç†æ ‡é¢˜
                title = re.sub(r'<[^>]+>', '', title).strip()
                
                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                if not url.startswith('http'):
                    url = urljoin(portal.base_url, url)
                
                # è·å–æ‘˜è¦
                summary = ""
                if i < len(summaries):
                    summary = re.sub(r'<[^>]+>', '', summaries[i]).strip()
                
                # è·å–æ—¥æœŸ
                published_date = None
                if i < len(dates):
                    published_date = dates[i]
                
                # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
                relevance_score = self._calculate_relevance(title + " " + summary, query)
                
                # è¯†åˆ«æ”¿ç­–ç±»å‹
                policy_type = self._identify_policy_type(title)
                
                result = PolicySearchResult(
                    title=title,
                    url=url,
                    content=summary,
                    published_date=published_date,
                    source=portal.name,
                    search_level=search_level,
                    relevance_score=relevance_score,
                    policy_type=policy_type,
                    extraction_method="traditional"
                )
                
                results.append(result)
            
            logger.info(f"ä» {portal.name} è§£æåˆ° {len(results)} ä¸ªç»“æœ")
            
        except Exception as e:
            logger.error(f"è§£æé—¨æˆ·æœç´¢ç»“æœå¤±è´¥: {str(e)}")
        
        return results
    
    def _calculate_relevance(self, text: str, query: str) -> float:
        """è®¡ç®—æ–‡æœ¬ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        try:
            text_lower = text.lower()
            query_lower = query.lower()
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
            words = query_lower.split()
            matches = 0
            total_words = len(words)
            
            for word in words:
                if word in text_lower:
                    matches += 1
            
            # åŸºç¡€å¾—åˆ†
            base_score = matches / total_words if total_words > 0 else 0
            
            # ä½ç½®æƒé‡ï¼ˆæ ‡é¢˜ä¸­çš„åŒ¹é…æƒé‡æ›´é«˜ï¼‰
            if query_lower in text_lower[:100]:  # å‡è®¾å‰100å­—ç¬¦æ˜¯æ ‡é¢˜
                base_score += 0.2
            
            return min(base_score, 1.0)
        except:
            return 0.0
    
    def _identify_policy_type(self, title: str) -> Optional[str]:
        """æ ¹æ®æ ‡é¢˜è¯†åˆ«æ”¿ç­–ç±»å‹"""
        try:
            title_lower = title.lower()
            
            policy_types = {
                "é€šçŸ¥": ["é€šçŸ¥", "å…¬å‘Š", "å…¬ç¤º"],
                "åŠæ³•": ["åŠæ³•", "è§„å®š", "ç»†åˆ™"],
                "æ„è§": ["æ„è§", "å»ºè®®", "æ–¹æ¡ˆ"],
                "æ³•è§„": ["æ¡ä¾‹", "æ³•è§„", "æ³•å¾‹"],
                "æ ‡å‡†": ["æ ‡å‡†", "è§„èŒƒ", "æŒ‡å—"]
            }
            
            for policy_type, keywords in policy_types.items():
                for keyword in keywords:
                    if keyword in title_lower:
                        return policy_type
            
            return None
        except:
            return None
    
    async def _search_engine_fallback(self, query: str) -> List[PolicySearchResult]:
        """æœç´¢å¼•æ“å›é€€ç­–ç•¥"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„æœç´¢å·¥å…·è¿›è¡Œæœç´¢
            search_query = f"{query} æ”¿ç­– site:gov.cn"
            search_results = await self.search_tool.search_async(search_query, max_results=10)
            
            results = []
            for result_data in search_results:
                result = self._parse_search_engine_result(result_data, query)
                if result:
                    results.append(result)
            
            logger.info(f"æœç´¢å¼•æ“å›é€€è·å¾— {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢å¼•æ“å›é€€å¤±è´¥: {str(e)}")
            return []
    
    def _parse_search_engine_result(self, result_data: Dict, query: str) -> Optional[PolicySearchResult]:
        """è§£ææœç´¢å¼•æ“ç»“æœ"""
        try:
            title = result_data.get("title", "")
            url = result_data.get("url", "")
            snippet = result_data.get("snippet", "")
            
            if not title or not url:
                return None
            
            # è®¡ç®—ç›¸å…³æ€§
            relevance_score = self._calculate_relevance(title + " " + snippet, query)
            
            # è¯†åˆ«æ”¿ç­–ç±»å‹
            policy_type = self._identify_policy_type(title)
            
            return PolicySearchResult(
                title=title,
                url=url,
                content=snippet,
                source="æœç´¢å¼•æ“",
                search_level=SearchLevel.SEARCH_ENGINE,
                relevance_score=relevance_score,
                policy_type=policy_type,
                extraction_method="traditional"
            )
            
        except Exception as e:
            logger.error(f"è§£ææœç´¢å¼•æ“ç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def _evaluate_results_quality(self, results: List[PolicySearchResult]) -> float:
        """è¯„ä¼°ç»“æœè´¨é‡"""
        if not results:
            return 0.0
        
        # ç»¼åˆè¯„ä¼°å„é¡¹æŒ‡æ ‡
        total_relevance = sum(r.relevance_score for r in results)
        avg_relevance = total_relevance / len(results)
        
        # è€ƒè™‘æ™ºèƒ½çˆ¬å–çš„è´¨é‡æå‡
        intelligent_results = [r for r in results if r.extraction_method == "intelligent_crawl"]
        intelligence_bonus = len(intelligent_results) / len(results) * 0.2
        
        return min(avg_relevance + intelligence_bonus, 1.0)
    
    async def _arun(
        self, 
        query: str, 
        region: str = "å…­ç›˜æ°´", 
        search_strategy: str = "auto",
        max_results: int = 10,
        enable_intelligent_crawling: bool = True
    ) -> str:
        """å¼‚æ­¥è¿è¡Œæ”¿ç­–æœç´¢"""
        await self._initialize_components()
        
        try:
            logger.info(f"å¼€å§‹æ”¿ç­–æ£€ç´¢: query={query}, region={region}, strategy={search_strategy}")
            
            all_results = []
            search_attempted = []
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ™ºèƒ½çˆ¬å–
            crawling_enabled = enable_intelligent_crawling and await self.config_manager.get_config_value(
                "crawling.enabled", True
            )
            
            # æ ¹æ®ç­–ç•¥æ‰§è¡Œæœç´¢
            if search_strategy in ["auto", "local_only"]:
                # å°è¯•åœ°æ–¹é—¨æˆ·æœç´¢
                if region in self.default_portals:
                    portal = self.default_portals[region]
                    results = await self._search_portal(
                        portal, query, SearchLevel.LOCAL, crawling_enabled
                    )
                    all_results.extend(results)
                    search_attempted.append("åœ°æ–¹é—¨æˆ·")
            
            if search_strategy in ["auto", "provincial_only"]:
                # å°è¯•çœçº§é—¨æˆ·æœç´¢
                province = region.split("å¸‚")[0] if "å¸‚" in region else region  # ç®€å•çš„çœä»½æå–
                if province in self.default_portals and province != region:
                    portal = self.default_portals[province]
                    results = await self._search_portal(
                        portal, query, SearchLevel.PROVINCIAL, crawling_enabled
                    )
                    all_results.extend(results)
                    search_attempted.append("çœçº§é—¨æˆ·")
            
            # è¯„ä¼°ç»“æœè´¨é‡
            quality_score = self._evaluate_results_quality(all_results)
            
            # å¦‚æœè´¨é‡ä¸å¤Ÿä¸”ç­–ç•¥å…è®¸ï¼Œä½¿ç”¨æœç´¢å¼•æ“
            if (search_strategy in ["auto", "search_only"] and 
                (quality_score < 0.6 or len(all_results) < 3)):
                
                search_results = await self._search_engine_fallback(query)
                all_results.extend(search_results)
                search_attempted.append("æœç´¢å¼•æ“")
            
            # æŒ‰ç›¸å…³æ€§æ’åºå¹¶é™åˆ¶ç»“æœæ•°
            all_results.sort(key=lambda x: (x.relevance_score, x.content_quality_score), reverse=True)
            final_results = all_results[:max_results]
            
            # æ ¼å¼åŒ–è¾“å‡º
            if not final_results:
                return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³æ”¿ç­–ä¿¡æ¯ã€‚\næœç´¢æ¸ é“ï¼š{', '.join(search_attempted)}"
            
            # æ„å»ºç»“æœå­—ç¬¦ä¸²
            result_text = f"ğŸ” æ”¿ç­–æ£€ç´¢ç»“æœï¼ˆæ‰¾åˆ° {len(final_results)} æ¡ï¼‰\n"
            result_text += f"ğŸ“Š æœç´¢æ¸ é“ï¼š{', '.join(search_attempted)}\n"
            result_text += f"âš¡ æ™ºèƒ½çˆ¬å–ï¼š{'å·²å¯ç”¨' if crawling_enabled else 'æœªå¯ç”¨'}\n"
            result_text += f"ğŸ“ˆ ç»“æœè´¨é‡ï¼š{quality_score:.2f}\n\n"
            
            for i, result in enumerate(final_results, 1):
                result_text += f"{i}. **{result.title}**\n"
                result_text += f"   ğŸ”— é“¾æ¥ï¼š{result.url}\n"
                result_text += f"   ğŸ“… æ¥æºï¼š{result.source} ({result.search_level.value})\n"
                
                if result.published_date:
                    result_text += f"   ğŸ“† å‘å¸ƒæ—¥æœŸï¼š{result.published_date}\n"
                
                if result.policy_type:
                    result_text += f"   ğŸ“‹ æ”¿ç­–ç±»å‹ï¼š{result.policy_type}\n"
                
                if result.department:
                    result_text += f"   ğŸ›ï¸ å‘å¸ƒéƒ¨é—¨ï¼š{result.department}\n"
                
                result_text += f"   â­ ç›¸å…³åº¦ï¼š{result.relevance_score:.2f}\n"
                result_text += f"   ğŸ¤– è§£ææ–¹å¼ï¼š{result.extraction_method}\n"
                
                if result.extraction_method == "intelligent_crawl":
                    result_text += f"   ğŸ¯ å†…å®¹è´¨é‡ï¼š{result.content_quality_score:.2f}\n"
                
                if result.content:
                    content_preview = result.content[:200] + "..." if len(result.content) > 200 else result.content
                    result_text += f"   ğŸ“„ æ‘˜è¦ï¼š{content_preview}\n"
                
                # å¦‚æœæœ‰æ™ºèƒ½åˆ†æç»“æœï¼Œæ·»åŠ å…³é”®æ´å¯Ÿ
                if result.intelligent_analysis and isinstance(result.intelligent_analysis, dict):
                    analysis = result.intelligent_analysis
                    if "key_insights" in analysis:
                        result_text += f"   ğŸ’¡ æ™ºèƒ½æ´å¯Ÿï¼š{analysis['key_insights']}\n"
                
                result_text += "\n"
            
            # æ·»åŠ æ™ºèƒ½åˆ†ææ‘˜è¦
            intelligent_count = len([r for r in final_results if r.extraction_method == "intelligent_crawl"])
            if intelligent_count > 0:
                result_text += f"\nğŸ¤– æ™ºèƒ½åˆ†ææ‘˜è¦ï¼š\n"
                result_text += f"â€¢ æˆåŠŸè§£æ {intelligent_count}/{len(final_results)} ä¸ªç»“æœ\n"
                avg_quality = sum(r.content_quality_score for r in final_results if r.extraction_method == "intelligent_crawl") / intelligent_count
                result_text += f"â€¢ å¹³å‡å†…å®¹è´¨é‡ï¼š{avg_quality:.2f}\n"
                result_text += f"â€¢ æ™ºèƒ½çˆ¬å–æå‡äº†å†…å®¹å‡†ç¡®æ€§å’Œç»“æ„åŒ–ç¨‹åº¦\n"
            
            return result_text
            
        except Exception as e:
            logger.error(f"æ”¿ç­–æ£€ç´¢å¤±è´¥: {str(e)}")
            return f"æ”¿ç­–æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        
        finally:
            # æ¸…ç†èµ„æº
            if self.session and not self.session.closed:
                await self.session.close()
    
    def _run(
        self, 
        query: str, 
        region: str = "å…­ç›˜æ°´", 
        search_strategy: str = "auto",
        max_results: int = 10,
        enable_intelligent_crawling: bool = True
    ) -> str:
        """åŒæ­¥è¿è¡Œæ”¿ç­–æœç´¢ï¼ˆLlamaIndexå…¼å®¹ï¼‰"""
        import asyncio
        return asyncio.run(self._arun(query, region, search_strategy, max_results, enable_intelligent_crawling))


@register_tool(
    name="policy_search",
    description="æ”¿ç­–æ£€ç´¢å·¥å…·ï¼Œæ”¯æŒæ™ºèƒ½åˆ†å±‚æ£€ç´¢ç­–ç•¥å’Œæ™ºèƒ½çˆ¬å–è§£æï¼Œä¼˜å…ˆä½¿ç”¨åœ°æ–¹æ”¿åºœé—¨æˆ·ï¼Œç„¶åçœçº§é—¨æˆ·ï¼Œæœ€åæœç´¢å¼•æ“",
    category="search",
    tags=["æ”¿ç­–", "æœç´¢", "æ”¿åºœ", "é—¨æˆ·", "æ™ºèƒ½çˆ¬å–"]
)
async def policy_search(
    query: str,
    region: str = "å…­ç›˜æ°´",
    search_strategy: str = "auto",
    max_results: int = 10,
    enable_intelligent_crawling: bool = True
) -> str:
    """
    æ”¿ç­–æ£€ç´¢å·¥å…·å‡½æ•°
    
    å‚æ•°:
        query: æœç´¢å…³é”®è¯
        region: ç›®æ ‡åœ°åŒº
        search_strategy: æ£€ç´¢ç­–ç•¥ï¼ˆauto|local_only|provincial_only|search_onlyï¼‰
        max_results: æœ€å¤§ç»“æœæ•°
        enable_intelligent_crawling: æ˜¯å¦å¯ç”¨æ™ºèƒ½çˆ¬å–
    """
    tool = PolicySearchTool()
    return await tool._arun(query, region, search_strategy, max_results, enable_intelligent_crawling)


def create_policy_search_function_tool() -> FunctionTool:
    """åˆ›å»ºæ”¿ç­–æ£€ç´¢åŠŸèƒ½å·¥å…·ï¼ˆLlamaIndexå…¼å®¹ï¼‰"""
    
    def sync_policy_search(
        query: str,
        region: str = "å…­ç›˜æ°´",
        search_strategy: str = "auto",
        max_results: int = 10,
        enable_intelligent_crawling: bool = True
    ) -> str:
        """åŒæ­¥ç‰ˆæœ¬çš„æ”¿ç­–æ£€ç´¢å‡½æ•°"""
        import asyncio
        return asyncio.run(policy_search(query, region, search_strategy, max_results, enable_intelligent_crawling))
    
    return FunctionTool.from_defaults(
        fn=sync_policy_search,
        name="policy_search",
        description=(
            "æ”¿ç­–æ£€ç´¢å·¥å…·ï¼šæ™ºèƒ½åˆ†å±‚æœç´¢æ”¿åºœæ”¿ç­–æ–‡æ¡£ã€‚"
            "é›†æˆæ™ºèƒ½çˆ¬å–è°ƒåº¦å™¨ï¼Œå¯è‡ªåŠ¨è§£æé¡µé¢å†…å®¹æä¾›é«˜è´¨é‡ç»“æœã€‚"
            "ä¼˜å…ˆä»åœ°æ–¹æ”¿åºœé—¨æˆ·ç½‘ç«™æœç´¢ï¼Œç„¶åçœçº§é—¨æˆ·ï¼Œæœ€åä½¿ç”¨æœç´¢å¼•æ“ã€‚"
            "æ”¯æŒè‡ªåŠ¨è´¨é‡è¯„ä¼°ã€ç­–ç•¥åˆ‡æ¢å’Œæ™ºèƒ½å†…å®¹è§£æã€‚"
            "é€‚ç”¨äºæŸ¥æ‰¾æ”¿åºœæ”¿ç­–ã€é€šçŸ¥ã€åŠäº‹æŒ‡å—ã€æ³•è§„ç­‰å®˜æ–¹æ–‡æ¡£ã€‚"
        )
    )


def get_policy_search_tool() -> PolicySearchTool:
    """è·å–æ”¿ç­–æ£€ç´¢å·¥å…·å®ä¾‹"""
    return PolicySearchTool()


# ============ å·¥å…·å¯¼å‡º ============

__all__ = [
    "PolicySearchTool",
    "PolicySearchConfig", 
    "PolicySearchResult",
    "SearchLevel",
    "PortalConfig",
    "policy_search",
    "create_policy_search_function_tool",
    "get_policy_search_tool"
] 