"""
å¼‚æ­¥æ”¿ç­–æ£€ç´¢å·¥å…·
æ”¯æŒå¤šå…³é”®è¯å¹¶å‘æ£€ç´¢ã€ç»“æœæ±‡æ€»å’Œé‡æ’
é›†æˆå¼‚æ­¥æ‰§è¡Œå¼•æ“ï¼Œæä¾›é«˜æ€§èƒ½çš„æ”¿ç­–æ–‡æ¡£æœç´¢èƒ½åŠ›
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass
from urllib.parse import urljoin, urlparse, parse_qs, urlencode

from llama_index.core.tools import BaseTool, FunctionTool
from app.frameworks.fastmcp.tools import register_tool
from core.tools.async_execution_engine import (
    AsyncExecutionEngine, 
    ExecutionConfig, 
    TaskType,
    get_global_engine
)
from core.tools.search_result_aggregator import (
    SearchResult,
    SearchResultAggregator,
    create_policy_search_aggregator
)

# å¯¼å…¥åŸæœ‰çš„æ¨¡å‹å®šä¹‰
from app.tools.advanced.search.policy_search_tool import (
    SearchLevel,
    PortalConfig,
    PolicySearchConfig,
    PolicySearchResult
)

logger = logging.getLogger(__name__)


class AsyncPolicySearchTool(BaseTool):
    """
    å¼‚æ­¥æ”¿ç­–æ£€ç´¢å·¥å…·
    æ”¯æŒå¤šå…³é”®è¯å¹¶å‘æ£€ç´¢ï¼Œé›†æˆç»“æœæ±‡æ€»å’Œé‡æ’æœºåˆ¶
    """
    
    def __init__(self, name: str = "async_policy_search"):
        """åˆå§‹åŒ–å¼‚æ­¥æ”¿ç­–æ£€ç´¢å·¥å…·"""
        super().__init__(name=name)
        self.description = (
            "å¼‚æ­¥æ”¿ç­–æ£€ç´¢å·¥å…·ï¼šæ”¯æŒå¤šå…³é”®è¯å¹¶å‘æ£€ç´¢ï¼Œä¼˜å…ˆä½¿ç”¨åœ°æ–¹æ”¿åºœé—¨æˆ·ç½‘ç«™ï¼Œ"
            "ç„¶åçœçº§é—¨æˆ·ï¼Œæœ€åä½¿ç”¨æœç´¢å¼•æ“ã€‚é›†æˆæ™ºèƒ½ç»“æœæ±‡æ€»å’Œé‡æ’æœºåˆ¶ï¼Œ"
            "æ˜¾è‘—æå‡å¤šå…³é”®è¯æ£€ç´¢çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚"
        )
        self.execution_engine: Optional[AsyncExecutionEngine] = None
        self.result_aggregator: Optional[SearchResultAggregator] = None
        self._init_default_portals()
        
    async def initialize(self):
        """åˆå§‹åŒ–å¼‚æ­¥ç»„ä»¶"""
        if not self.execution_engine:
            # é…ç½®å¼‚æ­¥æ‰§è¡Œå¼•æ“
            config = ExecutionConfig(
                max_concurrent_tasks=15,        # å…è®¸æ›´å¤šå¹¶å‘ä»»åŠ¡
                timeout_seconds=30,
                retry_attempts=2,
                enable_connection_pool=True,
                pool_size=50,
                enable_rate_limiting=True,
                rate_limit_per_second=20,       # æé«˜é€Ÿç‡é™åˆ¶
                enable_result_dedup=True,
                enable_result_ranking=True
            )
            self.execution_engine = AsyncExecutionEngine(config)
            await self.execution_engine.initialize()
        
        if not self.result_aggregator:
            self.result_aggregator = create_policy_search_aggregator()
    
    def _init_default_portals(self):
        """åˆå§‹åŒ–é»˜è®¤é—¨æˆ·é…ç½®"""
        self.default_portals = {
            "å…­ç›˜æ°´": PortalConfig(
                name="å…­ç›˜æ°´å¸‚äººæ°‘æ”¿åºœ",
                base_url="https://www.gzlps.gov.cn",
                search_endpoint="/so/search.shtml",
                search_params={
                    "tenantId": "30",
                    "tenantIds": "",
                    "configTenantId": "",
                    "searchWord": "{query}"
                },
                result_selector=".search-result-item",
                max_results=10
            ),
            "è´µå·": PortalConfig(
                name="è´µå·çœäººæ°‘æ”¿åºœ",
                base_url="https://www.guizhou.gov.cn",
                search_endpoint="/so/search.shtml",
                search_params={
                    "tenantId": "186",
                    "tenantIds": "",
                    "configTenantId": "",
                    "searchWord": "{query}"
                },
                result_selector=".search-result-item",
                max_results=10
            )
        }
    
    async def search_multi_keywords_concurrent(
        self,
        keywords: List[str],
        region: str = "å…­ç›˜æ°´",
        search_strategy: str = "auto",
        max_results: int = 50,
        enable_intelligent_crawling: bool = True
    ) -> List[SearchResult]:
        """
        å¹¶å‘æœç´¢å¤šä¸ªå…³é”®è¯
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            region: ç›®æ ‡åœ°åŒº
            search_strategy: æ£€ç´¢ç­–ç•¥
            max_results: æœ€å¤§ç»“æœæ•°
            enable_intelligent_crawling: æ˜¯å¦å¯ç”¨æ™ºèƒ½çˆ¬å–
            
        Returns:
            èšåˆå¹¶é‡æ’åçš„æœç´¢ç»“æœ
        """
        await self.initialize()
        
        try:
            logger.info(f"å¼€å§‹å¹¶å‘æœç´¢ {len(keywords)} ä¸ªå…³é”®è¯: {keywords}")
            
            # åˆ›å»ºæ¯ä¸ªå…³é”®è¯çš„æœç´¢ä»»åŠ¡
            search_tasks = []
            for keyword in keywords:
                task = self._create_keyword_search_task(
                    keyword=keyword,
                    region=region,
                    search_strategy=search_strategy,
                    enable_intelligent_crawling=enable_intelligent_crawling
                )
                search_tasks.append(task)
            
            # åˆ›å»ºç»“æœèšåˆå™¨
            aggregator = self.result_aggregator.create_keyword_specific_aggregator(keywords)
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æœç´¢ä»»åŠ¡
            execution_result = await self.execution_engine.execute_concurrent_tasks(
                tasks=search_tasks,
                task_type=TaskType.IO_BOUND,
                aggregator=aggregator,
                max_results=max_results
            )
            
            # æå–èšåˆåçš„ç»“æœ
            aggregated_results = execution_result.aggregated_data or []
            
            # è®°å½•æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯
            logger.info(
                f"å¤šå…³é”®è¯å¹¶å‘æœç´¢å®Œæˆ: "
                f"æˆåŠŸä»»åŠ¡: {execution_result.success_count}, "
                f"å¤±è´¥ä»»åŠ¡: {execution_result.failure_count}, "
                f"æ€»æ‰§è¡Œæ—¶é—´: {execution_result.total_execution_time:.2f}s, "
                f"æœ€ç»ˆç»“æœæ•°: {len(aggregated_results)}"
            )
            
            return aggregated_results[:max_results]
            
        except Exception as e:
            logger.error(f"å¤šå…³é”®è¯å¹¶å‘æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _create_keyword_search_task(
        self,
        keyword: str,
        region: str,
        search_strategy: str,
        enable_intelligent_crawling: bool
    ) -> Callable:
        """åˆ›å»ºå•ä¸ªå…³é”®è¯çš„æœç´¢ä»»åŠ¡"""
        async def search_task() -> List[SearchResult]:
            try:
                # æ‰§è¡Œå•å…³é”®è¯æœç´¢
                policy_results = await self._search_single_keyword(
                    keyword, region, search_strategy, enable_intelligent_crawling
                )
                
                # è½¬æ¢ä¸ºæ ‡å‡†åŒ–æœç´¢ç»“æœ
                search_results = self._convert_to_search_results(policy_results)
                
                # ä¸ºæ¯ä¸ªç»“æœæ ‡è®°å…³é”®è¯
                for result in search_results:
                    result.keywords_matched = [keyword]
                
                logger.info(f"å…³é”®è¯ '{keyword}' æœç´¢å®Œæˆï¼Œè·å¾— {len(search_results)} ä¸ªç»“æœ")
                return search_results
                
            except Exception as e:
                logger.error(f"å…³é”®è¯ '{keyword}' æœç´¢å¤±è´¥: {str(e)}")
                return []
        
        return search_task
    
    async def _search_single_keyword(
        self,
        keyword: str,
        region: str,
        search_strategy: str,
        enable_intelligent_crawling: bool
    ) -> List[PolicySearchResult]:
        """æœç´¢å•ä¸ªå…³é”®è¯"""
        try:
            all_results = []
            
            # æ ¹æ®ç­–ç•¥æ‰§è¡Œæœç´¢
            if search_strategy in ["auto", "local_only"]:
                # å°è¯•åœ°æ–¹é—¨æˆ·æœç´¢
                if region in self.default_portals:
                    portal = self.default_portals[region]
                    results = await self._search_portal_async(
                        portal, keyword, SearchLevel.LOCAL, enable_intelligent_crawling
                    )
                    all_results.extend(results)
            
            if search_strategy in ["auto", "provincial_only"]:
                # å°è¯•çœçº§é—¨æˆ·æœç´¢
                province = region.split("å¸‚")[0] if "å¸‚" in region else region
                if province in self.default_portals and province != region:
                    portal = self.default_portals[province]
                    results = await self._search_portal_async(
                        portal, keyword, SearchLevel.PROVINCIAL, enable_intelligent_crawling
                    )
                    all_results.extend(results)
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            all_results.sort(key=lambda x: x.relevance_score, reverse=True)
            
            return all_results
            
        except Exception as e:
            logger.error(f"å•å…³é”®è¯æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def _search_portal_async(
        self,
        portal: PortalConfig,
        query: str,
        search_level: SearchLevel,
        enable_intelligent_crawling: bool = True
    ) -> List[PolicySearchResult]:
        """å¼‚æ­¥æœç´¢æŒ‡å®šé—¨æˆ·"""
        try:
            if not self.execution_engine or not self.execution_engine.session:
                await self.initialize()
            
            session = self.execution_engine.session
            
            # æ„å»ºæœç´¢URL
            search_params = {}
            for key, value in portal.search_params.items():
                if "{query}" in value:
                    search_params[key] = value.format(query=query)
                else:
                    search_params[key] = value
            
            search_url = urljoin(portal.base_url, portal.search_endpoint)
            if search_params:
                search_url += "?" + urlencode(search_params)
            
            logger.debug(f"æœç´¢URL: {search_url}")
            
            # å‘é€å¼‚æ­¥HTTPè¯·æ±‚
            async with session.get(search_url) as response:
                if response.status == 200:
                    content = await response.text(encoding=getattr(portal, 'encoding', 'utf-8'))
                    
                    # è§£ææœç´¢ç»“æœ
                    results = await self._parse_portal_results_async(
                        content, portal, search_level, query
                    )
                    
                    return results
                else:
                    logger.warning(f"é—¨æˆ·æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"å¼‚æ­¥é—¨æˆ·æœç´¢å‡ºé”™: {str(e)}")
            return []
    
    async def _parse_portal_results_async(
        self,
        html_content: str,
        portal: PortalConfig,
        search_level: SearchLevel,
        query: str
    ) -> List[PolicySearchResult]:
        """å¼‚æ­¥è§£æé—¨æˆ·æœç´¢ç»“æœ"""
        import re
        
        results = []
        
        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_pattern = r'<h3[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></h3>'
            matches = re.findall(title_pattern, html_content, re.IGNORECASE | re.DOTALL)
            
            # æå–æ‘˜è¦
            summary_pattern = r'<p[^>]*class="[^"]*summary[^"]*"[^>]*>([^<]*)</p>'
            summaries = re.findall(summary_pattern, html_content, re.IGNORECASE | re.DOTALL)
            
            # æå–æ—¥æœŸ
            date_pattern = r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            dates = re.findall(date_pattern, html_content)
            
            for i, (url, title) in enumerate(matches[:portal.max_results]):
                # æ¸…ç†æ ‡é¢˜
                title = re.sub(r'<[^>]+>', '', title).strip()
                
                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                if not url.startswith('http'):
                    url = urljoin(portal.base_url, url)
                
                # è·å–æ‘˜è¦
                summary = ""
                if i < len(summaries):
                    summary = re.sub(r'<[^>]+>', '', summaries[i]).strip()
                
                # è·å–æ—¥æœŸ
                published_date = None
                if i < len(dates):
                    published_date = dates[i]
                
                # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
                relevance_score = self._calculate_relevance(title + " " + summary, query)
                
                # è¯†åˆ«æ”¿ç­–ç±»å‹
                policy_type = self._identify_policy_type(title)
                
                result = PolicySearchResult(
                    title=title,
                    url=url,
                    content=summary,
                    published_date=published_date,
                    source=portal.name,
                    search_level=search_level,
                    relevance_score=relevance_score,
                    policy_type=policy_type,
                    extraction_method="async_traditional"
                )
                
                results.append(result)
            
            logger.debug(f"ä» {portal.name} å¼‚æ­¥è§£æåˆ° {len(results)} ä¸ªç»“æœ")
            
        except Exception as e:
            logger.error(f"å¼‚æ­¥è§£æé—¨æˆ·æœç´¢ç»“æœå¤±è´¥: {str(e)}")
        
        return results
    
    def _convert_to_search_results(self, policy_results: List[PolicySearchResult]) -> List[SearchResult]:
        """å°†æ”¿ç­–æœç´¢ç»“æœè½¬æ¢ä¸ºæ ‡å‡†åŒ–æœç´¢ç»“æœ"""
        search_results = []
        
        for policy_result in policy_results:
            search_result = SearchResult(
                title=policy_result.title,
                url=policy_result.url,
                content=policy_result.content,
                source=policy_result.source,
                published_date=policy_result.published_date,
                relevance_score=policy_result.relevance_score,
                quality_score=getattr(policy_result, 'content_quality_score', 0.0),
                metadata={
                    "search_level": policy_result.search_level.value,
                    "policy_type": policy_result.policy_type,
                    "extraction_method": policy_result.extraction_method
                }
            )
            search_results.append(search_result)
        
        return search_results
    
    def _calculate_relevance(self, text: str, query: str) -> float:
        """è®¡ç®—æ–‡æœ¬ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        try:
            text_lower = text.lower()
            query_lower = query.lower()
            
            words = query_lower.split()
            matches = 0
            total_words = len(words)
            
            for word in words:
                if word in text_lower:
                    matches += 1
            
            base_score = matches / total_words if total_words > 0 else 0
            
            # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
            if query_lower in text_lower[:100]:
                base_score += 0.2
            
            return min(base_score, 1.0)
        except:
            return 0.0
    
    def _identify_policy_type(self, title: str) -> Optional[str]:
        """æ ¹æ®æ ‡é¢˜è¯†åˆ«æ”¿ç­–ç±»å‹"""
        try:
            title_lower = title.lower()
            
            policy_types = {
                "é€šçŸ¥": ["é€šçŸ¥", "å…¬å‘Š", "å…¬ç¤º"],
                "åŠæ³•": ["åŠæ³•", "è§„å®š", "ç»†åˆ™"],
                "æ„è§": ["æ„è§", "å»ºè®®", "æ–¹æ¡ˆ"],
                "æ³•è§„": ["æ¡ä¾‹", "æ³•è§„", "æ³•å¾‹"],
                "æ ‡å‡†": ["æ ‡å‡†", "è§„èŒƒ", "æŒ‡å—"]
            }
            
            for policy_type, keywords in policy_types.items():
                for keyword in keywords:
                    if keyword in title_lower:
                        return policy_type
            
            return None
        except:
            return None
    
    def _extract_keywords_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯"""
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶åˆ†è¯
        clean_query = re.sub(r'[^\w\s]', ' ', query)
        words = clean_query.split()
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {"çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "å…³äº", "æˆ‘"}
        keywords = [word for word in words if len(word) > 1 and word not in stop_words]
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
        if not keywords:
            keywords = [query]
        
        return keywords[:5]  # é™åˆ¶æœ€å¤š5ä¸ªå…³é”®è¯
    
    async def _arun(
        self,
        query: str,
        region: str = "å…­ç›˜æ°´",
        search_strategy: str = "auto",
        max_results: int = 10,
        enable_intelligent_crawling: bool = True,
        enable_multi_keyword: bool = True
    ) -> str:
        """å¼‚æ­¥è¿è¡Œæ”¿ç­–æœç´¢"""
        try:
            logger.info(f"å¼€å§‹å¼‚æ­¥æ”¿ç­–æ£€ç´¢: query={query}, region={region}, strategy={search_strategy}")
            
            # æå–å…³é”®è¯
            keywords = self._extract_keywords_from_query(query)
            
            # å¦‚æœå¯ç”¨å¤šå…³é”®è¯ä¸”æœ‰å¤šä¸ªå…³é”®è¯ï¼Œä½¿ç”¨å¹¶å‘æœç´¢
            if enable_multi_keyword and len(keywords) > 1:
                search_results = await self.search_multi_keywords_concurrent(
                    keywords=keywords,
                    region=region,
                    search_strategy=search_strategy,
                    max_results=max_results,
                    enable_intelligent_crawling=enable_intelligent_crawling
                )
            else:
                # å•å…³é”®è¯æœç´¢
                main_keyword = keywords[0] if keywords else query
                policy_results = await self._search_single_keyword(
                    main_keyword, region, search_strategy, enable_intelligent_crawling
                )
                search_results = self._convert_to_search_results(policy_results)[:max_results]
            
            # æ ¼å¼åŒ–è¾“å‡º
            if not search_results:
                return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³æ”¿ç­–ä¿¡æ¯ã€‚"
            
            # æ„å»ºç»“æœå­—ç¬¦ä¸²
            result_text = f"ğŸ” å¼‚æ­¥æ”¿ç­–æ£€ç´¢ç»“æœï¼ˆæ‰¾åˆ° {len(search_results)} æ¡ï¼‰\n"
            result_text += f"ğŸ¯ æ£€ç´¢å…³é”®è¯ï¼š{', '.join(keywords)}\n"
            result_text += f"ğŸ“Š æœç´¢ç­–ç•¥ï¼š{search_strategy}\n"
            result_text += f"âš¡ å¹¶å‘æ¨¡å¼ï¼š{'å·²å¯ç”¨' if enable_multi_keyword and len(keywords) > 1 else 'å•å…³é”®è¯'}\n\n"
            
            for i, result in enumerate(search_results, 1):
                result_text += f"{i}. **{result.title}**\n"
                result_text += f"   ğŸ”— é“¾æ¥ï¼š{result.url}\n"
                result_text += f"   ğŸ“… æ¥æºï¼š{result.source}\n"
                
                if result.published_date:
                    result_text += f"   ğŸ“† å‘å¸ƒæ—¥æœŸï¼š{result.published_date}\n"
                
                if result.metadata.get("policy_type"):
                    result_text += f"   ğŸ“‹ æ”¿ç­–ç±»å‹ï¼š{result.metadata['policy_type']}\n"
                
                result_text += f"   â­ ç›¸å…³åº¦ï¼š{result.relevance_score:.2f}\n"
                
                if result.keywords_matched:
                    result_text += f"   ğŸ¯ åŒ¹é…å…³é”®è¯ï¼š{', '.join(result.keywords_matched)}\n"
                
                if result.content:
                    content_preview = result.content[:200] + "..." if len(result.content) > 200 else result.content
                    result_text += f"   ğŸ“„ æ‘˜è¦ï¼š{content_preview}\n"
                
                result_text += "\n"
            
            return result_text
            
        except Exception as e:
            logger.error(f"å¼‚æ­¥æ”¿ç­–æ£€ç´¢å¤±è´¥: {str(e)}")
            return f"æ”¿ç­–æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
    
    def _run(
        self,
        query: str,
        region: str = "å…­ç›˜æ°´",
        search_strategy: str = "auto",
        max_results: int = 10,
        enable_intelligent_crawling: bool = True,
        enable_multi_keyword: bool = True
    ) -> str:
        """åŒæ­¥è¿è¡Œæ”¿ç­–æœç´¢ï¼ˆLlamaIndexå…¼å®¹ï¼‰"""
        import asyncio
        return asyncio.run(self._arun(query, region, search_strategy, max_results, enable_intelligent_crawling, enable_multi_keyword))


@register_tool(
    name="async_policy_search",
    description="å¼‚æ­¥æ”¿ç­–æ£€ç´¢å·¥å…·ï¼Œæ”¯æŒå¤šå…³é”®è¯å¹¶å‘æ£€ç´¢å’Œæ™ºèƒ½ç»“æœæ±‡æ€»é‡æ’ï¼Œæ˜¾è‘—æå‡æ£€ç´¢æ€§èƒ½å’Œå‡†ç¡®æ€§",
    category="search",
    tags=["æ”¿ç­–", "æœç´¢", "å¼‚æ­¥", "å¹¶å‘", "æ™ºèƒ½æ±‡æ€»"]
)
async def async_policy_search(
    query: str,
    region: str = "å…­ç›˜æ°´",
    search_strategy: str = "auto",
    max_results: int = 10,
    enable_intelligent_crawling: bool = True,
    enable_multi_keyword: bool = True
) -> str:
    """
    å¼‚æ­¥æ”¿ç­–æ£€ç´¢å·¥å…·å‡½æ•°
    
    å‚æ•°:
        query: æœç´¢æŸ¥è¯¢
        region: ç›®æ ‡åœ°åŒº
        search_strategy: æ£€ç´¢ç­–ç•¥ï¼ˆauto|local_only|provincial_only|search_onlyï¼‰
        max_results: æœ€å¤§ç»“æœæ•°
        enable_intelligent_crawling: æ˜¯å¦å¯ç”¨æ™ºèƒ½çˆ¬å–
        enable_multi_keyword: æ˜¯å¦å¯ç”¨å¤šå…³é”®è¯å¹¶å‘æ£€ç´¢
    """
    tool = AsyncPolicySearchTool()
    return await tool._arun(query, region, search_strategy, max_results, enable_intelligent_crawling, enable_multi_keyword)


def create_async_policy_search_function_tool() -> FunctionTool:
    """åˆ›å»ºå¼‚æ­¥æ”¿ç­–æ£€ç´¢åŠŸèƒ½å·¥å…·ï¼ˆLlamaIndexå…¼å®¹ï¼‰"""
    
    def sync_async_policy_search(
        query: str,
        region: str = "å…­ç›˜æ°´",
        search_strategy: str = "auto",
        max_results: int = 10,
        enable_intelligent_crawling: bool = True,
        enable_multi_keyword: bool = True
    ) -> str:
        """åŒæ­¥ç‰ˆæœ¬çš„å¼‚æ­¥æ”¿ç­–æ£€ç´¢å‡½æ•°"""
        import asyncio
        return asyncio.run(async_policy_search(query, region, search_strategy, max_results, enable_intelligent_crawling, enable_multi_keyword))
    
    return FunctionTool.from_defaults(
        fn=sync_async_policy_search,
        name="async_policy_search",
        description=(
            "å¼‚æ­¥æ”¿ç­–æ£€ç´¢å·¥å…·ï¼šæ”¯æŒå¤šå…³é”®è¯å¹¶å‘æ£€ç´¢å’Œæ™ºèƒ½ç»“æœæ±‡æ€»é‡æ’ã€‚"
            "é›†æˆå¼‚æ­¥æ‰§è¡Œå¼•æ“ï¼Œæ˜¾è‘—æå‡æ£€ç´¢æ€§èƒ½ã€‚ä¼˜å…ˆä»åœ°æ–¹æ”¿åºœé—¨æˆ·ç½‘ç«™æœç´¢ï¼Œ"
            "ç„¶åçœçº§é—¨æˆ·ï¼Œæœ€åä½¿ç”¨æœç´¢å¼•æ“ã€‚æ”¯æŒè‡ªåŠ¨å…³é”®è¯æå–ã€å¹¶å‘æ‰§è¡Œã€"
            "ç»“æœå»é‡ã€è´¨é‡è¯„ä¼°å’Œæ™ºèƒ½é‡æ’ã€‚é€‚ç”¨äºå¤æ‚çš„æ”¿ç­–æŸ¥è¯¢å’Œå¤šç»´åº¦æ£€ç´¢éœ€æ±‚ã€‚"
        )
    )


def get_async_policy_search_tool() -> AsyncPolicySearchTool:
    """è·å–å¼‚æ­¥æ”¿ç­–æ£€ç´¢å·¥å…·å®ä¾‹"""
    return AsyncPolicySearchTool() 