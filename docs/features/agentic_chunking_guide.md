# Agenticæ–‡æ¡£åˆ‡åˆ†å·¥å…·ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

åŸºäºAgnoæ¡†æ¶çš„Agenticæ–‡æ¡£åˆ‡åˆ†å·¥å…·æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£åˆ†æ®µç³»ç»Ÿï¼Œä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)æ¥ç¡®å®šæ–‡æ¡£çš„è‡ªç„¶è¯­ä¹‰è¾¹ç•Œï¼Œè€Œä¸æ˜¯ç®€å•åœ°æŒ‰å›ºå®šå¤§å°æˆ–è§„åˆ™è¿›è¡Œåˆ‡åˆ†ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)ç³»ç»Ÿçš„æ€§èƒ½ã€‚

## æ ¸å¿ƒç‰¹æ€§

### ğŸ§  æ™ºèƒ½è¯­ä¹‰è¾¹ç•Œè¯†åˆ«
- ä½¿ç”¨LLMåˆ†ææ–‡æ¡£å†…å®¹ï¼Œè¯†åˆ«è‡ªç„¶çš„è¯­ä¹‰æ–­ç‚¹
- ä¿æŒç›¸å…³å†…å®¹çš„å®Œæ•´æ€§å’Œè¿è´¯æ€§
- é¿å…ä»»æ„åˆ‡æ–­å¥å­ã€æ®µè½æˆ–æ¦‚å¿µ

### ğŸ“Š å¤šç§åˆ‡åˆ†ç­–ç•¥
- **è¯­ä¹‰è¾¹ç•Œåˆ‡åˆ†**: åŸºäºè¯­ä¹‰ç›¸å…³æ€§çš„æ™ºèƒ½åˆ†æ®µ
- **ä¸»é¢˜è½¬æ¢åˆ‡åˆ†**: è¯†åˆ«ä¸»é¢˜å˜åŒ–è¿›è¡Œåˆ†æ®µ
- **æ®µè½æ„ŸçŸ¥åˆ‡åˆ†**: å°Šé‡æ–‡æ¡£æ®µè½ç»“æ„
- **å¯¹è¯æµåˆ‡åˆ†**: é€‚åˆå¯¹è¯å’Œé—®ç­”å†…å®¹
- **æŠ€æœ¯æ–‡æ¡£åˆ‡åˆ†**: ä¿ç•™ä»£ç å—å’ŒæŠ€æœ¯ç»“æ„

### ğŸ¯ è´¨é‡è¯„ä¼°ç³»ç»Ÿ
- è¯­ä¹‰è¿è´¯æ€§è¯„åˆ†
- å¤§å°åˆé€‚æ€§è¯„åˆ†
- è¾¹ç•Œè‡ªç„¶æ€§è¯„åˆ†
- ç»“æ„ä¿ç•™æ€§è¯„åˆ†

### âš™ï¸ çµæ´»é…ç½®é€‰é¡¹
- å¯è°ƒèŠ‚çš„å—å¤§å°èŒƒå›´
- å¯é…ç½®çš„é‡å ç­–ç•¥
- æ”¯æŒå¤šç§è¯­è¨€
- è‡ªå®šä¹‰è´¨é‡é˜ˆå€¼

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from app.tools.advanced.agentic_chunking import (
    AgenticDocumentChunker,
    AgenticChunkingConfig,
    AgenticChunkingStrategy
)

# åˆ›å»ºé…ç½®
config = AgenticChunkingConfig(
    strategy=AgenticChunkingStrategy.SEMANTIC_BOUNDARY,
    max_chunk_size=4000,
    min_chunk_size=200,
    language="zh"
)

# åˆ›å»ºåˆ‡åˆ†å™¨
chunker = AgenticDocumentChunker(config)

# æ‰§è¡Œåˆ‡åˆ†
result = await chunker.chunk_document(document_content)

print(f"ç”Ÿæˆ {result.total_chunks} ä¸ªæ–‡æ¡£å—")
for chunk in result.chunks:
    print(f"å—å¤§å°: {len(chunk.content)} å­—ç¬¦")
```

### ä¾¿åˆ©å‡½æ•°ä½¿ç”¨

```python
from app.tools.advanced.agentic_chunking import agentic_chunk_text

# ç®€å•æ–‡æœ¬åˆ‡åˆ†
result = await agentic_chunk_text(
    content="æ‚¨çš„æ–‡æ¡£å†…å®¹...",
    strategy=AgenticChunkingStrategy.TOPIC_TRANSITION,
    max_chunk_size=3000
)
```

### æ™ºèƒ½è‡ªåŠ¨åˆ‡åˆ†

```python
from app.tools.advanced.agentic_chunking_integration import (
    smart_chunk_text,
    get_chunking_recommendations
)

# è·å–åˆ‡åˆ†å»ºè®®
recommendations = get_chunking_recommendations(content_sample)
print(f"æ¨èå·¥å…·: {recommendations['recommended_tool']}")

# æ™ºèƒ½è‡ªåŠ¨åˆ‡åˆ†
result = await smart_chunk_text(
    content="æ‚¨çš„æ–‡æ¡£å†…å®¹...",
    content_type="technical_document"
)
```

## åˆ‡åˆ†ç­–ç•¥è¯¦è§£

### 1. è¯­ä¹‰è¾¹ç•Œåˆ‡åˆ† (SEMANTIC_BOUNDARY)
**é€‚ç”¨åœºæ™¯**: é€šç”¨æ–‡æ¡£ã€æ–°é—»æ–‡ç« ã€åšå®¢æ–‡ç« 

**ç‰¹ç‚¹**:
- åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œåˆ‡åˆ†
- ä¿æŒç›¸å…³æ¦‚å¿µçš„å®Œæ•´æ€§
- é€‚åˆå¤§å¤šæ•°æ–‡æ¡£ç±»å‹

**é…ç½®å»ºè®®**:
```python
config = AgenticChunkingConfig(
    strategy=AgenticChunkingStrategy.SEMANTIC_BOUNDARY,
    max_chunk_size=4000,
    semantic_threshold=0.75,
    quality_threshold=0.8
)
```

### 2. ä¸»é¢˜è½¬æ¢åˆ‡åˆ† (TOPIC_TRANSITION)
**é€‚ç”¨åœºæ™¯**: å­¦æœ¯è®ºæ–‡ã€ç ”ç©¶æŠ¥å‘Šã€å¤šä¸»é¢˜æ–‡æ¡£

**ç‰¹ç‚¹**:
- è¯†åˆ«ä¸»é¢˜å˜åŒ–çš„ä¿¡å·
- åœ¨ä¸»é¢˜è¾¹ç•Œå¤„è¿›è¡Œåˆ‡åˆ†
- ä¿æŒæ¯ä¸ªä¸»é¢˜çš„å®Œæ•´æ€§

**é…ç½®å»ºè®®**:
```python
config = AgenticChunkingConfig(
    strategy=AgenticChunkingStrategy.TOPIC_TRANSITION,
    max_chunk_size=5000,
    topic_coherence_weight=0.8
)
```

### 3. æ®µè½æ„ŸçŸ¥åˆ‡åˆ† (PARAGRAPH_AWARE)
**é€‚ç”¨åœºæ™¯**: ç»“æ„åŒ–æ–‡æ¡£ã€æ•™ç§‘ä¹¦ã€æ‰‹å†Œ

**ç‰¹ç‚¹**:
- å°Šé‡æ–‡æ¡£çš„æ®µè½ç»“æ„
- é¿å…åœ¨æ®µè½ä¸­é—´åˆ‡åˆ†
- ä¿æŒæ ¼å¼å’Œç»“æ„çš„å®Œæ•´æ€§

**é…ç½®å»ºè®®**:
```python
config = AgenticChunkingConfig(
    strategy=AgenticChunkingStrategy.PARAGRAPH_AWARE,
    max_chunk_size=3500,
    preserve_structure=True,
    structure_preservation_weight=0.7
)
```

### 4. å¯¹è¯æµåˆ‡åˆ† (CONVERSATION_FLOW)
**é€‚ç”¨åœºæ™¯**: å®¢æœå¯¹è¯ã€é—®ç­”è®°å½•ã€èŠå¤©è®°å½•

**ç‰¹ç‚¹**:
- ä¿æŒé—®ç­”å¯¹çš„å®Œæ•´æ€§
- æŒ‰å¯¹è¯è½®æ¬¡åˆ†ç»„
- é€‚åˆäº¤äº’å¼å†…å®¹

**é…ç½®å»ºè®®**:
```python
config = AgenticChunkingConfig(
    strategy=AgenticChunkingStrategy.CONVERSATION_FLOW,
    max_chunk_size=2500,
    min_chunk_size=100
)
```

### 5. æŠ€æœ¯æ–‡æ¡£åˆ‡åˆ† (TECHNICAL_DOCUMENT)
**é€‚ç”¨åœºæ™¯**: APIæ–‡æ¡£ã€æŠ€æœ¯è§„èŒƒã€ä»£ç æ–‡æ¡£

**ç‰¹ç‚¹**:
- ä¿ç•™ä»£ç å—çš„å®Œæ•´æ€§
- ä¿æŒæŠ€æœ¯æ¦‚å¿µçš„è¿è´¯æ€§
- è¯†åˆ«æŠ€æœ¯ç»“æ„è¾¹ç•Œ

**é…ç½®å»ºè®®**:
```python
config = AgenticChunkingConfig(
    strategy=AgenticChunkingStrategy.TECHNICAL_DOCUMENT,
    max_chunk_size=6000,
    preserve_structure=True,
    quality_threshold=0.85
)
```

## å·¥å…·ç®¡ç†å™¨ä½¿ç”¨

### è·å–å·¥å…·ç®¡ç†å™¨

```python
from app.tools.advanced.agentic_chunking_integration import (
    get_agentic_chunking_manager
)

manager = get_agentic_chunking_manager()
```

### æŸ¥çœ‹å¯ç”¨å·¥å…·

```python
tools = manager.get_available_tools()
for tool_name, tool_info in tools.items():
    print(f"å·¥å…·: {tool_name}")
    print(f"æè¿°: {tool_info['description']}")
    print(f"é€‚ç”¨åœºæ™¯: {tool_info['use_cases']}")
```

### æ‰¹é‡å¤„ç†æ–‡æ¡£

```python
documents = [
    {
        "content": "æ–‡æ¡£å†…å®¹1...",
        "content_type": "technical",
        "metadata": {"source": "doc1"}
    },
    {
        "content": "æ–‡æ¡£å†…å®¹2...",
        "content_type": "academic",
        "metadata": {"source": "doc2"}
    }
]

results = await manager.batch_chunk_documents(
    documents,
    auto_select=True,  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
    max_workers=5      # å¹¶å‘å¤„ç†æ•°é‡
)
```

## çŸ¥è¯†åº“é›†æˆ

### åˆ‡åˆ†çŸ¥è¯†åº“æ–‡æ¡£

```python
from app.tools.advanced.agentic_chunking_integration import (
    smart_chunk_knowledge_base
)

result = await smart_chunk_knowledge_base(
    kb_id="your_knowledge_base_id",
    auto_select=True,
    batch_size=10
)

print(f"å¤„ç†çŠ¶æ€: {result['status']}")
print(f"æˆåŠŸæ–‡æ¡£: {result['successful_documents']}")
print(f"æ€»å—æ•°: {result['total_chunks']}")
```

## è´¨é‡è¯„ä¼°

### ç†è§£è´¨é‡æŒ‡æ ‡

1. **è¯­ä¹‰è¿è´¯æ€§ (semantic_coherence)**
   - è¯„ä¼°å—å†…å®¹çš„è¯­ä¹‰è¿è´¯æ€§
   - æ£€æŸ¥å¥å­å®Œæ•´æ€§å’Œé€»è¾‘è¿æ¥

2. **å¤§å°åˆé€‚æ€§ (size_appropriateness)**
   - è¯„ä¼°å—å¤§å°æ˜¯å¦åœ¨ç†æƒ³èŒƒå›´å†…
   - é¿å…è¿‡å¤§æˆ–è¿‡å°çš„å—

3. **è¾¹ç•Œè‡ªç„¶æ€§ (boundary_naturalness)**
   - è¯„ä¼°åˆ‡åˆ†è¾¹ç•Œæ˜¯å¦è‡ªç„¶
   - æ£€æŸ¥å¼€å¤´å’Œç»“å°¾çš„å®Œæ•´æ€§

4. **ç»“æ„ä¿ç•™æ€§ (structure_preservation)**
   - è¯„ä¼°æ˜¯å¦ä¿ç•™äº†æ–‡æ¡£ç»“æ„
   - æ£€æŸ¥æ®µè½å’Œæ ¼å¼çš„å®Œæ•´æ€§

### è´¨é‡åˆ†æç¤ºä¾‹

```python
result = await chunker.chunk_document(content)

for i, chunk in enumerate(result.chunks):
    quality = result.processing_metadata["quality_scores"][i]
    print(f"å— {i+1} è´¨é‡åˆ†æ:")
    print(f"  è¯­ä¹‰è¿è´¯æ€§: {quality['semantic_coherence']:.3f}")
    print(f"  å¤§å°åˆé€‚æ€§: {quality['size_appropriateness']:.3f}")
    print(f"  è¾¹ç•Œè‡ªç„¶æ€§: {quality['boundary_naturalness']:.3f}")
    print(f"  ç»“æ„ä¿ç•™æ€§: {quality['structure_preservation']:.3f}")
    print(f"  æ€»ä½“å¾—åˆ†: {quality['overall_score']:.3f}")
```

## è‡ªå®šä¹‰é…ç½®

### åˆ›å»ºè‡ªå®šä¹‰é…ç½®æ–‡ä»¶

```python
custom_config = AgenticChunkingConfig(
    strategy=AgenticChunkingStrategy.SEMANTIC_BOUNDARY,
    max_chunk_size=2000,    # è‡ªå®šä¹‰æœ€å¤§å—å¤§å°
    min_chunk_size=500,     # è‡ªå®šä¹‰æœ€å°å—å¤§å°
    chunk_overlap=150,      # å—é‡å å¤§å°
    llm_model="gpt-4",      # æŒ‡å®šLLMæ¨¡å‹
    language="zh",          # æ–‡æ¡£è¯­è¨€
    semantic_threshold=0.8, # è¯­ä¹‰é˜ˆå€¼
    quality_threshold=0.85  # è´¨é‡é˜ˆå€¼
)
```

### æ³¨å†Œè‡ªå®šä¹‰é…ç½®æ–‡ä»¶

```python
manager = get_agentic_chunking_manager()

custom_profile_name = manager.create_custom_profile(
    name="custom_academic",
    description="è‡ªå®šä¹‰å­¦æœ¯æ–‡æ¡£åˆ‡åˆ†é…ç½®",
    config=custom_config,
    use_cases=["å­¦æœ¯è®ºæ–‡", "ç ”ç©¶æŠ¥å‘Š"],
    recommended_for=["ç§‘ç ”æ–‡æ¡£", "å­¦ä½è®ºæ–‡"]
)
```

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„ç­–ç•¥
- **é€šç”¨æ–‡æ¡£**: ä½¿ç”¨ `SEMANTIC_BOUNDARY`
- **æŠ€æœ¯æ–‡æ¡£**: ä½¿ç”¨ `TECHNICAL_DOCUMENT`
- **å¯¹è¯è®°å½•**: ä½¿ç”¨ `CONVERSATION_FLOW`
- **å­¦æœ¯è®ºæ–‡**: ä½¿ç”¨ `TOPIC_TRANSITION`
- **ç»“æ„åŒ–æ–‡æ¡£**: ä½¿ç”¨ `PARAGRAPH_AWARE`

### 2. ä¼˜åŒ–é…ç½®å‚æ•°
- **å—å¤§å°**: æ ¹æ®æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£è°ƒæ•´
- **é‡å å¤§å°**: ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼Œé€šå¸¸è®¾ç½®ä¸ºå—å¤§å°çš„10-20%
- **è´¨é‡é˜ˆå€¼**: æ ¹æ®åº”ç”¨è¦æ±‚è°ƒæ•´ï¼Œé«˜è´¨é‡åº”ç”¨è®¾ç½®ä¸º0.8ä»¥ä¸Š

### 3. æ‰¹é‡å¤„ç†å»ºè®®
- ä½¿ç”¨åˆé€‚çš„å¹¶å‘æ•°é‡ï¼ˆæ¨è3-5ä¸ªworkersï¼‰
- å¯¹ç›¸ä¼¼ç±»å‹çš„æ–‡æ¡£è¿›è¡Œæ‰¹æ¬¡å¤„ç†
- ç›‘æ§å¤„ç†è¿›åº¦å’Œè´¨é‡æŒ‡æ ‡

### 4. é”™è¯¯å¤„ç†
- è®¾ç½®å›é€€ç­–ç•¥
- ç›‘æ§LLMå¯ç”¨æ€§
- è®°å½•å¤„ç†æ—¥å¿—

## æ€§èƒ½ç›‘æ§

### è·å–å¤„ç†ç»Ÿè®¡

```python
stats = manager.get_stats()
print(f"å¤„ç†æ–‡æ¡£æ€»æ•°: {stats['total_documents_processed']}")
print(f"æˆåŠŸå¤„ç†: {stats['successful_processing']}")
print(f"å¤±è´¥å¤„ç†: {stats['failed_processing']}")
print(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['average_processing_time']:.2f}ç§’")
print(f"ç”Ÿæˆå—æ€»æ•°: {stats['total_chunks_generated']}")
```

### é‡ç½®ç»Ÿè®¡ä¿¡æ¯

```python
manager.reset_stats()
```

## è¿è¡Œæ¼”ç¤º

æ‰§è¡Œæ¼”ç¤ºè„šæœ¬æ¥ä½“éªŒåŠŸèƒ½ï¼š

```bash
cd /Users/wxn/Desktop/ZZDSJ/zzdsj-backend-api
python scripts/demo/agentic_chunking_demo.py
```

æ¼”ç¤ºåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
1. åŸºç¡€åˆ‡åˆ†åŠŸèƒ½å±•ç¤º
2. ä¸åŒç­–ç•¥å¯¹æ¯”
3. æ™ºèƒ½è‡ªåŠ¨é€‰æ‹©
4. è‡ªå®šä¹‰é…ç½®æ¼”ç¤º
5. æ‰¹é‡å¤„ç†æ¼”ç¤º
6. è´¨é‡åˆ†ææ¼”ç¤º

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„LLMæ¨¡å‹ï¼Ÿ
A: æ¨èä½¿ç”¨ `gpt-4o-mini` ä½œä¸ºé»˜è®¤é€‰æ‹©ï¼Œå¹³è¡¡äº†æ€§èƒ½å’Œæˆæœ¬ã€‚å¯¹äºé«˜è´¨é‡è¦æ±‚å¯ä»¥ä½¿ç”¨ `gpt-4`ã€‚

### Q: åˆ‡åˆ†æ•ˆæœä¸ç†æƒ³æ€ä¹ˆåŠï¼Ÿ
A: 
1. è°ƒæ•´ç­–ç•¥ç±»å‹
2. ä¿®æ”¹å—å¤§å°å‚æ•°
3. æé«˜è´¨é‡é˜ˆå€¼
4. æ£€æŸ¥æ–‡æ¡£é¢„å¤„ç†

### Q: å¦‚ä½•å¤„ç†å¤šè¯­è¨€æ–‡æ¡£ï¼Ÿ
A: åœ¨é…ç½®ä¸­è®¾ç½®æ­£ç¡®çš„è¯­è¨€å‚æ•°ï¼Œç³»ç»Ÿä¼šç›¸åº”è°ƒæ•´å¤„ç†ç­–ç•¥ã€‚

### Q: æ‰¹é‡å¤„ç†æ—¶å¦‚ä½•ä¼˜åŒ–æ€§èƒ½ï¼Ÿ
A: 
1. ä½¿ç”¨åˆé€‚çš„å¹¶å‘æ•°é‡
2. æŒ‰æ–‡æ¡£ç±»å‹åˆ†ç»„å¤„ç†
3. å¯ç”¨ç¼“å­˜æœºåˆ¶
4. ç›‘æ§èµ„æºä½¿ç”¨

## è”ç³»æ”¯æŒ

å¦‚æœæ‚¨åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- é¡¹ç›®Issue: GitHub Issues
- æ–‡æ¡£æ›´æ–°: æäº¤PRåˆ°é¡¹ç›®ä»“åº“
- æŠ€æœ¯è®¨è®º: é¡¹ç›®DiscussionåŒºåŸŸ 