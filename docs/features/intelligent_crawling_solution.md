# æ™ºèƒ½åŒ–è¯­ä¹‰åŒ–ç½‘é¡µçˆ¬å–è§£å†³æ–¹æ¡ˆ

## æ¦‚è¿°

æœ¬é¡¹ç›®æä¾›äº†ä¸¤å¥—äº’è¡¥çš„æ™ºèƒ½åŒ–ç½‘é¡µçˆ¬å–å·¥å…·ï¼Œä¸“é—¨é’ˆå¯¹æ£€ç´¢ç»“æœé¡µé¢çš„è‡ªåŠ¨è§£æå’Œæ™ºèƒ½åˆ†æï¼š

1. **Crawl4AI + LlamaIndex**: é«˜æ€§èƒ½æ‰¹é‡çˆ¬å–ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®å¤„ç†
2. **Browser Use + LlamaIndex**: æ™ºèƒ½äº¤äº’å¼çˆ¬å–ï¼Œé€‚åˆå¤æ‚é¡µé¢å’Œè¡¨å•å¤„ç†

## ğŸ“‹ åŠŸèƒ½ç‰¹æ€§å¯¹æ¯”

### Crawl4AIæ–¹æ¡ˆç‰¹ç‚¹

#### ğŸš€ æ€§èƒ½ä¼˜åŠ¿
- **é«˜å¹¶å‘å¤„ç†**: æ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œçˆ¬å–
- **AIä¼˜åŒ–ç­–ç•¥**: ä¸“ä¸ºLLMåº”ç”¨è®¾è®¡çš„æ™ºèƒ½æå–
- **å¼‚æ­¥æ¶æ„**: åŸºäºasyncioçš„é«˜æ€§èƒ½æ¡†æ¶
- **ç¼“å­˜æœºåˆ¶**: æ™ºèƒ½ç¼“å­˜å‡å°‘é‡å¤è¯·æ±‚

#### ğŸ¯ æ ¸å¿ƒå·¥å…·
1. **é«˜çº§é¡µé¢æ™ºèƒ½åˆ†æ** (`advanced_page_intelligence`)
   - AIé©±åŠ¨çš„é¡µé¢ç»“æ„è¯†åˆ«
   - å¤šå±‚æ¬¡è¯­ä¹‰åˆ†æï¼ˆåŸºç¡€/æ ‡å‡†/ç»¼åˆï¼‰
   - æ”¯æŒå¤šç§å†…å®¹ç±»å‹æå–
   - æ·±åº¦å†…å®¹è´¨é‡è¯„ä¼°

2. **ç»“æ„åŒ–å†…å®¹æŒ–æ˜** (`structural_content_mining`)
   - CSSé€‰æ‹©å™¨ + LLMè¯­ä¹‰ç»“åˆ
   - é’ˆå¯¹ç‰¹å®šç»“æ„ç±»å‹çš„ç²¾ç¡®æå–
   - è‡ªåŠ¨å±•å¼€æŠ˜å å†…å®¹
   - ç»“æ„åŒ–æ•°æ®ç»„ç»‡

3. **æ‰¹é‡æ™ºèƒ½çˆ¬å–** (`batch_intelligent_crawling`)
   - å¹¶è¡Œ/é¡ºåºçˆ¬å–ç­–ç•¥
   - ç»Ÿä¸€è·¨ç«™ç‚¹åˆ†æ
   - è‡ªåŠ¨å¤±è´¥å¤„ç†å’Œé‡è¯•
   - æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡

4. **åŠ¨æ€å†…å®¹åˆ†æ** (`dynamic_content_analysis`)
   - JavaScriptäº¤äº’æ‰§è¡Œ
   - æ»šåŠ¨ã€ç‚¹å‡»ã€ç­‰å¾…ç­–ç•¥
   - å¼‚æ­¥å†…å®¹è¯†åˆ«
   - åŠ¨æ€æ¨¡å¼åˆ†æ

5. **è¯­ä¹‰æœç´¢æå–** (`semantic_search_extraction`)
   - åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„ç²¾ç¡®æœç´¢
   - å¤šæŸ¥è¯¢å¹¶è¡Œå¤„ç†
   - ç½®ä¿¡åº¦è¯„ä¼°
   - ç›¸å…³æ€§è¯„åˆ†

### Browser Useæ–¹æ¡ˆç‰¹ç‚¹

#### ğŸ­ äº¤äº’ä¼˜åŠ¿
- **è§†è§‰ç†è§£**: ç»“åˆHTMLå’Œè§†è§‰è¯†åˆ«
- **äººç±»è¡Œä¸ºæ¨¡æ‹Ÿ**: è‡ªç„¶çš„æµè§ˆå™¨äº¤äº’
- **è‡ªåŠ¨çº é”™**: æ™ºèƒ½é”™è¯¯å¤„ç†å’Œæ¢å¤
- **å¤šæ ‡ç­¾ç®¡ç†**: å¤æ‚å·¥ä½œæµæ”¯æŒ

#### ğŸ¯ æ ¸å¿ƒå·¥å…·
1. **æ™ºèƒ½é¡µé¢åˆ†æ** (`intelligent_page_analysis`)
   - è§†è§‰+æ–‡æœ¬åŒé‡ç†è§£
   - è‡ªåŠ¨é¡µé¢ç»“æ„è¯†åˆ«
   - æ·±åº¦è¯­ä¹‰åˆ†æ
   - å®ä½“æå–å’Œå…³ç³»åˆ†æ

2. **æ™ºèƒ½å†…å®¹æå–** (`smart_content_extraction`)
   - è‡ªç„¶è¯­è¨€è§„åˆ™æè¿°
   - è‡ªé€‚åº”å†…å®¹å®šä½
   - å¤šæ ¼å¼è¾“å‡ºæ”¯æŒ
   - æå–ç½®ä¿¡åº¦è¯„ä¼°

3. **å¤šé¡µé¢æ™ºèƒ½åˆ†æ** (`multi_page_intelligence`)
   - è‡ªåŠ¨å¯¼èˆªç­–ç•¥
   - è·¨é¡µé¢å…³è”åˆ†æ
   - å†…å®¹ç›¸ä¼¼æ€§æ£€æµ‹
   - ä¿¡æ¯æµåŠ¨è¿½è¸ª

4. **è‡ªé€‚åº”è¡¨å•äº¤äº’** (`adaptive_form_interaction`)
   - æ™ºèƒ½è¡¨å•è¯†åˆ«
   - è‡ªåŠ¨å­—æ®µå¡«å†™
   - äº¤äº’æ•ˆæœåˆ†æ
   - ç»“æœéªŒè¯

## ğŸ› ï¸ å®‰è£…å’Œé…ç½®

### ä¾èµ–å®‰è£…

```bash
# Crawl4AIç›¸å…³ä¾èµ–
pip install crawl4ai
pip install llama-index
pip install llama-index-llms-openai
pip install llama-index-embeddings-openai

# Browser Useç›¸å…³ä¾èµ–
pip install browser-use
pip install playwright
playwright install

# å¯é€‰ï¼šå…¶ä»–LLMæ”¯æŒ
pip install llama-index-llms-anthropic
pip install llama-index-llms-ollama
```

### ç¯å¢ƒé…ç½®

```bash
# è®¾ç½®APIå¯†é’¥
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"  # å¯é€‰

# Browser Useé…ç½®
export BROWSER_USE_HEADLESS=true  # æ— å¤´æ¨¡å¼
export BROWSER_USE_TIMEOUT=30000  # è¶…æ—¶è®¾ç½®
```

### MCPæœåŠ¡é…ç½®

```yaml
# config/mcp_services.yaml
mcp_services:
  crawl4ai_intelligence:
    provider: "crawl4ai"
    description: "Crawl4AIæ™ºèƒ½è§£ææœåŠ¡"
    model_config:
      provider: "openai"
      name: "gpt-4o"
      temperature: 0.1
    capabilities:
      - tools
      - resources
    
  browser_use_intelligence:
    provider: "browser_use"
    description: "Browser Useæ™ºèƒ½æµè§ˆæœåŠ¡"
    model_config:
      provider: "openai"
      name: "gpt-4o"
      temperature: 0.3
    capabilities:
      - tools
      - resources
      - chat
```

## ğŸš¦ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

```python
import asyncio
from app.frameworks.fastmcp.integrations.providers.crawl4ai_llamaindex import Crawl4AILlamaIndexClient
from app.frameworks.fastmcp.integrations.providers.browser_use_llamaindex import BrowserUseLlamaIndexClient

async def quick_start():
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    crawl4ai = Crawl4AILlamaIndexClient(service_config, api_key)
    browser_use = BrowserUseLlamaIndexClient(service_config, api_key)
    
    # åŸºç¡€é¡µé¢åˆ†æ
    result = await crawl4ai.call_tool(
        "advanced_page_intelligence",
        {"url": "https://example.com", "intelligence_level": "standard"}
    )
    
    print(f"åˆ†æç»“æœ: {result['status']}")
```

### åœºæ™¯åŒ–ä½¿ç”¨ç¤ºä¾‹

#### 1. æ–°é—»ç«™ç‚¹å†…å®¹æå–

```python
async def extract_news_content():
    """æå–æ–°é—»ç«™ç‚¹çš„æ–‡ç« å†…å®¹"""
    
    # ä½¿ç”¨Crawl4AIè¿›è¡Œç»“æ„åŒ–æŒ–æ˜
    result = await crawl4ai_client.call_tool(
        "structural_content_mining",
        {
            "url": "https://news.example.com",
            "target_structures": ["articles", "headlines", "timestamps"],
            "semantic_rules": [
                "æå–æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦",
                "è¯†åˆ«å‘å¸ƒæ—¶é—´å’Œä½œè€…",
                "æå–æ­£æ–‡å†…å®¹å’Œå›¾ç‰‡"
            ]
        }
    )
    
    return result
```

#### 2. ç”µå•†äº§å“ä¿¡æ¯çˆ¬å–

```python
async def extract_product_info():
    """æå–ç”µå•†ç½‘ç«™çš„äº§å“ä¿¡æ¯"""
    
    # ä½¿ç”¨Browser Useè¿›è¡Œæ™ºèƒ½äº¤äº’
    result = await browser_use_client.call_tool(
        "smart_content_extraction",
        {
            "url": "https://shop.example.com/product/123",
            "extraction_rules": [
                "æå–äº§å“åç§°ã€ä»·æ ¼å’Œè§„æ ¼",
                "è·å–ç”¨æˆ·è¯„ä»·å’Œè¯„åˆ†",
                "æå–äº§å“æè¿°å’Œå›¾ç‰‡é“¾æ¥"
            ],
            "output_format": "json"
        }
    )
    
    return result
```

#### 3. æœç´¢ç»“æœé¡µé¢æ‰¹é‡å¤„ç†

```python
async def process_search_results():
    """æ‰¹é‡å¤„ç†æœç´¢å¼•æ“ç»“æœé¡µé¢"""
    
    search_urls = [
        "https://www.google.com/search?q=AI+research",
        "https://www.bing.com/search?q=machine+learning",
        "https://duckduckgo.com/?q=neural+networks"
    ]
    
    # ä½¿ç”¨Crawl4AIæ‰¹é‡å¤„ç†
    result = await crawl4ai_client.call_tool(
        "batch_intelligent_crawling",
        {
            "urls": search_urls,
            "crawl_strategy": "parallel",
            "max_concurrent": 3,
            "unified_analysis": True
        }
    )
    
    return result
```

#### 4. åŠ¨æ€å†…å®¹ç½‘ç«™å¤„ç†

```python
async def handle_dynamic_content():
    """å¤„ç†JavaScriptæ¸²æŸ“çš„åŠ¨æ€å†…å®¹"""
    
    # ä½¿ç”¨Crawl4AIå¤„ç†åŠ¨æ€å†…å®¹
    result = await crawl4ai_client.call_tool(
        "dynamic_content_analysis",
        {
            "url": "https://spa.example.com",
            "interaction_script": """
                // ç‚¹å‡»åŠ è½½æ›´å¤šæŒ‰é’®
                document.querySelector('.load-more').click();
                await new Promise(resolve => setTimeout(resolve, 3000));
            """,
            "analysis_triggers": ["scroll", "click", "wait"]
        }
    )
    
    return result
```

#### 5. è¡¨å•è‡ªåŠ¨åŒ–å¤„ç†

```python
async def automate_form_submission():
    """è‡ªåŠ¨åŒ–è¡¨å•å¡«å†™å’Œæäº¤"""
    
    # ä½¿ç”¨Browser Useå¤„ç†å¤æ‚è¡¨å•
    result = await browser_use_client.call_tool(
        "adaptive_form_interaction",
        {
            "url": "https://form.example.com",
            "form_intent": "æ³¨å†Œæ–°ç”¨æˆ·è´¦æˆ·",
            "auto_submit": False,  # å®‰å…¨èµ·è§ä¸è‡ªåŠ¨æäº¤
            "result_analysis": True
        }
    )
    
    return result
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### Crawl4AIä¼˜åŒ–

```python
# æ‰¹é‡çˆ¬å–ä¼˜åŒ–é…ç½®
batch_config = {
    "crawl_strategy": "parallel",
    "max_concurrent": 5,  # æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´
    "unified_analysis": True,
    "cache_strategy": "intelligent"
}

# åŠ¨æ€å†…å®¹ä¼˜åŒ–
dynamic_config = {
    "page_timeout": 45000,  # å¢åŠ è¶…æ—¶æ—¶é—´
    "wait_strategy": "smart",  # æ™ºèƒ½ç­‰å¾…ç­–ç•¥
    "resource_filtering": True  # è¿‡æ»¤æ— å…³èµ„æº
}
```

### Browser Useä¼˜åŒ–

```python
# æµè§ˆå™¨é…ç½®ä¼˜åŒ–
browser_config = {
    "headless": True,  # æ— å¤´æ¨¡å¼æå‡æ€§èƒ½
    "disable_images": True,  # ç¦ç”¨å›¾ç‰‡åŠ è½½
    "disable_javascript": False,  # ä¿æŒJSæ”¯æŒ
    "user_agent_rotation": True  # è½®æ¢User Agent
}

# äº¤äº’ä¼˜åŒ–
interaction_config = {
    "max_steps": 10,  # é™åˆ¶æ“ä½œæ­¥æ•°
    "step_timeout": 5000,  # å•æ­¥è¶…æ—¶
    "error_recovery": True  # å¯ç”¨é”™è¯¯æ¢å¤
}
```

## ğŸ”’ å®‰å…¨å’Œåˆè§„

### åçˆ¬è™«å¯¹ç­–

```python
# è¯·æ±‚é¢‘ç‡æ§åˆ¶
rate_limit_config = {
    "requests_per_second": 2,
    "burst_limit": 5,
    "backoff_strategy": "exponential"
}

# ä»£ç†é…ç½®
proxy_config = {
    "proxy_rotation": True,
    "proxy_pool": ["proxy1:port", "proxy2:port"],
    "failure_threshold": 3
}

# User Agentè½®æ¢
ua_config = {
    "rotate_user_agents": True,
    "custom_user_agents": [
        "Mozilla/5.0 (compatible; CustomBot/1.0)",
        "Mozilla/5.0 (compatible; ResearchBot/1.0)"
    ]
}
```

### åˆè§„æ€§æ£€æŸ¥

```python
async def compliance_check(url: str):
    """æ£€æŸ¥ç½‘ç«™çš„robots.txtå’Œçˆ¬å–æ”¿ç­–"""
    
    robots_checker = RobotsChecker()
    
    # æ£€æŸ¥robots.txt
    if not robots_checker.can_crawl(url, "CustomBot"):
        return {"allowed": False, "reason": "robots.txtç¦æ­¢"}
    
    # æ£€æŸ¥çˆ¬å–é¢‘ç‡
    if rate_limiter.is_rate_limited(url):
        return {"allowed": False, "reason": "é¢‘ç‡é™åˆ¶"}
    
    return {"allowed": True}
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. é¡µé¢åŠ è½½è¶…æ—¶
```python
# å¢åŠ è¶…æ—¶æ—¶é—´å’Œé‡è¯•æœºåˆ¶
config = {
    "page_timeout": 60000,
    "retry_attempts": 3,
    "retry_delay": 5000
}
```

#### 2. JavaScriptæ‰§è¡Œå¤±è´¥
```python
# ä½¿ç”¨æ›´å¼ºçš„JSæ‰§è¡Œç­–ç•¥
js_config = {
    "wait_for_selector": "body",
    "evaluate_on_new_document": True,
    "ignore_https_errors": True
}
```

#### 3. å†…å®¹æå–ä¸å‡†ç¡®
```python
# ä½¿ç”¨æ›´ç²¾ç¡®çš„æå–ç­–ç•¥
extraction_config = {
    "extraction_strategy": "hybrid",  # CSS + LLMæ··åˆ
    "confidence_threshold": 0.8,
    "fallback_strategy": "manual_selectors"
}
```

### ç›‘æ§å’Œæ—¥å¿—

```python
import logging

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawling.log'),
        logging.StreamHandler()
    ]
)

# æ€§èƒ½ç›‘æ§
async def monitor_performance():
    """ç›‘æ§çˆ¬å–æ€§èƒ½"""
    
    metrics = {
        "success_rate": calculate_success_rate(),
        "average_response_time": calculate_avg_response_time(),
        "error_rate": calculate_error_rate(),
        "throughput": calculate_throughput()
    }
    
    logger.info(f"Performance metrics: {metrics}")
```

## ğŸš€ éƒ¨ç½²å»ºè®®

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```yaml
# docker-compose.yml
version: '3.8'
services:
  crawl4ai-service:
    build: .
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
      - PROXY_POOL=proxy1:8080,proxy2:8080
    depends_on:
      - redis
      - proxy-pool
    
  browser-use-service:
    build: .
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - BROWSER_POOL_SIZE=5
      - HEADLESS_MODE=true
    volumes:
      - /dev/shm:/dev/shm  # å…±äº«å†…å­˜ä¼˜åŒ–
```

### æ‰©å±•æ€§é…ç½®

```python
# åˆ†å¸ƒå¼ä»»åŠ¡é…ç½®
celery_config = {
    "broker_url": "redis://localhost:6379/0",
    "result_backend": "redis://localhost:6379/0",
    "task_routes": {
        "crawl4ai.tasks": {"queue": "crawl4ai"},
        "browser_use.tasks": {"queue": "browser_use"}
    }
}

# è´Ÿè½½å‡è¡¡é…ç½®
load_balancer_config = {
    "strategy": "round_robin",
    "health_check_interval": 30,
    "max_connections_per_instance": 100
}
```

## ğŸ“ˆ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„å·¥å…·

- **å¤§æ‰¹é‡ã€é«˜é¢‘ç‡çˆ¬å–** â†’ Crawl4AI
- **å¤æ‚äº¤äº’ã€ç²¾ç»†æ§åˆ¶** â†’ Browser Use
- **æ··åˆéœ€æ±‚** â†’ ä¸¤è€…é…åˆä½¿ç”¨

### 2. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

- åˆç†è®¾ç½®å¹¶å‘æ•°é‡
- ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è¯·æ±‚
- å®æ–½æ™ºèƒ½é‡è¯•æœºåˆ¶
- ç›‘æ§å’Œè°ƒä¼˜ç³»ç»Ÿæ€§èƒ½

### 3. æ•°æ®è´¨é‡ä¿è¯

- å¤šé‡éªŒè¯æœºåˆ¶
- ç½®ä¿¡åº¦è¯„ä¼°
- å¼‚å¸¸æ•°æ®æ£€æµ‹
- äººå·¥å®¡æ ¸æµç¨‹

### 4. åˆè§„æ€§è¦æ±‚

- éµå®ˆrobots.txtè§„åˆ™
- å®æ–½é¢‘ç‡é™åˆ¶
- å°Šé‡ç½‘ç«™æ¡æ¬¾
- ä¿æŠ¤ç”¨æˆ·éšç§

è¿™å¥—æ™ºèƒ½åŒ–è§£å†³æ–¹æ¡ˆä¸ºæ£€ç´¢ç»“æœé¡µé¢çš„è‡ªåŠ¨è§£ææä¾›äº†å¼ºå¤§è€Œçµæ´»çš„å·¥å…·é›†ï¼Œèƒ½å¤Ÿé€‚åº”å„ç§å¤æ‚çš„ç½‘ç«™ç»“æ„å’Œéœ€æ±‚åœºæ™¯ã€‚ 