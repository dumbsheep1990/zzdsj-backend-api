# æ™ºèƒ½å†…å®¹å¤„ç†é›†æˆæ–¹æ¡ˆ

## æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»äº†ZZDSJåç«¯APIé¡¹ç›®ä¸­æ™ºèƒ½å†…å®¹å¤„ç†åŠŸèƒ½çš„å®Œæ•´é›†æˆæ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé›†æˆäº†markitdownæ¡†æ¶ã€æ™ºèƒ½ç½‘é¡µçˆ¬è™«ã€å†…å®¹åˆ†æå’Œå‘é‡åŒ–å¤„ç†åŠŸèƒ½ï¼Œä¸ºé¡¹ç›®æä¾›äº†å¼ºå¤§çš„ç½‘é¡µå†…å®¹å¤„ç†èƒ½åŠ›ã€‚

## æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

```
æ™ºèƒ½å†…å®¹å¤„ç†ç³»ç»Ÿ
â”œâ”€â”€ MarkItDowné€‚é…å™¨          # å¤šæ ¼å¼å†…å®¹è½¬æ¢
â”œâ”€â”€ å¢å¼ºç½‘é¡µçˆ¬è™«              # æ™ºèƒ½ç½‘é¡µé‡‡é›†
â”œâ”€â”€ æ™ºèƒ½çˆ¬è™«è°ƒåº¦å™¨            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ
â”œâ”€â”€ å†…å®¹è´¨é‡åˆ†æå™¨            # å…¨æ–¹ä½è´¨é‡è¯„ä¼°
â”œâ”€â”€ æ™ºèƒ½å†…å®¹åˆ†æå™¨            # æ·±åº¦å†…å®¹è§£æ
â””â”€â”€ æ”¿ç­–æœç´¢é›†æˆ              # ä¸šåŠ¡åœºæ™¯åº”ç”¨
```

### æ•°æ®æµç¨‹

```mermaid
graph TD
    A[åŸå§‹å†…å®¹] --> B[æ ¼å¼æ£€æµ‹]
    B --> C{å†…å®¹ç±»å‹}
    C -->|HTML| D[MarkItDownè½¬æ¢]
    C -->|å…¶ä»–æ ¼å¼| E[æ ¼å¼é€‚é…å™¨]
    D --> F[å†…å®¹æ¸…æ´—]
    E --> F
    F --> G[è´¨é‡åˆ†æ]
    G --> H{è´¨é‡è¯„ä¼°}
    H -->|é«˜è´¨é‡| I[æ™ºèƒ½æå–]
    H -->|ä½è´¨é‡| J[è´¨é‡æ”¹è¿›]
    I --> K[å‘é‡åŒ–å¤„ç†]
    K --> L[å­˜å‚¨&ç´¢å¼•]
    J --> F
```

## åŠŸèƒ½æ¨¡å—

### 1. MarkItDowné€‚é…å™¨

**æ–‡ä»¶ä½ç½®**: `app/tools/advanced/content/markitdown_adapter.py`

**ä¸»è¦åŠŸèƒ½**:
- æ”¯æŒHTMLã€XMLã€PDFã€DOCXç­‰æ ¼å¼è½¬æ¢
- æ™ºèƒ½å†…å®¹æ¸…æ´—å’Œæ ¼å¼ä¼˜åŒ–
- å…ƒæ•°æ®æå–å’Œç»“æ„åŒ–å¤„ç†
- å¼‚æ­¥å¤„ç†æ”¯æŒ

**æ ¸å¿ƒAPI**:
```python
from app.tools.advanced.content import get_markitdown_adapter

adapter = get_markitdown_adapter()
await adapter.initialize()

result = adapter.convert_to_markdown(html_content, "html", source_url)
```

**é…ç½®é¡¹**:
- `markitdown.enabled`: å¯ç”¨/ç¦ç”¨åŠŸèƒ½
- `markitdown.timeout`: å¤„ç†è¶…æ—¶æ—¶é—´
- `markitdown.quality_threshold`: è´¨é‡é˜ˆå€¼

### 2. å¢å¼ºç½‘é¡µçˆ¬è™«

**æ–‡ä»¶ä½ç½®**: `app/tools/advanced/search/enhanced_web_crawler.py`

**ä¸»è¦åŠŸèƒ½**:
- æ™ºèƒ½å†…å®¹é‡‡é›†å’Œè´¨é‡åˆ†æ
- å¹¶å‘å¤„ç†å’Œé”™è¯¯æ¢å¤
- ç»“æ„åŒ–æ•°æ®æå–
- é‡å¤å†…å®¹æ£€æµ‹

**æ ¸å¿ƒAPI**:
```python
from app.tools.advanced.search import get_enhanced_web_crawler

crawler = get_enhanced_web_crawler()
await crawler.initialize()

# å•URLçˆ¬å–
result = await crawler.crawl_url(url)

# æ‰¹é‡çˆ¬å–
results = await crawler.crawl_urls_batch(urls)
```

**è´¨é‡åˆ†ææŒ‡æ ‡**:
- æ–‡æœ¬å¯†åº¦è¯„åˆ†
- ç»“æ„è´¨é‡è¯„åˆ†
- å†…å®¹ç›¸å…³æ€§è¯„åˆ†
- è¯­è¨€è´¨é‡è¯„åˆ†

### 3. æ™ºèƒ½çˆ¬è™«è°ƒåº¦å™¨

**æ–‡ä»¶ä½ç½®**: `app/tools/advanced/search/intelligent_crawler_scheduler.py`

**ä¸»è¦åŠŸèƒ½**:
- è‡ªåŠ¨é€‰æ‹©æœ€ä½³çˆ¬å–ç­–ç•¥
- æ¨¡å‹é…ç½®åŠ¨æ€è·å–
- æ™ºèƒ½é‡è¯•æœºåˆ¶
- è´¨é‡è¯„ä¼°å’Œç»“æœèåˆ

**æ ¸å¿ƒAPI**:
```python
from app.tools.advanced.search import IntelligentCrawlerScheduler

scheduler = IntelligentCrawlerScheduler()
await scheduler.initialize()

result = await scheduler.intelligent_crawl(url)
```

**è°ƒåº¦ç­–ç•¥**:
- é¡µé¢å¤æ‚åº¦æ£€æµ‹
- å†…å®¹ç±»å‹è¯†åˆ«
- èµ„æºä½¿ç”¨ä¼˜åŒ–
- é”™è¯¯å¤„ç†ç­–ç•¥

### 4. å†…å®¹è´¨é‡åˆ†æå™¨

**é›†æˆä½ç½®**: `enhanced_web_crawler.py` ä¸­çš„ `ContentQualityAnalyzer`

**åˆ†æç»´åº¦**:
- **æ–‡æœ¬å¯†åº¦**: æœ‰æ•ˆæ–‡æœ¬ä¸HTMLæ ‡ç­¾æ¯”ä¾‹
- **ç»“æ„è´¨é‡**: æ ‡é¢˜å±‚æ¬¡ã€æ®µè½ç»„ç»‡ã€è¯­ä¹‰æ ‡ç­¾
- **å†…å®¹ç›¸å…³æ€§**: é•¿åº¦åˆç†æ€§ã€è¯æ±‡å¤šæ ·æ€§
- **è¯­è¨€è´¨é‡**: å¥å­ç»“æ„ã€æ®µè½ç»“æ„ã€è¯­è¨€è¯†åˆ«

**è¯„åˆ†ç®—æ³•**:
```python
overall_score = (
    text_density * 0.3 +
    structure_quality * 0.25 +
    content_relevance * 0.25 +
    language_quality * 0.2
)
```

### 5. æ™ºèƒ½å†…å®¹åˆ†æå™¨

**æ–‡ä»¶ä½ç½®**: `app/tools/advanced/content/intelligent_content_analyzer.py`

**ä¸»è¦åŠŸèƒ½**:
- æ·±åº¦æ–‡æœ¬ç»Ÿè®¡åˆ†æ
- ç»“æ„åŒ–å†…å®¹è§£æ
- å¯è¯»æ€§è¯„ä¼°
- è¯­è¨€æ£€æµ‹

**åˆ†ææŠ¥å‘Š**:
- å†…å®¹æ¦‚è§ˆç»Ÿè®¡
- ç»“æ„è´¨é‡è¯„ä¼°
- å¯è¯»æ€§æŒ‡æ ‡
- æ”¹è¿›å»ºè®®

### 6. æ”¿ç­–æœç´¢é›†æˆ

**æ–‡ä»¶ä½ç½®**: `app/tools/advanced/search/policy_search_tool.py`

**å¢å¼ºåŠŸèƒ½**:
- æ™ºèƒ½çˆ¬è™«ç»“æœå¢å¼º
- è‡ªåŠ¨å†…å®¹è´¨é‡è¯„ä¼°
- å¤šæºç»“æœèåˆ
- é…ç½®åŒ–æ™ºèƒ½å¤„ç†

## é…ç½®ç®¡ç†

### ç³»ç»Ÿé…ç½®é¡¹

**æ–‡ä»¶ä½ç½®**: `migrations/sql/common/03_crawling_configs.sql`

ä¸»è¦é…ç½®ç±»åˆ«:

1. **æ¨¡å‹é…ç½®**
   - LLMæä¾›å•†è®¾ç½®
   - APIå¯†é’¥ç®¡ç†
   - æ¨¡å‹å‚æ•°é…ç½®

2. **çˆ¬è™«é…ç½®**
   - å¹¶å‘æ•°æ§åˆ¶
   - è¶…æ—¶è®¾ç½®
   - é‡è¯•ç­–ç•¥

3. **è´¨é‡æ§åˆ¶**
   - è´¨é‡é˜ˆå€¼è®¾å®š
   - å†…å®¹é•¿åº¦é™åˆ¶
   - è¿‡æ»¤è§„åˆ™

4. **ç¼“å­˜é…ç½®**
   - ç¼“å­˜ç­–ç•¥
   - è¿‡æœŸæ—¶é—´
   - å­˜å‚¨é€‰é¡¹

### é…ç½®ç¤ºä¾‹

```sql
-- å¯ç”¨æ™ºèƒ½çˆ¬è™«
INSERT INTO system_configs (config_key, config_value, description) 
VALUES ('crawler.intelligent_enabled', 'true', 'å¯ç”¨æ™ºèƒ½çˆ¬è™«åŠŸèƒ½');

-- è´¨é‡é˜ˆå€¼è®¾ç½®
INSERT INTO system_configs (config_key, config_value, description) 
VALUES ('crawler.quality_threshold', '0.6', 'å†…å®¹è´¨é‡æœ€ä½é˜ˆå€¼');

-- MarkItDowné…ç½®
INSERT INTO system_configs (config_key, config_value, description) 
VALUES ('markitdown.enabled', 'true', 'å¯ç”¨MarkItDownè½¬æ¢');
```

## ä½¿ç”¨æŒ‡å—

### åŸºç¡€ä½¿ç”¨

```python
# 1. ç®€å•å†…å®¹è½¬æ¢
from app.tools.advanced.content import convert_html_to_markdown

result = convert_html_to_markdown(html_content, source_url)

# 2. æ™ºèƒ½ç½‘é¡µçˆ¬å–
from app.tools.advanced.search import crawl_and_process_url

result = await crawl_and_process_url(url)

# 3. æ‰¹é‡å¤„ç†
from app.tools.advanced.search import crawl_and_process_urls

results = await crawl_and_process_urls(urls)
```

### é«˜çº§ä½¿ç”¨

```python
# 1. è‡ªå®šä¹‰é…ç½®çš„çˆ¬è™«
crawler = get_enhanced_web_crawler()
crawler.config.quality_threshold = 0.8
crawler.config.enable_markitdown = True

result = await crawler.crawl_url(url)

# 2. æ™ºèƒ½è°ƒåº¦å™¨
scheduler = IntelligentCrawlerScheduler()
scheduler.force_crawler_type = "crawl4ai"  # å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šçˆ¬è™«

result = await scheduler.intelligent_crawl(url)

# 3. å†…å®¹åˆ†æ
analyzer = get_intelligent_content_analyzer()
analysis = await analyzer.analyze_content(content, "html", url)
```

### æ”¿ç­–æœç´¢é›†æˆ

```python
# å¢å¼ºæ”¿ç­–æœç´¢
from app.frameworks.llamaindex.adapters import PolicySearchAdapter

adapter = PolicySearchAdapter()
await adapter.initialize()

# è‡ªåŠ¨å¯ç”¨æ™ºèƒ½çˆ¬è™«å¢å¼º
results = await adapter.enhanced_policy_search(
    query="æ•°æ®å®‰å…¨æ”¿ç­–",
    enable_intelligent_crawling=True
)
```

## æ€§èƒ½ä¼˜åŒ–

### å¹¶å‘æ§åˆ¶

- **çˆ¬è™«å¹¶å‘**: é»˜è®¤5ä¸ªå¹¶å‘è¿æ¥
- **å¤„ç†é˜Ÿåˆ—**: å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
- **èµ„æºé™åˆ¶**: å†…å­˜å’ŒCPUä½¿ç”¨ç›‘æ§

### ç¼“å­˜ç­–ç•¥

- **URLæŒ‡çº¹**: é¿å…é‡å¤å¤„ç†
- **ç»“æœç¼“å­˜**: ä¸´æ—¶å­˜å‚¨å¤„ç†ç»“æœ
- **é…ç½®ç¼“å­˜**: ç³»ç»Ÿé…ç½®æœ¬åœ°ç¼“å­˜

### é”™è¯¯å¤„ç†

- **é‡è¯•æœºåˆ¶**: æ™ºèƒ½æŒ‡æ•°é€€é¿
- **é™çº§ç­–ç•¥**: è´¨é‡ä¸è¾¾æ ‡æ—¶çš„å¤„ç†
- **å¼‚å¸¸æ¢å¤**: è‡ªåŠ¨é”™è¯¯æ¢å¤

## è´¨é‡ä¿è¯

### æµ‹è¯•è¦†ç›–

1. **å•å…ƒæµ‹è¯•**: å„ç»„ä»¶åŠŸèƒ½æµ‹è¯•
2. **é›†æˆæµ‹è¯•**: ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•
3. **æ€§èƒ½æµ‹è¯•**: å¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•
4. **è´¨é‡æµ‹è¯•**: å†…å®¹å¤„ç†è´¨é‡éªŒè¯

### ç›‘æ§æŒ‡æ ‡

- **å¤„ç†æˆåŠŸç‡**: çˆ¬å–å’Œè½¬æ¢æˆåŠŸç‡
- **è´¨é‡åˆ†å¸ƒ**: å†…å®¹è´¨é‡è¯„åˆ†åˆ†å¸ƒ
- **æ€§èƒ½æŒ‡æ ‡**: å¤„ç†æ—¶é—´å’Œèµ„æºä½¿ç”¨
- **é”™è¯¯ç»Ÿè®¡**: é”™è¯¯ç±»å‹å’Œé¢‘ç‡

## æ¼”ç¤ºå’Œæµ‹è¯•

### æ¼”ç¤ºè„šæœ¬

**æ–‡ä»¶ä½ç½®**: `scripts/demo/intelligent_content_processing_demo.py`

**è¿è¡Œæ–¹å¼**:
```bash
# å®Œæ•´æ¼”ç¤º
python scripts/demo/intelligent_content_processing_demo.py full

# äº¤äº’å¼æ¼”ç¤º
python scripts/demo/intelligent_content_processing_demo.py interactive
```

**æ¼”ç¤ºå†…å®¹**:
1. MarkItDownå†…å®¹è½¬æ¢æ¼”ç¤º
2. å¢å¼ºç½‘é¡µçˆ¬è™«æ¼”ç¤º
3. æ™ºèƒ½è°ƒåº¦å™¨æ¼”ç¤º
4. æ‰¹é‡å¤„ç†æ¼”ç¤º
5. æ–¹æ³•å¯¹æ¯”æ¼”ç¤º

### æµ‹è¯•æ•°æ®

**å†…ç½®æµ‹è¯•URL**:
- `https://httpbin.org/html` - ç®€å•HTMLæµ‹è¯•
- `https://httpbin.org/json` - JSONæ ¼å¼æµ‹è¯•
- `https://example.com` - æ ‡å‡†ç½‘ç«™æµ‹è¯•

## éƒ¨ç½²å»ºè®®

### ä¾èµ–å®‰è£…

```bash
# æ ¸å¿ƒä¾èµ–
pip install markitdown
pip install aiohttp beautifulsoup4
pip install llama-index

# å¯é€‰å¢å¼º
pip install crawl4ai  # é«˜çº§çˆ¬è™«æ”¯æŒ
pip install playwright  # æµè§ˆå™¨è‡ªåŠ¨åŒ–
```

### ç¯å¢ƒé…ç½®

```env
# MarkItDowné…ç½®
MARKITDOWN_ENABLED=true
MARKITDOWN_TIMEOUT=30

# çˆ¬è™«é…ç½®
CRAWLER_MAX_CONCURRENT=5
CRAWLER_TIMEOUT=30
CRAWLER_QUALITY_THRESHOLD=0.6

# æ¨¡å‹é…ç½®
LLM_PROVIDER=openai
OPENAI_API_KEY=your_api_key
```

### ç³»ç»Ÿè¦æ±‚

- **Python**: 3.8+
- **å†…å­˜**: æœ€ä½4GBï¼Œæ¨è8GB+
- **ç½‘ç»œ**: ç¨³å®šçš„å¤–ç½‘è¿æ¥
- **å­˜å‚¨**: SSDæ¨èï¼Œç”¨äºç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **MarkItDownåˆå§‹åŒ–å¤±è´¥**
   - æ£€æŸ¥ä¾èµ–å®‰è£…
   - éªŒè¯ç½‘ç»œè¿æ¥
   - æŸ¥çœ‹æ—¥å¿—é”™è¯¯ä¿¡æ¯

2. **çˆ¬è™«è¶…æ—¶**
   - å¢åŠ è¶…æ—¶æ—¶é—´è®¾ç½®
   - æ£€æŸ¥ç›®æ ‡ç½‘ç«™å¯è®¿é—®æ€§
   - è€ƒè™‘ä½¿ç”¨ä»£ç†

3. **è´¨é‡è¯„åˆ†å¼‚å¸¸**
   - æ£€æŸ¥å†…å®¹æ ¼å¼
   - è°ƒæ•´è´¨é‡é˜ˆå€¼
   - æŸ¥çœ‹è¯¦ç»†åˆ†æç»“æœ

4. **é…ç½®åŠ è½½å¤±è´¥**
   - éªŒè¯æ•°æ®åº“è¿æ¥
   - æ£€æŸ¥é…ç½®è¡¨ç»“æ„
   - ç¡®è®¤é…ç½®é¡¹å­˜åœ¨

### è°ƒè¯•æ–¹æ³•

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
crawler = get_enhanced_web_crawler()
await crawler.initialize()
```

## æ‰©å±•å¼€å‘

### è‡ªå®šä¹‰çˆ¬è™«

```python
class CustomCrawler(EnhancedWebCrawler):
    async def custom_extraction(self, html_content):
        # å®ç°è‡ªå®šä¹‰æå–é€»è¾‘
        pass
```

### è‡ªå®šä¹‰è´¨é‡åˆ†æ

```python
class CustomQualityAnalyzer(ContentQualityAnalyzer):
    def analyze_custom_metrics(self, content):
        # å®ç°è‡ªå®šä¹‰è´¨é‡æŒ‡æ ‡
        pass
```

### æ–°æ ¼å¼æ”¯æŒ

```python
class CustomFormatAdapter:
    def convert_custom_format(self, content):
        # å®ç°æ–°æ ¼å¼è½¬æ¢
        pass
```

## æ›´æ–°æ—¥å¿—

### v1.0.0 (å½“å‰ç‰ˆæœ¬)
- âœ… MarkItDownæ¡†æ¶é›†æˆ
- âœ… å¢å¼ºç½‘é¡µçˆ¬è™«å®ç°
- âœ… æ™ºèƒ½è°ƒåº¦å™¨å¼€å‘
- âœ… å†…å®¹è´¨é‡åˆ†æç³»ç»Ÿ
- âœ… æ”¿ç­–æœç´¢åŠŸèƒ½é›†æˆ
- âœ… å®Œæ•´é…ç½®ç®¡ç†
- âœ… æ¼”ç¤ºå’Œæµ‹è¯•è„šæœ¬

### è®¡åˆ’åŠŸèƒ½
- ğŸ”„ å‘é‡åŒ–å¤„ç†å¢å¼º
- ğŸ”„ å¤šè¯­è¨€å†…å®¹æ”¯æŒ
- ğŸ”„ å®æ—¶å¤„ç†æµæ°´çº¿
- ğŸ”„ æœºå™¨å­¦ä¹ è´¨é‡æ¨¡å‹
- ğŸ”„ åˆ†å¸ƒå¼å¤„ç†æ”¯æŒ

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»å¼€å‘å›¢é˜Ÿæˆ–æäº¤Issueåˆ°é¡¹ç›®ä»“åº“ã€‚

---

*æ–‡æ¡£ç‰ˆæœ¬: v1.0.0*  
*æœ€åæ›´æ–°: 2024-01-XX*  
*ç»´æŠ¤å›¢é˜Ÿ: ZZDSJå¼€å‘ç»„* 