#!/usr/bin/env python3
"""
å·¥å…·æ€§èƒ½ç›‘æ§ç³»ç»Ÿæµ‹è¯•
"""

import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import statistics

class MetricType(Enum):
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"  
    ERROR_RATE = "error_rate"
    MEMORY_USAGE = "memory_usage"
    CPU_USAGE = "cpu_usage"
    CONCURRENT_USERS = "concurrent_users"

class AlertLevel(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"

@dataclass
class PerformanceMetric:
    metric_id: str
    metric_type: MetricType
    tool_name: str
    value: float
    timestamp: datetime
    unit: str
    metadata: Optional[Dict[str, Any]] = field(default_factory=dict)

@dataclass
class PerformanceAlert:
    alert_id: str
    tool_name: str
    metric_type: MetricType
    level: AlertLevel
    message: str
    value: float
    threshold: float
    timestamp: datetime

@dataclass
class PerformanceThreshold:
    metric_type: MetricType
    warning_threshold: float
    critical_threshold: float
    tool_pattern: str = "*"

@dataclass
class PerformanceReport:
    tool_name: str
    time_period: str
    avg_response_time: float = 0.0
    max_response_time: float = 0.0
    min_response_time: float = 0.0
    throughput: float = 0.0
    error_rate: float = 0.0
    availability: float = 100.0
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0

class PerformanceMonitor:
    def __init__(self):
        self._metrics: List[PerformanceMetric] = []
        self._alerts: List[PerformanceAlert] = []
        self._thresholds: List[PerformanceThreshold] = []
        self._max_metrics = 50000
        
        # å®æ—¶æ€§èƒ½è®¡æ•°å™¨
        self._tool_metrics: Dict[str, Dict[str, List[float]]] = {}
        
        # åˆå§‹åŒ–é»˜è®¤é˜ˆå€¼
        self._initialize_default_thresholds()
    
    def _initialize_default_thresholds(self):
        """åˆå§‹åŒ–é»˜è®¤æ€§èƒ½é˜ˆå€¼"""
        default_thresholds = [
            PerformanceThreshold(MetricType.RESPONSE_TIME, 2000.0, 5000.0),  # 2sè­¦å‘Šï¼Œ5sä¸¥é‡
            PerformanceThreshold(MetricType.ERROR_RATE, 5.0, 20.0),          # 5%è­¦å‘Šï¼Œ20%ä¸¥é‡
            PerformanceThreshold(MetricType.MEMORY_USAGE, 80.0, 95.0),       # 80%è­¦å‘Šï¼Œ95%ä¸¥é‡
            PerformanceThreshold(MetricType.CPU_USAGE, 70.0, 90.0),          # 70%è­¦å‘Šï¼Œ90%ä¸¥é‡
            PerformanceThreshold(MetricType.THROUGHPUT, 10.0, 5.0),          # ä½äº10è¯·æ±‚/ç§’è­¦å‘Šï¼Œ5è¯·æ±‚/ç§’ä¸¥é‡
        ]
        self._thresholds.extend(default_thresholds)
    
    async def record_metric(self, metric: PerformanceMetric) -> bool:
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        try:
            # æ·»åŠ æŒ‡æ ‡
            self._metrics.append(metric)
            
            # é™åˆ¶æŒ‡æ ‡æ•°é‡
            if len(self._metrics) > self._max_metrics:
                self._metrics = self._metrics[self._max_metrics // 2:]
            
            # æ›´æ–°å®æ—¶è®¡æ•°å™¨
            await self._update_tool_metrics(metric)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
            await self._check_thresholds(metric)
            
            return True
            
        except Exception as e:
            print(f"Failed to record metric: {e}")
            return False
    
    async def _update_tool_metrics(self, metric: PerformanceMetric):
        """æ›´æ–°å·¥å…·å®æ—¶æŒ‡æ ‡"""
        tool_name = metric.tool_name
        metric_type = metric.metric_type.value
        
        if tool_name not in self._tool_metrics:
            self._tool_metrics[tool_name] = {}
        
        if metric_type not in self._tool_metrics[tool_name]:
            self._tool_metrics[tool_name][metric_type] = []
        
        # ä¿æŒæœ€è¿‘100ä¸ªæ•°æ®ç‚¹
        metrics_list = self._tool_metrics[tool_name][metric_type]
        metrics_list.append(metric.value)
        if len(metrics_list) > 100:
            metrics_list.pop(0)
    
    async def _check_thresholds(self, metric: PerformanceMetric):
        """æ£€æŸ¥æ€§èƒ½é˜ˆå€¼"""
        for threshold in self._thresholds:
            if threshold.metric_type != metric.metric_type:
                continue
            
            if threshold.tool_pattern != "*" and threshold.tool_pattern != metric.tool_name:
                continue
            
            # æ£€æŸ¥ä¸´ç•Œé˜ˆå€¼
            if metric.value >= threshold.critical_threshold:
                alert = PerformanceAlert(
                    alert_id=f"alert_{len(self._alerts) + 1}",
                    tool_name=metric.tool_name,
                    metric_type=metric.metric_type,
                    level=AlertLevel.CRITICAL,
                    message=f"Critical {metric.metric_type.value}: {metric.value} {metric.unit}",
                    value=metric.value,
                    threshold=threshold.critical_threshold,
                    timestamp=metric.timestamp
                )
                self._alerts.append(alert)
            
            # æ£€æŸ¥è­¦å‘Šé˜ˆå€¼
            elif metric.value >= threshold.warning_threshold:
                alert = PerformanceAlert(
                    alert_id=f"alert_{len(self._alerts) + 1}",
                    tool_name=metric.tool_name,
                    metric_type=metric.metric_type,
                    level=AlertLevel.WARNING,
                    message=f"Warning {metric.metric_type.value}: {metric.value} {metric.unit}",
                    value=metric.value,
                    threshold=threshold.warning_threshold,
                    timestamp=metric.timestamp
                )
                self._alerts.append(alert)
    
    async def get_tool_performance(self, tool_name: str, start_time: datetime = None, end_time: datetime = None) -> PerformanceReport:
        """è·å–å·¥å…·æ€§èƒ½æŠ¥å‘Š"""
        # è¿‡æ»¤æŒ‡æ ‡
        filtered_metrics = []
        for metric in self._metrics:
            if metric.tool_name != tool_name:
                continue
            if start_time and metric.timestamp < start_time:
                continue
            if end_time and metric.timestamp > end_time:
                continue
            filtered_metrics.append(metric)
        
        if not filtered_metrics:
            return PerformanceReport(tool_name=tool_name, time_period="No data")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        response_times = [m.value for m in filtered_metrics if m.metric_type == MetricType.RESPONSE_TIME]
        error_rates = [m.value for m in filtered_metrics if m.metric_type == MetricType.ERROR_RATE]
        
        report = PerformanceReport(
            tool_name=tool_name,
            time_period=f"{start_time} to {end_time}" if start_time and end_time else "All time"
        )
        
        # å“åº”æ—¶é—´ç»Ÿè®¡
        if response_times:
            report.avg_response_time = statistics.mean(response_times)
            report.max_response_time = max(response_times)
            report.min_response_time = min(response_times)
        
        # é”™è¯¯ç‡ç»Ÿè®¡
        if error_rates:
            report.error_rate = statistics.mean(error_rates)
            report.availability = 100.0 - report.error_rate
        
        # è¯·æ±‚ç»Ÿè®¡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        report.total_requests = len(response_times)
        if error_rates:
            report.failed_requests = int(report.total_requests * report.error_rate / 100)
            report.successful_requests = report.total_requests - report.failed_requests
        else:
            report.successful_requests = report.total_requests
        
        # ååé‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        if response_times and len(response_times) > 1:
            time_span_hours = (filtered_metrics[-1].timestamp - filtered_metrics[0].timestamp).total_seconds() / 3600
            if time_span_hours > 0:
                report.throughput = len(response_times) / time_span_hours
        
        return report
    
    async def get_real_time_metrics(self, tool_name: str) -> Dict[str, Any]:
        """è·å–å®æ—¶æ€§èƒ½æŒ‡æ ‡"""
        if tool_name not in self._tool_metrics:
            return {"error": "No metrics found for tool"}
        
        tool_data = self._tool_metrics[tool_name]
        real_time_metrics = {}
        
        for metric_type, values in tool_data.items():
            if not values:
                continue
            
            real_time_metrics[metric_type] = {
                "current": values[-1],
                "average": statistics.mean(values),
                "max": max(values),
                "min": min(values),
                "samples": len(values)
            }
        
        return real_time_metrics
    
    async def get_alerts(self, level: AlertLevel = None, tool_name: str = None) -> List[PerformanceAlert]:
        """è·å–æ€§èƒ½å‘Šè­¦"""
        filtered_alerts = []
        
        for alert in self._alerts:
            if level and alert.level != level:
                continue
            if tool_name and alert.tool_name != tool_name:
                continue
            filtered_alerts.append(alert)
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        filtered_alerts.sort(key=lambda a: a.timestamp, reverse=True)
        return filtered_alerts
    
    async def add_threshold(self, threshold: PerformanceThreshold) -> bool:
        """æ·»åŠ æ€§èƒ½é˜ˆå€¼"""
        try:
            self._thresholds.append(threshold)
            return True
        except Exception:
            return False
    
    def get_system_health(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        total_tools = len(set(m.tool_name for m in self._metrics))
        
        # æœ€è¿‘çš„å‘Šè­¦
        recent_alerts = [a for a in self._alerts if (datetime.now() - a.timestamp).seconds < 3600]  # æœ€è¿‘1å°æ—¶
        critical_alerts = [a for a in recent_alerts if a.level == AlertLevel.CRITICAL]
        warning_alerts = [a for a in recent_alerts if a.level == AlertLevel.WARNING]
        
        return {
            "total_tools_monitored": total_tools,
            "total_metrics": len(self._metrics),
            "total_alerts": len(self._alerts),
            "recent_critical_alerts": len(critical_alerts),
            "recent_warning_alerts": len(warning_alerts),
            "system_status": "CRITICAL" if critical_alerts else "WARNING" if warning_alerts else "HEALTHY"
        }

async def test_performance_monitor():
    try:
        print("=== å·¥å…·æ€§èƒ½ç›‘æ§ç³»ç»Ÿæµ‹è¯• ===")
        
        monitor = PerformanceMonitor()
        print("âœ… æ€§èƒ½ç›‘æ§å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•1: è®°å½•æ€§èƒ½æŒ‡æ ‡
        print("\nğŸ“‹ æµ‹è¯•1: è®°å½•æ€§èƒ½æŒ‡æ ‡")
        
        now = datetime.now()
        
        # æ­£å¸¸å“åº”æ—¶é—´
        normal_metric = PerformanceMetric(
            metric_id="metric_001",
            metric_type=MetricType.RESPONSE_TIME,
            tool_name="agno_reasoning",
            value=1500.0,
            timestamp=now - timedelta(minutes=30),
            unit="ms"
        )
        
        # é«˜å“åº”æ—¶é—´ï¼ˆè§¦å‘è­¦å‘Šï¼‰
        high_metric = PerformanceMetric(
            metric_id="metric_002", 
            metric_type=MetricType.RESPONSE_TIME,
            tool_name="agno_reasoning",
            value=3000.0,
            timestamp=now - timedelta(minutes=25),
            unit="ms"
        )
        
        # æé«˜å“åº”æ—¶é—´ï¼ˆè§¦å‘ä¸¥é‡å‘Šè­¦ï¼‰
        critical_metric = PerformanceMetric(
            metric_id="metric_003",
            metric_type=MetricType.RESPONSE_TIME,
            tool_name="haystack_extract_answers",
            value=6000.0,
            timestamp=now - timedelta(minutes=20),
            unit="ms"
        )
        
        # é”™è¯¯ç‡æŒ‡æ ‡
        error_metric = PerformanceMetric(
            metric_id="metric_004",
            metric_type=MetricType.ERROR_RATE,
            tool_name="agno_reasoning",
            value=3.5,
            timestamp=now - timedelta(minutes=15),
            unit="%"
        )
        
        success1 = await monitor.record_metric(normal_metric)
        success2 = await monitor.record_metric(high_metric)
        success3 = await monitor.record_metric(critical_metric)
        success4 = await monitor.record_metric(error_metric)
        
        print(f"  â€¢ è®°å½•æ­£å¸¸æŒ‡æ ‡: {'âœ…' if success1 else 'âŒ'}")
        print(f"  â€¢ è®°å½•é«˜å“åº”æ—¶é—´æŒ‡æ ‡: {'âœ…' if success2 else 'âŒ'}")
        print(f"  â€¢ è®°å½•ä¸¥é‡æŒ‡æ ‡: {'âœ…' if success3 else 'âŒ'}")
        print(f"  â€¢ è®°å½•é”™è¯¯ç‡æŒ‡æ ‡: {'âœ…' if success4 else 'âŒ'}")
        
        # æµ‹è¯•2: è·å–æ€§èƒ½æŠ¥å‘Š
        print("\nğŸ“‹ æµ‹è¯•2: è·å–æ€§èƒ½æŠ¥å‘Š")
        
        agno_report = await monitor.get_tool_performance("agno_reasoning")
        print(f"  â€¢ Agnoæ¨ç†å·¥å…·æ€§èƒ½æŠ¥å‘Š:")
        print(f"    - å¹³å‡å“åº”æ—¶é—´: {agno_report.avg_response_time:.1f}ms")
        print(f"    - æœ€å¤§å“åº”æ—¶é—´: {agno_report.max_response_time:.1f}ms")
        print(f"    - æœ€å°å“åº”æ—¶é—´: {agno_report.min_response_time:.1f}ms")
        print(f"    - é”™è¯¯ç‡: {agno_report.error_rate:.1f}%")
        print(f"    - å¯ç”¨æ€§: {agno_report.availability:.1f}%")
        print(f"    - æ€»è¯·æ±‚æ•°: {agno_report.total_requests}")
        
        # æµ‹è¯•3: è·å–å®æ—¶æŒ‡æ ‡
        print("\nğŸ“‹ æµ‹è¯•3: è·å–å®æ—¶æŒ‡æ ‡")
        
        real_time = await monitor.get_real_time_metrics("agno_reasoning")
        if "response_time" in real_time:
            rt_metrics = real_time["response_time"]
            print(f"  â€¢ å®æ—¶å“åº”æ—¶é—´æŒ‡æ ‡:")
            print(f"    - å½“å‰å€¼: {rt_metrics['current']:.1f}ms")
            print(f"    - å¹³å‡å€¼: {rt_metrics['average']:.1f}ms")
            print(f"    - æœ€å¤§å€¼: {rt_metrics['max']:.1f}ms")
            print(f"    - æ ·æœ¬æ•°: {rt_metrics['samples']}")
        
        # æµ‹è¯•4: è·å–å‘Šè­¦
        print("\nğŸ“‹ æµ‹è¯•4: è·å–å‘Šè­¦")
        
        all_alerts = await monitor.get_alerts()
        critical_alerts = await monitor.get_alerts(AlertLevel.CRITICAL)
        warning_alerts = await monitor.get_alerts(AlertLevel.WARNING)
        
        print(f"  â€¢ æ€»å‘Šè­¦æ•°: {len(all_alerts)}")
        print(f"  â€¢ ä¸¥é‡å‘Šè­¦æ•°: {len(critical_alerts)}")
        print(f"  â€¢ è­¦å‘Šå‘Šè­¦æ•°: {len(warning_alerts)}")
        
        if all_alerts:
            latest_alert = all_alerts[0]
            print(f"  â€¢ æœ€æ–°å‘Šè­¦:")
            print(f"    - çº§åˆ«: {latest_alert.level.value}")
            print(f"    - å·¥å…·: {latest_alert.tool_name}")
            print(f"    - æ¶ˆæ¯: {latest_alert.message}")
            print(f"    - å€¼: {latest_alert.value}")
            print(f"    - é˜ˆå€¼: {latest_alert.threshold}")
        
        # æµ‹è¯•5: ç³»ç»Ÿå¥åº·çŠ¶æ€
        print("\nğŸ“‹ æµ‹è¯•5: ç³»ç»Ÿå¥åº·çŠ¶æ€")
        
        health = monitor.get_system_health()
        print(f"  â€¢ ç›‘æ§å·¥å…·æ•°: {health['total_tools_monitored']}")
        print(f"  â€¢ æ€»æŒ‡æ ‡æ•°: {health['total_metrics']}")
        print(f"  â€¢ æ€»å‘Šè­¦æ•°: {health['total_alerts']}")
        print(f"  â€¢ æœ€è¿‘ä¸¥é‡å‘Šè­¦: {health['recent_critical_alerts']}")
        print(f"  â€¢ æœ€è¿‘è­¦å‘Šå‘Šè­¦: {health['recent_warning_alerts']}")
        print(f"  â€¢ ç³»ç»ŸçŠ¶æ€: {health['system_status']}")
        
        # æµ‹è¯•6: æ·»åŠ è‡ªå®šä¹‰é˜ˆå€¼
        print("\nğŸ“‹ æµ‹è¯•6: æ·»åŠ è‡ªå®šä¹‰é˜ˆå€¼")
        
        custom_threshold = PerformanceThreshold(
            metric_type=MetricType.RESPONSE_TIME,
            warning_threshold=1000.0,
            critical_threshold=2000.0,
            tool_pattern="custom_*"
        )
        
        threshold_success = await monitor.add_threshold(custom_threshold)
        print(f"  â€¢ æ·»åŠ è‡ªå®šä¹‰é˜ˆå€¼: {'âœ…' if threshold_success else 'âŒ'}")
        
        print("\nâœ… å·¥å…·æ€§èƒ½ç›‘æ§ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
        
        print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print("âœ… æ€§èƒ½æŒ‡æ ‡è®°å½•åŠŸèƒ½æ­£å¸¸")
        print("âœ… æ€§èƒ½æŠ¥å‘Šç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        print("âœ… å®æ—¶æŒ‡æ ‡è·å–åŠŸèƒ½æ­£å¸¸")
        print("âœ… å‘Šè­¦æœºåˆ¶åŠŸèƒ½æ­£å¸¸")
        print("âœ… é˜ˆå€¼æ£€æŸ¥åŠŸèƒ½æ­£å¸¸")
        print("âœ… ç³»ç»Ÿå¥åº·ç›‘æ§åŠŸèƒ½æ­£å¸¸")
        print("âœ… è‡ªå®šä¹‰é˜ˆå€¼åŠŸèƒ½æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½ç›‘æ§ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(test_performance_monitor())
    print(f"\nğŸ¯ æµ‹è¯•ç»“æœ: {'âœ… å…¨éƒ¨é€šè¿‡' if success else 'âŒ æœ‰å¤±è´¥é¡¹'}")
    exit(0 if success else 1) 