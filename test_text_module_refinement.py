#!/usr/bin/env python3
"""
Textæ¨¡å—ç²¾ç»†åŒ–é‡æ„éªŒè¯æµ‹è¯•
éªŒè¯æ–°çš„textæ¨¡å—é‡æ„ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath('.'))

def test_text_core_base():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†æ ¸å¿ƒåŸºç±»"""
    print("ğŸ” æµ‹è¯• text.core.base æ¨¡å—...")
    
    try:
        from app.utils.text.core.base import (
            TextLanguage, 
            TextProcessingConfig, 
            ChunkConfig, 
            TokenConfig,
            AnalysisResult,
            TextProcessingError,
            InvalidTextError
        )
        
        # æµ‹è¯•æšä¸¾
        assert TextLanguage.CHINESE.value == "zh"
        assert TextLanguage.ENGLISH.value == "en"
        
        # æµ‹è¯•é…ç½®ç±»
        config = TextProcessingConfig(
            language=TextLanguage.CHINESE,
            normalize_whitespace=True
        )
        assert config.language == TextLanguage.CHINESE
        assert config.encoding == "utf-8"
        
        chunk_config = ChunkConfig(chunk_size=1500, chunk_overlap=300)
        assert chunk_config.chunk_size == 1500
        assert chunk_config.chunk_overlap == 300
        
        token_config = TokenConfig(model="gpt-4")
        assert token_config.model == "gpt-4"
        
        # æµ‹è¯•å¼‚å¸¸
        try:
            raise InvalidTextError("æµ‹è¯•å¼‚å¸¸")
        except TextProcessingError:
            pass  # åº”è¯¥èƒ½æ•è·
        
        print("   âœ… åŸºç¡€ç»„ä»¶æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ åŸºç¡€ç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_tokenizer():
    """æµ‹è¯•ä»¤ç‰Œè®¡æ•°å™¨"""
    print("ğŸ” æµ‹è¯• tokenizer æ¨¡å—...")
    
    try:
        from app.utils.text.core.tokenizer import (
            TikTokenCounter, 
            SimpleTokenCounter, 
            create_token_counter,
            count_tokens
        )
        from app.utils.text.core.base import TokenConfig
        
        # æµ‹è¯•ç®€å•è®¡æ•°å™¨ï¼ˆä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰
        config = TokenConfig(model="gpt-3.5-turbo")
        simple_counter = SimpleTokenCounter(config)
        
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯ä»¤ç‰Œè®¡æ•°åŠŸèƒ½ã€‚"
        token_count = simple_counter.count_tokens(test_text)
        
        assert isinstance(token_count, int)
        assert token_count > 0
        print(f"   ğŸ“Š æ–‡æœ¬ä»¤ç‰Œæ•°: {token_count}")
        
        # æµ‹è¯•æˆæœ¬ä¼°ç®—
        cost = simple_counter.estimate_cost(test_text, 0.0001)
        assert isinstance(cost, float)
        assert cost > 0
        print(f"   ğŸ’° ä¼°ç®—æˆæœ¬: ${cost:.6f}")
        
        # æµ‹è¯•å·¥å‚å‡½æ•°
        counter = create_token_counter(use_tiktoken=False, config=config)
        assert isinstance(counter, SimpleTokenCounter)
        
        # æµ‹è¯•å‘åå…¼å®¹å‡½æ•°
        compat_count = count_tokens(test_text, "gpt-3.5-turbo")
        assert isinstance(compat_count, int)
        assert compat_count > 0
        
        print("   âœ… ä»¤ç‰Œè®¡æ•°å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ ä»¤ç‰Œè®¡æ•°å™¨æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_chunker():
    """æµ‹è¯•æ–‡æœ¬åˆ†å—å™¨"""
    print("ğŸ” æµ‹è¯• chunker æ¨¡å—...")
    
    try:
        from app.utils.text.core.chunker import (
            SmartTextChunker,
            SemanticChunker,
            FixedSizeChunker,
            create_chunker,
            chunk_text
        )
        from app.utils.text.core.base import ChunkConfig
        
        # æµ‹è¯•æ–‡æœ¬
        test_text = """
        è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ï¼ŒåŒ…å«äº†ä¸€äº›åŸºæœ¬çš„å†…å®¹ç”¨äºæµ‹è¯•åˆ†å—åŠŸèƒ½ã€‚è¿™æ®µæ–‡æœ¬åº”è¯¥è¶³å¤Ÿé•¿ï¼Œä»¥ä¾¿èƒ½å¤Ÿè§¦å‘åˆ†å—é€»è¾‘ã€‚
        
        è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬ï¼Œå®ƒåŒ…å«äº†ä¸åŒçš„å†…å®¹ã€‚è¿™æ®µæ–‡æœ¬çš„ç›®çš„æ˜¯æµ‹è¯•æ®µè½æ„ŸçŸ¥çš„åˆ†å—åŠŸèƒ½ï¼Œç¡®ä¿åˆ†å—å™¨èƒ½å¤Ÿæ­£ç¡®å¤„ç†æ®µè½è¾¹ç•Œã€‚
        
        è¿™æ˜¯ç¬¬ä¸‰æ®µæ–‡æœ¬ï¼Œç›¸å¯¹è¾ƒçŸ­ä¸€äº›ã€‚ä½†ä»ç„¶åŒ…å«è¶³å¤Ÿçš„å†…å®¹æ¥æµ‹è¯•å„ç§åˆ†å—ç­–ç•¥ã€‚æœ€åä¸€å¥è¯ç”¨æ¥ç»“æŸè¿™ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚
        """ * 3
        
        # æµ‹è¯•æ™ºèƒ½åˆ†å—å™¨
        config = ChunkConfig(chunk_size=200, chunk_overlap=50)
        smart_chunker = SmartTextChunker(config)
        
        chunks = smart_chunker.chunk(test_text)
        assert isinstance(chunks, list)
        assert len(chunks) > 1
        print(f"   ğŸ“„ æ™ºèƒ½åˆ†å—ç»“æœ: {len(chunks)} ä¸ªå—")
        
        # éªŒè¯å—çš„å¤§å°
        for i, chunk in enumerate(chunks):
            assert len(chunk) <= config.chunk_size + 50  # å…è®¸ä¸€äº›å®¹å·®
            print(f"     å— {i+1}: {len(chunk)} å­—ç¬¦")
        
        # æµ‹è¯•è¯­ä¹‰åˆ†å—å™¨
        semantic_chunker = SemanticChunker(config)
        semantic_chunks = semantic_chunker.chunk(test_text)
        assert isinstance(semantic_chunks, list)
        print(f"   ğŸ§  è¯­ä¹‰åˆ†å—ç»“æœ: {len(semantic_chunks)} ä¸ªå—")
        
        # æµ‹è¯•å›ºå®šå¤§å°åˆ†å—å™¨
        fixed_chunker = FixedSizeChunker(config)
        fixed_chunks = fixed_chunker.chunk(test_text)
        assert isinstance(fixed_chunks, list)
        print(f"   ğŸ“ å›ºå®šåˆ†å—ç»“æœ: {len(fixed_chunks)} ä¸ªå—")
        
        # æµ‹è¯•å·¥å‚å‡½æ•°
        chunker = create_chunker("smart", config)
        assert isinstance(chunker, SmartTextChunker)
        
        # æµ‹è¯•å‘åå…¼å®¹å‡½æ•°
        compat_chunks = chunk_text(test_text, chunk_size=200, chunk_overlap=50)
        assert isinstance(compat_chunks, list)
        assert len(compat_chunks) > 0
        
        print("   âœ… æ–‡æœ¬åˆ†å—å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ æ–‡æœ¬åˆ†å—å™¨æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("ğŸ” æµ‹è¯•å‘åå…¼å®¹æ€§...")
    
    try:
        # æµ‹è¯•åŸå§‹æ¥å£æ˜¯å¦ä»ç„¶å¯ç”¨
        from app.utils.text.core.tokenizer import count_tokens
        from app.utils.text.core.chunker import chunk_text
        
        test_text = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•å‘åå…¼å®¹æ€§çš„æ–‡æœ¬ã€‚"
        
        # åŸå§‹ä»¤ç‰Œè®¡æ•°æ¥å£
        tokens = count_tokens(test_text)
        assert isinstance(tokens, int)
        assert tokens > 0
        
        # åŸå§‹åˆ†å—æ¥å£
        chunks = chunk_text(test_text, chunk_size=50)
        assert isinstance(chunks, list)
        
        print("   âœ… å‘åå…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_performance():
    """æµ‹è¯•æ€§èƒ½æ”¹è¿›"""
    print("ğŸ” æµ‹è¯•æ€§èƒ½æ”¹è¿›...")
    
    try:
        import time
        from app.utils.text.core.tokenizer import SimpleTokenCounter
        from app.utils.text.core.chunker import SmartTextChunker
        from app.utils.text.core.base import TokenConfig, ChunkConfig
        
        # åˆ›å»ºå¤§æ–‡æœ¬è¿›è¡Œæ€§èƒ½æµ‹è¯•
        large_text = "è¿™æ˜¯ä¸€ä¸ªå¤§æ–‡æœ¬ç”¨äºæ€§èƒ½æµ‹è¯•ã€‚" * 1000
        
        # æµ‹è¯•ä»¤ç‰Œè®¡æ•°æ€§èƒ½
        config = TokenConfig()
        counter = SimpleTokenCounter(config)
        
        start_time = time.time()
        tokens = counter.count_tokens(large_text)
        token_time = time.time() - start_time
        
        assert tokens > 0
        print(f"   âš¡ ä»¤ç‰Œè®¡æ•°æ€§èƒ½: {token_time:.4f}ç§’ ({tokens} ä»¤ç‰Œ)")
        
        # æµ‹è¯•åˆ†å—æ€§èƒ½
        chunk_config = ChunkConfig(chunk_size=500)
        chunker = SmartTextChunker(chunk_config)
        
        start_time = time.time()
        chunks = chunker.chunk(large_text)
        chunk_time = time.time() - start_time
        
        assert len(chunks) > 0
        print(f"   âš¡ åˆ†å—æ€§èƒ½: {chunk_time:.4f}ç§’ ({len(chunks)} å—)")
        
        # éªŒè¯æ€§èƒ½åˆç†
        assert token_time < 1.0  # ä»¤ç‰Œè®¡æ•°åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
        assert chunk_time < 2.0  # åˆ†å—åº”è¯¥åœ¨2ç§’å†…å®Œæˆ
        
        print("   âœ… æ€§èƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("ğŸš€ Textæ¨¡å—ç²¾ç»†åŒ–é‡æ„éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("åŸºç¡€ç»„ä»¶", test_text_core_base),
        ("ä»¤ç‰Œè®¡æ•°å™¨", test_tokenizer),
        ("æ–‡æœ¬åˆ†å—å™¨", test_chunker),
        ("å‘åå…¼å®¹æ€§", test_backward_compatibility),
        ("æ€§èƒ½æ”¹è¿›", test_performance),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name}æµ‹è¯•:")
        if test_func():
            passed += 1
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼textæ¨¡å—ç²¾ç»†åŒ–é‡æ„æˆåŠŸï¼")
        
        # è¾“å‡ºé‡æ„æˆæœæ€»ç»“
        print("\nğŸ“ˆ é‡æ„æˆæœæ€»ç»“:")
        print("  âœ… å®ç°äº†ç»Ÿä¸€çš„æŠ½è±¡åŸºç±»æ¶æ„")
        print("  âœ… ä¼˜åŒ–äº†ä»¤ç‰Œè®¡æ•°å™¨æ€§èƒ½å’Œç¼“å­˜æœºåˆ¶")
        print("  âœ… å¢å¼ºäº†æ–‡æœ¬åˆ†å—å™¨çš„æ™ºèƒ½è¾¹ç•Œæ£€æµ‹")
        print("  âœ… ä¿æŒäº†å®Œæ•´çš„å‘åå…¼å®¹æ€§")
        print("  âœ… æå‡äº†æ•´ä½“æ€§èƒ½è¡¨ç°")
        print("  âœ… å»ºç«‹äº†å¯æ‰©å±•çš„å·¥å‚æ¨¡å¼æ¶æ„")
        
        return True
    else:
        print(f"âŒ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 